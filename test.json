{"title": ["StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?"], "authors": ["[arxiv.Result.Author('Stefan Pasch'), arxiv.Result.Author('Daniel Ehnes')]"], "link": ["http://arxiv.org/pdf/2202.02268v1"], "summary": "To answer this question, we fine-tune transformer-based language models,\nincluding BERT, on different sources of company-related text data for a\nclassification task to predict the one-year stock price performance. We use\nthree different types of text data: News articles, blogs, and annual reports.\nThis allows us to analyze to what extent the performance of language models is\ndependent on the type of the underlying document. StonkBERT, our\ntransformer-based stock performance classifier, shows substantial improvement\nin predictive accuracy compared to traditional language models. The highest\nperformance was achieved with news articles as text source. Performance\nsimulations indicate that these improvements in classification accuracy also\ntranslate into above-average stock market returns.", "entities_include_in_text": ["Devlin et al. 2018", "Loughran and McDonald 2011; Loughran and McDonald 2016; Jiang et al. 2019", "Chen 2021; Sivri et al. 2022", "Gennadiy 2020", "Loughran  and  McDonald  2011", "Loughran  and \n\nMcDonald 2016", "Ahmad  et  al.  2016", "Hillert et al. 2014", "Duz Tan and Tas 2021; Sprenger et al. \n\n2014", "Chan et al. 2020", "Peters et al. 2018", "Devlin  et  al.  2018", "Gonzalez-Carvajal and Garrido-Merchan 2020", "Liu et al. 2020). \n\nNot  surprisingly,  researchers  have  integrated  these  deep  learning  approaches  to  predict \n\nstock price movements by applying such language models on company-related text data, such \n\nas tweets or company reports. For example, Sawhney et al. (2020", "Gennadiy 2020", "Devlin  et  al.  2018", "Araci 2019", "Liu et al. \n\n2019", "Devlin et al. 2018", "Clark et al. 2020", "Gonzalez-Carvajal and Garrido-Merchan \n\n2020", "Fama 1970", "Hoberg and Phillips 2016", "Jegadeesh and Titman 2001", "Yuan et al. 2021", "Devlin et al. 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["Interpretability methods of machine learning algorithms with applications in breast cancer diagnosis"], "authors": ["[arxiv.Result.Author('Panagiota Karatza'), arxiv.Result.Author('Kalliopi V. Dalakleidi'), arxiv.Result.Author('Maria Athanasiou'), arxiv.Result.Author('Konstantina S. Nikita')]"], "link": ["http://arxiv.org/pdf/2202.02131v1"], "summary": "Early detection of breast cancer is a powerful tool towards decreasing its\nsocioeconomic burden. Although, artificial intelligence (AI) methods have shown\nremarkable results towards this goal, their \"black box\" nature hinders their\nwide adoption in clinical practice. To address the need for AI guided breast\ncancer diagnosis, interpretability methods can be utilized. In this study, we\nused AI methods, i.e., Random Forests (RF), Neural Networks (NN) and Ensembles\nof Neural Networks (ENN), towards this goal and explained and optimized their\nperformance through interpretability techniques, such as the Global Surrogate\n(GS) method, the Individual Conditional Expectation (ICE) plots and the Shapley\nvalues (SV). The Wisconsin Diagnostic Breast Cancer (WDBC) dataset of the open\nUCI repository was used for the training and evaluation of the AI algorithms.\nThe best performance for breast cancer diagnosis was achieved by the proposed\nENN (96.6% accuracy and 0.96 area under the ROC curve), and its predictions\nwere explained by ICE plots, proving that its decisions were compliant with\ncurrent medical knowledge and can be further utilized to gain new insights in\nthe pathophysiological mechanisms of breast cancer. Feature selection based on\nfeatures' importance according to the GS model improved the performance of the\nRF (leading the accuracy from 96.49% to 97.18% and the area under the ROC curve\nfrom 0.96 to 0.97) and feature selection based on features' importance\naccording to SV improved the performance of the NN (leading the accuracy from\n94.6% to 95.53% and the area under the ROC curve from 0.94 to 0.95). Compared\nto other approaches on the same dataset, our proposed models demonstrated state\nof the art performance while being interpretable.", "entities_include_in_text": [], "entities_from_reference": ["Online", "Anderson", "Saygili", "Classification", "Breast Cancers", "Breast Cancer Malignancy", "Networks", "Breast Cancer", "Neighborhood Component Analysis", "Machine Learning Techniques", "Advanced Communication", "Toward Medical XAI", "Nikita", "Diabetes Mellitus", "Santos", "Antunes", "J. Kasmanas", "Machine Learning", "Santurkar", "How Does Batch Normalization Help", "Baba"]}{"title": ["OntoSeer -- A Recommendation System to Improve the Quality of Ontologies"], "authors": ["[arxiv.Result.Author('Pramit Bhattacharyya'), arxiv.Result.Author('Raghava Mutharaju')]"], "link": ["http://arxiv.org/pdf/2202.02125v1"], "summary": "Building an ontology is not only a time-consuming process, but it is also\nconfusing, especially for beginners and the inexperienced. Although ontology\ndevelopers can take the help of domain experts in building an ontology, they\nare not readily available in several cases for a variety of reasons. Ontology\ndevelopers have to grapple with several questions related to the choice of\nclasses, properties, and the axioms that should be included. Apart from this,\nthere are aspects such as modularity and reusability that should be taken care\nof. From among the thousands of publicly available ontologies and vocabularies\nin repositories such as Linked Open Vocabularies (LOV) and BioPortal, it is\nhard to know the terms (classes and properties) that can be reused in the\ndevelopment of an ontology. A similar problem exists in implementing the right\nset of ontology design patterns (ODPs) from among the several available.\nGenerally, ontology developers make use of their experience in handling these\nissues, and the inexperienced ones have a hard time. In order to bridge this\ngap, we propose a tool named OntoSeer, that monitors the ontology development\nprocess and provides suggestions in real-time to improve the quality of the\nontology under development. It can provide suggestions on the naming\nconventions to follow, vocabulary to reuse, ODPs to implement, and axioms to be\nadded to the ontology. OntoSeer has been implemented as a Prot\\'eg\\'e plug-in.", "entities_include_in_text": ["Apr 2014"], "entities_from_reference": ["Bhattacharyya", "Mutharaju", "Bhattachayya", "Indraprastha Institute", "Cahyono", "Science", "Dreler", "Ngonga Ngomo", "Bounded Jaro Winkler Distances", "Garc", "Hitzler", "Janowicz", "Zaveri", "Gray", "Lopez", "Haller", "Springer International Publishing", "Angele", "Knowledge Management", "Guarino", "Staab", "Springer Berlin Heidelberg", "Hammar", "Ontology Design Patterns", "Gangemi", "Krisnadhi", "Le", "Mikolov", "Machine Learning", "Noy", "Ontology Evaluation", "Ren", "Deemter", "Stevens"]}{"title": ["Generative Modeling of Complex Data"], "authors": ["[arxiv.Result.Author('Luca Canale'), arxiv.Result.Author('Nicolas Grislain'), arxiv.Result.Author('Gr\u00e9goire Lothe'), arxiv.Result.Author('Johan Leduc')]"], "link": ["http://arxiv.org/pdf/2202.02145v1"], "summary": "In recent years, several models have improved the capacity to generate\nsynthetic tabular datasets. However, such models focus on synthesizing simple\ncolumnar tables and are not useable on real-life data with complex structures.\nThis paper puts forward a generic framework to synthesize more complex data\nstructures with composite and nested types. It then proposes one practical\nimplementation, built with causal transformers, for struct (mappings of types)\nand lists (repeated instances of a type). The results on standard benchmark\ndatasets show that such implementation consistently outperforms current\nstate-of-the-art models both in terms of machine learning utility and\nstatistical similarity. Moreover, it shows very strong results on two complex\nhierarchical datasets with multiple nesting and sparse data, that were\npreviously out of reach.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Implementation of a Type-2 Fuzzy Logic Based Prediction System for the Nigerian Stock Exchange"], "authors": ["[arxiv.Result.Author('Isobo Nelson Davies'), arxiv.Result.Author('Donald Ene'), arxiv.Result.Author('Ibiere Boma Cookey'), arxiv.Result.Author('Godwin Fred Lenu')]"], "link": ["http://arxiv.org/pdf/2202.02107v1"], "summary": "Stock Market can be easily seen as one of the most attractive places for\ninvestors, but it is also very complex in terms of making trading decisions.\nPredicting the market is a risky venture because of the uncertainties and\nnonlinear nature of the market. Deciding on the right time to trade is key to\nevery successful trader as it can lead to either a huge gain of money or\ntotally a loss in investment that will be recorded as a careless trade. The aim\nof this research is to develop a prediction system for stock market using Fuzzy\nLogic Type2 which will handle these uncertainties and complexities of human\nbehaviour in general when it comes to buy, hold or sell decision making in\nstock trading. The proposed system was developed using VB.NET programming\nlanguage as frontend and Microsoft SQL Server as backend. A total of four\ndifferent technical indicators were selected for this research. The selected\nindicators are the Relative Strength Index, William Average, Moving Average\nConvergence and Divergence, and Stochastic Oscillator. These indicators serve\nas input variable to the Fuzzy System. The MACD and SO are deployed as primary\nindicators, while the RSI and WA are used as secondary indicators. Fibonacci\nretracement ratio was adopted for the secondary indicators to determine their\nsupport and resistance level in terms of making trading decisions. The input\nvariables to the Fuzzy System is fuzzified to Low, Medium, and High using the\nTriangular and Gaussian Membership Function. The Mamdani Type Fuzzy Inference\nrules were used for combining the trading rules for each input variable to the\nfuzzy system. The developed system was tested using sample data collected from\nten different companies listed on the Nigerian Stock Exchange for a total of\nfifty two periods. The dataset collected are Opening, High, Low, and Closing\nprices of each security.", "entities_include_in_text": ["Ahmed et al., 2007", "Rajendran  et  al.,  2014", "Suanu et al., 2012", "Ahmed et al., 2007", "Zhang et al., 2018", "Ijegwa et al., 2014", "Zadeh,  1975", "Kamath,  2012", "Marques et al., 2010", "AAMAS 2011"], "entities_from_reference": ["Raaffat", "Stock Technical Analysis", "Multi Agent", "Fuzzy", "J. Chinnappan", "Stock Market", "Applied Information Technology", "Zhang", "Wang", "Fang", "Indian Stock Market", "Machine Learning", "Nigerian Stock Market", "Issue", "Fuzzy Membership Functionon Fuzzified RIPPER", "Paper", "Soft Computing", "Trendy Eknomiky", "Stock Market Analysis", "Mater", "San Jose State University", "Hybrid", "Berlin Heidelberg", "Crnkovic", "Dennis", "System Analysis", "John Wiley", "Sons", "Inc.", "Escobar", "J. Moreno", "Vol", "Fuzzy Logic", "Steele Ed", "Canada", "Social", "Cengage Learning.86", "Jiang", "Fuzzy Logic System", "Stock Index Forecasting", "Fuzzy Time Series", "Stock", "Modern System Analysis", "Roberts Ed", "Third Edition", "Pearson Education", "Kenneth", "Systems Analysis", "Robert Horan", "John Roberts Ed", "Fifth Edition", "Neuro Fuzzy", "Techniques", "Stock Trends", "Applied Software Architecture", "Jacobson", "Software", "Economic", "Economic Cybernetics Studies", "Marques", "Artificial Counselor System", "Stock Investment", "Fuzzy Logic Relation", "Global Journal", "Applied Mathematics", "Dual Time Frame Relative Strength Stock", "Hong Kong", "Stock Price Volatility Perception", "Stock Brokers", "Artificial Intelligence", "Kataria", "Fuzzy System", "Software Engineering", "Applications"]}{"title": ["Fixed-Point Code Synthesis For Neural Networks"], "authors": ["[arxiv.Result.Author('Hanane Benmaghnia'), arxiv.Result.Author('Matthieu Martel'), arxiv.Result.Author('Yassamine Seladji')]"], "link": ["http://arxiv.org/pdf/2202.02095v1"], "summary": "Over the last few years, neural networks have started penetrating safety\ncritical systems to take decisions in robots, rockets, autonomous driving car,\netc. A problem is that these critical systems often have limited computing\nresources. Often, they use the fixed-point arithmetic for its many advantages\n(rapidity, compatibility with small memory devices.) In this article, a new\ntechnique is introduced to tune the formats (precision) of already trained\nneural networks using fixed-point arithmetic, which can be implemented using\ninteger operations only. The new optimized neural network computes the output\nwith fixed-point numbers without modifying the accuracy up to a threshold fixed\nby the user. A fixed-point code is synthesized for the new optimized neural\nnetwork ensuring the respect of the threshold for any input vector belonging\nthe range [xmin, xmax] determined during the analysis. From a technical point\nof view, we do a preliminary analysis of our floating neural network to\ndetermine the worst cases, then we generate a system of linear constraints\namong integer variables that we can solve by linear programming. The solution\nof this system is the new fixed-point format of each neuron. The experimental\nresults obtained show the efficiency of our method which can ensure that the\nnew fixed-point neural network has the same behavior as the initial\nfloating-point neural network.", "entities_include_in_text": [], "entities_from_reference": ["Abraham", "Aeberhard", "James Cook University", "Albawi", "Mohammed", "Becv", "Stukjunger", "Acta Polytechnica", "Catrina", "Hoogh", "Dicle Medical", "Chen", "Zhou", "Anchorage", "Dutta", "Jha", "Sankaranarayanan", "Enderich", "Timm", "Rosenbaum", "Burgard", "Gehr", "Mirman", "Tsankov", "Chaudhuri", "Vechev", "Gopinath", "Han", "Zhi", "Wang", "Liu", "Parallel Program", "Ioualalen", "Martel", "Jin", "Liang", "Tao", "Joseph", "Gopalakrishnan", "Garg", "Katz", "Barrett", "Dill", "Lauter", "Volkova", "Lin", "Talathi", "Annapureddy", "Weinberger", "Machine Learning", "Lopez", "Pierre", "Marie Curie University", "Moura", "Bjrner", "Ramakrishnan", "Rehof", "Tools", "Held", "Najahi", "Perpignan", "Sharma", "Towards Data", "Singh", "Huang", "Sharp", "Hill", "Swain", "Dash", "Mohapatra", "Tran", "Yang", "Musau", "Xiang", "Bak", "Johnson", "Urban", "Min", "Welke", "Bauckhage", "Tech", "Technical Report", "Wolberg", "Street", "Numerical Accuracy", "Abstract Interpretation", "Code Transformations", "Synthesis", "Safety", "Frugal Computing", "Seladji", "Ecole Polytechnique", "Currently", "Static Analysis"]}{"title": ["Beam Management with Orientation and RSRP using Deep Learning for Beyond 5G Systems"], "authors": ["[arxiv.Result.Author('Khuong N. Nguyen'), arxiv.Result.Author('Anum Ali'), arxiv.Result.Author('Jianhua Mo'), arxiv.Result.Author('Boon Loong Ng'), arxiv.Result.Author('Vutha Va'), arxiv.Result.Author('Jianzhong Charlie Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02247v1"], "summary": "Beam management (BM), i.e., the process of finding and maintaining a suitable\ntransmit and receive beam pair, can be challenging, particularly in highly\ndynamic scenarios. Side-information, e.g., orientation, from on-board sensors\ncan assist the user equipment (UE) BM. In this work, we use the orientation\ninformation coming from the inertial measurement unit (IMU) for effective BM.\nWe use a data-driven strategy that fuses the reference signal received power\n(RSRP) with orientation information using a recurrent neural network (RNN).\nSimulation results show that the proposed strategy performs much better than\nthe conventional BM and an orientation-assisted BM strategy that utilizes\nparticle filter in another study. Specifically, the proposed data-driven\nstrategy improves the beam-prediction accuracy up to 34% and increases mean\nRSRP by up to 4.2 dB when the UE orientation changes quickly.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Identifying Self-Admitted Technical Debt in Issue Tracking Systems using Machine Learning"], "authors": ["[arxiv.Result.Author('Yikun Li'), arxiv.Result.Author('Mohamed Soliman'), arxiv.Result.Author('Paris Avgeriou')]"], "link": ["http://arxiv.org/pdf/2202.02180v1"], "summary": "Technical debt is a metaphor indicating sub-optimal solutions implemented for\nshort-term benefits by sacrificing the long-term maintainability and\nevolvability of software. A special type of technical debt is explicitly\nadmitted by software engineers (e.g. using a TODO comment); this is called\nSelf-Admitted Technical Debt or SATD. Most work on automatically identifying\nSATD focuses on source code comments. In addition to source code comments,\nissue tracking systems have shown to be another rich source of SATD, but there\nare no approaches specifically for automatically identifying SATD in issues. In\nthis paper, we first create a training dataset by collecting and manually\nanalyzing 4,200 issues (that break down to 23,180 sections of issues) from\nseven open-source projects (i.e., Camel, Chromium, Gerrit, Hadoop, HBase,\nImpala, and Thrift) using two popular issue tracking systems (i.e., Jira and\nGoogle Monorail). We then propose and optimize an approach for automatically\nidentifying SATD in issue tracking systems using machine learning. Our findings\nindicate that: 1) our approach outperforms baseline approaches by a wide margin\nwith regard to the F1-score; 2) transferring knowledge from suitable datasets\ncan improve the predictive performance of our approach; 3) extracted SATD\nkeywords are intuitive and potentially indicating types and indicators of SATD;\n4) projects using different issue tracking systems have less common SATD\nkeywords compared to projects using the same issue tracking system; 5) a small\namount of training data is needed to achieve good accuracy.", "entities_include_in_text": ["Avgeriou et al., 2016", "Li\net al., 2015", "Alves et al., 2016", "Tufano et al., 2017", "Ernst, 2012)", "Potdar and Shihab, 2014", "Potdar and Shihab, 2014", "Sierra\net al., 2019", "Li\net al., 2020; Bellomo et al., 2016", "Merten et al., 2015; Li et al., 2020", "Li et al., 2020", "Li\net al., 2020", "Kim, 2014", "Li et al., 2020", "van Solingen et al., 2002", "Shalev-Shwartz and Ben-David, 2014", "Potdar and Shihab, 2014; Wehaibi et al., 2016; Zampetti et al., 2018", "Smith, 2018", "Potdar and Shihab, 2014; Wehaibi et al., 2016; d. S. Maldon-\nado et al., 2017", "Li et al., 2020", "Alves\net al., 2014", "Fleiss et al., 1981", "Sun et al., 2009", "McCallum et al., 1998", "Tan, 2006", "Genkin et al., 2007", "Xu et al., 2012", "Kowsari et al., 2019", "d. S. Mal-\ndonado et al., 2017; Huang et al., 2018; Flisar and Podgorelec, 2019", "Yao et al., 2019", "Yao et al., 2019", "Ren et al.,\n2019", "Bavota\nand Russo, 2016", "Wei and Zou, 2019", "Phan et al., 2017; Ren\net al., 2019", "Joulin et al., 2017; Wieting et al., 2015", "Mikolov et al., 2018", "Kim, 2014", "see Potdar and Shihab (2014", "Mikolov et al., 2018", "Bo-\njanowski et al., 2017", "Zhang and Wallace, 2017", "Zhang and Wallace, 2017", "Semwal et al., 2018", "d. S. Maldonado et al.,\n2017", "Zhang et al.,\n2015", "Semwal\net al., 2018", "Ortu et al., 2016; Calefato et al., 2018", "Perkins et al., 1992", "see\nPotdar and Shihab (2014)", "Li et al., 2020", "Gu\net al., 2014", "HBase-1990", "see Potdar and Shihab (2014)", "d. S. Mal-\ndonado et al., 2017; Flisar and Podgorelec, 2019; Ren et al., 2019", "d. S. Maldonado et al., 2017; Huang et al., 2018", "Zhang and Wallace, 2017", "Semwal et al., 2018", "LREC 2018)\nOrtu M, Murgia A, Destefanis G, Tourani P, Tonelli R, Marchesi M, Adams\nB (2016"], "entities_from_reference": ["Ribeiro LF", "Caires V", "Mendes TS", "Mendon", "Sp", "Shull F", "Seaman C", "Kruchten P", "Ozkaya", "Dagstuhl", "Russo B", "Nord RL", "Popeck M", "Bojanowski P", "Grave E", "Joulin A", "Mikolov T", "Calefato F", "Lanubile F", "Maiorano F", "Novielli N", "Empirical Software", "Dai K", "Efstathiou V", "Chatzilenas C", "Spinellis D", "Fern", "Garc", "Galar M", "Prati RC", "Krawczyk B", "Herrera F", "Springer Fleiss JL", "Levin B", "Paik MC", "Flisar J", "Podgorelec V", "Freitas Farias MA", "Santos JA", "Kalinowski M", "Springer", "Lewis DD", "Madigan D", "Gu L", "Eils R", "Schlesner M", "Brors B", "Huang Q", "Shihab E", "Xia X", "Lo D", "Li S", "Short Papers", "Maldonado EdS", "Ubayashi N", "Kim Y", "Kowsari K", "Jafari Meimandi K", "Heidarysafa M", "Mendu S", "Barnes L", "Brown D", "Li Y", "Soliman M", "Avgeriou P", "Advanced Applications", "Machine Learning", "Liang P", "Nigam K", "Mager B", "Quirchmayr T", "Paech B", "Puhrsch C", "Ortu M", "Murgia A", "Destefanis G", "Tourani P", "Tonelli R", "Marchesi M", "Adams B", "Perkins DN", "Salomon G", "Gerkmann T", "Potdar A", "Ren X", "Wang X", "Grundy J", "Runeson P", "Rainer A", "Regnell B", "John Wiley", "S Maldonado E", "Maldonado E", "Tsantalis N", "Semwal T", "Yenigalla P", "Mathur G", "Nair SB", "Data Mining", "Ben-David S", "Cambridge", "Sierra G", "Kamei Y", "Smith T", "Solingen R", "Basili V", "Caldiera G", "Rombach HD", "Goal Question Metric", "Sons", "Hoboken", "Steidl D", "Hummel B", "Juergens E", "Ieee", "Lim EP", "Liu Y", "Tan S", "Tufano M", "Palomba F", "Bavota G", "Oliveto R", "Di Penta M", "De Lucia A", "Poshyvanyk D", "Wehaibi S", "Guerrouj L", "Evolution", "Zou K", "Bansal M", "Gimpel K", "Livescu K", "Xavier L", "Ferreira F", "Brito R", "Valente MT", "Guo X", "Ye Y", "Cheng J", "Yao L", "Mao C", "Luo Y", "Serebrenik A", "Di Penta", "Zhang X", "Zhao J", "Zhang Y", "Wallace BC"]}{"title": ["Urban Region Profiling via A Multi-Graph Representation Learning Framework"], "authors": ["[arxiv.Result.Author('Y. Luo'), arxiv.Result.Author('F. Chung'), arxiv.Result.Author('K. Chen')]"], "link": ["http://arxiv.org/pdf/2202.02074v1"], "summary": "Urban region profiling can benefit urban analytics. Although existing studies\nhave made great efforts to learn urban region representation from multi-source\nurban data, there are still three limitations: (1) Most related methods focused\nmerely on global-level inter-region relations while overlooking local-level\ngeographical contextual signals and intra-region information; (2) Most previous\nworks failed to develop an effective yet integrated fusion module which can\ndeeply fuse multi-graph correlations; (3) State-of-the-art methods do not\nperform well in regions with high variance socioeconomic attributes. To address\nthese challenges, we propose a multi-graph representative learning framework,\ncalled Region2Vec, for urban region profiling. Specifically, except that human\nmobility is encoded for inter-region relations, geographic neighborhood is\nintroduced for capturing geographical contextual information while POI side\ninformation is adopted for representing intra-region information by knowledge\ngraph. Then, graphs are used to capture accessibility, vicinity, and\nfunctionality correlations among regions. To consider the discriminative\nproperties of multiple graphs, an encoder-decoder multi-graph fusion module is\nfurther proposed to jointly learn comprehensive representations. Experiments on\nreal-world datasets show that Region2Vec can be employed in three applications\nand outperforms all state-of-the-art baselines. Particularly, Region2Vec has\nbetter performance than previous studies in regions with high variance\nsocioeconomic attributes.", "entities_include_in_text": [], "entities_from_reference": ["Chang", "Li", "Wang", "Transportation Research Part", "Leskovec", "Liu", "Zhao", "Kadavankandy", "Avrachenkov", "Kamper", "Livescu", "Katsaggelos", "Kipf", "Zhang", "Perozzi", "Online", "Data Mining", "Machinery", "Tang", "Shang", "Ren", "Han", "Title", "Due", "Excessive Length", "Bui", "Hsieh", "Yan", "Mei", "Line", "Tibshirani", "Tobler", "Economic Geography", "Vaswani", "Shazeer", "Uszkoreit", "Jones", "Gomez", "Polosukhin", "Red Hook", "Velickovi", "Casanova", "Bengio", "Yao", "Cao", "Xie", "Tao", "Yuan", "Peng", "Zheng", "Hanratty", "Hui", "Zhu", "Huang", "Zhou"]}{"title": ["Deep invariant networks with differentiable augmentation layers"], "authors": ["[arxiv.Result.Author('C\u00e9dric Rommel'), arxiv.Result.Author('Thomas Moreau'), arxiv.Result.Author('Alexandre Gramfort')]"], "link": ["http://arxiv.org/pdf/2202.02142v1"], "summary": "Designing learning systems which are invariant to certain data\ntransformations is critical in machine learning. Practitioners can typically\nenforce a desired invariance on the trained model through the choice of a\nnetwork architecture, e.g. using convolutions for translations, or using data\naugmentation. Yet, enforcing true invariance in the network can be difficult,\nand data invariances are not always known a piori. State-of-the-art methods for\nlearning data augmentation policies require held-out data and are based on\nbilevel optimization problems, which are complex to solve and often\ncomputationally demanding. In this work we investigate new ways of learning\ninvariances only from the training data. Using learnable augmentation layers\nbuilt directly in the network, we demonstrate that our method is very\nversatile. It can incorporate any type of differentiable augmentation and be\napplied to a broad class of learning problems beyond computer vision. We\nprovide empirical evidence showing that our approach is easier and faster to\ntrain than modern automatic data augmentation techniques based on bilevel\noptimization, while achieving comparable results. Experiments show that while\nthe invariances transferred to a model through automatic data augmentation are\nlimited by the model expressivity, the invariance yielded by our approach is\ninsensitive to it by design.", "entities_include_in_text": ["LeCun et al., 1989", "Krizhevsky et al., 2012", "Zhou\net al., 2021", "Chambon et al., 2018; Phan et al.,\n2021", "Cubuk et al., 2019", "Cubuk\net al., 2019", "Ho et al., 2019", "Lim et al., 2019", "Hataya et al., 2020", "Li\net al., 2020", "Rommel et al., 2021", "Liu\net al., 2019", "Zaheer et al., 2017)\nencode permutation invariance by summing networks pre-\ndictions. Although related, these methods are not designed\nfor learning symmetries from the data, which is the objective\nof this study.\n\nPrior work on this matter from van der Wilk et al. (2018) pro-\nposes to learn invariances using the marginal likehood in the\ncontext of Gaussian processes. In contrast, we are mostly fo-\ncused on deep neural networks. Zhou et al. (2021", "Benton\net al., 2020", "cf. Chen et al. (2020", "Schulman\net al., 2015", "Bengio et al., 2013", "Grathwohl\net al., 2018", "Benton et al., 2020", "Benton et al., 2020", "Benton et al.,\n2020", "Rommel et al.,\n2021", "Schwabedal et al., 2019", "Wang et al., 2018", "Howard,\n2014", "Inoue, 2018)", "Iber\net al., 2007", "Krizhevsky et al., 2009", "Benton et al., 2020", "Chambon et al.,\n2018", "Cubuk et al., 2019", "Lim\net al., 2019", "Hataya et al., 2020", "Li et al., 2020", "Rommel et al., 2021", "Benton et al., 2020", "medium according to Benton et al. (2020)", "Schirrmeister et al., 2017", "Akiba et al., 2019", "Riba\net al., 2020", "Hataya et al., 2020", "Chambon\net al., 2018", "Gramfort et al.,\n2013", "Schirrmeister et al.,\n2017", "Chambon et al., 2018"], "entities_from_reference": ["Ohta", "Data Mining", "Bengio", "Benton", "Finzi", "Izmailov", "Wilson", "Bouchacourt", "Ibrahim", "Chambon", "Galtier", "Wainrib", "Chen", "Lee", "J.", "Data Augmentation", "Hsieh", "Machine Learning", "Cohen", "Cubuk", "Zoph", "Mane", "Vasudevan", "Le", "Pattern Recognition", "Gramfort", "Larson", "Strohmeier", "Goj", "Jas", "Grathwohl", "Choi", "Roeder", "Duvenaud", "Hataya", "Springer International Publishing", "Zhang", "Ren", "Deep", "Liang", "Population", "Howard", "Chesson", "Quan", "Terminology", "Inoue", "Kingma", "J. Adam", "Krizhevsky", "Denker", "J. S.", "Henderson", "Hubbard", "Jackel", "Wang", "Robertson", "Yang", "Lim", "Kim", "Zela", "Elsken", "Marrakchi", "Brox", "Hutter", "Chang", "Abbasnejad", "Haffari", "Stochastic Implicit Gradients", "Zhou", "Knowles", "Liu", "Simonyan", "Gosselin", "Carrier", "Nielsen", "Phan", "Koch", "Mertins", "De Vos", "Pattern Analysis", "Machine Intelligence", "Riba", "Mishkin", "Bradski", "Source Differentiable Computer", "Rommel", "Paillard", "J. T.", "Glasstetter", "Eggensperger", "Tangermann", "Burgard", "Human Brain Mapping", "Schulman", "Heess", "Schwabedal", "J. T. C.", "J. C.", "Nemati", "Clifford", "Noisy Signals", "Fourier Transform Surrogates", "Wilk", "Bauer", "John", "Hensman", "Peng", "Jiang", "Deep Convolutional Neural Networks", "Series Title", "Zaheer", "Salakhutdinov", "Augerino", "Conv2D ReLU Conv2D", "Figure", "Sinusoids", "Faster AA", "Akiba", "Faster", "Adam", "Balanced", "Tesla V100", "Dense", "Softmax Table", "Frequency Sensors", "Figure B.2", "Loss", "Random"]}