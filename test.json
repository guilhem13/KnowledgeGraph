{"title": ["Neural Models for Output-Space Invariance in Combinatorial Problems"], "authors": ["[arxiv.Result.Author('Yatin Nandwani'), arxiv.Result.Author('Vidit Jain'), arxiv.Result.Author('Mausam'), arxiv.Result.Author('Parag Singla')]"], "link": ["http://arxiv.org/pdf/2202.03229v1"], "summary": "Recently many neural models have been proposed to solve combinatorial puzzles\nby implicitly learning underlying constraints using their solved instances,\nsuch as sudoku or graph coloring (GCP). One drawback of the proposed\narchitectures, which are often based on Graph Neural Networks (GNN), is that\nthey cannot generalize across the size of the output space from which variables\nare assigned a value, for example, set of colors in a GCP, or board-size in\nsudoku. We call the output space for the variables as 'value-set'. While many\nworks have demonstrated generalization of GNNs across graph size, there has\nbeen no study on how to design a GNN for achieving value-set invariance for\nproblems that come from the same domain. For example, learning to solve 16 x 16\nsudoku after being trained on only 9 x 9 sudokus. In this work, we propose\nnovel methods to extend GNN based architectures to achieve value-set\ninvariance. Specifically, our model builds on recently proposed Recurrent\nRelational Networks. Our first approach exploits the graph-size invariance of\nGNNs by converting a multi-class node classification problem into a binary node\nclassification problem. Our second approach works directly with multiple\nclasses by adding multiple nodes corresponding to the values in the value-set,\nand then connecting variable nodes to value nodes depending on the problem\ninitialization. Our experimental evaluation on three different combinatorial\nproblems demonstrates that both our models perform well on our novel problem,\ncompared to a generic neural reasoner. Between two of our models, we observe an\ninherent trade-off: while the binarized model gives better performance when\ntrained on smaller value-sets, multi-valued model is much more memory\nefficient, resulting in improved performance when trained on larger value-sets,\nwhere binarized model fails to train.", "entities_include_in_text": ["Zhou et al., 2020", "Palm et al., 2018", "Kahneman, 2011", "Dong et al., 2019", "Palm et al., 2018; Wang et al., 2019", "Palm et al., 2018", "Selsam\net al., 2019", "Selsam et al., 2019", "Dong et al., 2019", "Manhaeve et al., 2018", "Dong et al.,\n2019", "Tamar et al., 2017; Bajpai et al., 2018", "Selsam et al., 2019", "Manhaeve et al., 2018", "Wang et al., 2019", "Palm et al., 2018", "Garg et al., 2020", "Dong et al., 2019", "Nandwani et al., 2021), and are thus directly comparable to our work. The main challenge of\nsuch approaches is that they fail to scale to the size of the problems considered in this work. In our\nexperiments, we compare our methods against both deep and shallow versions of NLM. Note that our\nwork relies on the assumption that GNNs generalize across graph sizes. Yehudai et al. (2021", "de Kleer, 1989; Walsh, 2000", "Palm et al., 2018", "Dong\net al., 2019", "Pieters,\n2019", "Izmailov et al., 2018", "Nandwani et al., 2021", "AAAI-MAKE 2019", "Bahdanau et al., 2015", "Palm\net al., 2018", "Selsam et al., 2019", "Palm et al., 2018", "Palm et al., 2018", "Pieters, 2019", "Nandwani et al., 2021", "Izmailov et al., 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["Personalized Public Policy Analysis in Social Sciences using Causal-Graphical Normalizing Flows"], "authors": ["[arxiv.Result.Author('Sourabh Balgi'), arxiv.Result.Author('Jose M. Pena'), arxiv.Result.Author('Adel Daoud')]"], "link": ["http://arxiv.org/pdf/2202.03281v1"], "summary": "Structural Equation/Causal Models (SEMs/SCMs) are widely used in epidemiology\nand social sciences to identify and analyze the average treatment effect (ATE)\nand conditional ATE (CATE). Traditional causal effect estimation methods such\nas Inverse Probability Weighting (IPW) and more recently\nRegression-With-Residuals (RWR) are widely used - as they avoid the challenging\ntask of identifying the SCM parameters - to estimate ATE and CATE. However,\nmuch work remains before traditional estimation methods can be used for\ncounterfactual inference, and for the benefit of Personalized Public Policy\nAnalysis (P$^3$A) in the social sciences. While doctors rely on personalized\nmedicine to tailor treatments to patients in laboratory settings (relatively\nclosed systems), P$^3$A draws inspiration from such tailoring but adapts it for\nopen social systems. In this article, we develop a method for counterfactual\ninference that we name causal-Graphical Normalizing Flow (c-GNF), facilitating\nP$^3$A. First, we show how c-GNF captures the underlying SCM without making any\nassumption about functional forms. Second, we propose a novel dequantization\ntrick to deal with discrete variables, which is a limitation of normalizing\nflows in general. Third, we demonstrate in experiments that c-GNF performs\non-par with IPW and RWR in terms of bias and variance for estimating the ATE,\nwhen the true functional forms are known, and better when they are unknown.\nFourth and most importantly, we conduct counterfactual inference with c-GNFs,\ndemonstrating promising empirical performance. Because IPW and RWR, like other\ntraditional methods, lack the capability of counterfactual inference, c-GNFs\nwill likely play a major role in tailoring personalized treatment, facilitating\nP$^3$A, optimizing social interventions - in contrast to the current\n`one-size-fits-all' approach of existing methods.", "entities_include_in_text": ["Wright 1921; Fisher 1936", "Wright 1921", "Haavelmo 1943; Gold-\nberger 1972", "King 1974; Ploch,\nGoldberger, and Duncan 1975; Fienberg and Duncan 1975", "Kino et al. 2021", "Wodtke 2020", "Pearl 2009b,\n2012", "Bang and Robins 2005", "Wodtke 2020", "Kino et al. 2021", "GeoLytics 2003", "Pearl 2012", "Ru-\nbin 1990", "Rosenbaum and Rubin 1983", "Cox 1958", "Tabak and Vanden-Eijnden 2010;\nTabak and Turner 2013; Rezende and Mohamed 2015;\nKobyzev, Prince, and Brubaker 2020; Papamakarios et al.\n2021", "Milnor and Weaver 1997", "Kingma et al. 2016; Papamakar-\nios, Murray, and Pavlakou 2017; Huang et al. 2018", "Pa-\npamakarios et al. 2021", "Huang et al. 2018", "Wehenkel and Louppe 2021", "We-\nhenkel and Louppe 2019", "Uria, Murray, and Larochelle 2013;\nHoogeboom et al. 2019; Tran et al. 2019; Ho et al. 2019;\nZiegler and Rush 2019; Ma et al. 2019; Nielsen and Winther\n2020; Pawlowski, de Castro, and Glocker 2020", "Paszke et al.\n2017", "Loshchilov\nand Hutter 2019"], "entities_from_reference": ["Robins", "J.", "Causal Inference Models", "Cole", "Cox", "Wiley", "Fienberg", "Fisher", "Brunswick", "Goldberger", "Econometrica", "Haavelmo", "Hern", "Causal Inference", "Boca Raton", "Architecture Design", "Machine Learning", "Hoogeboom", "J. W.", "Berg", "Huang", "Krueger", "Lacoste", "Neural Autoregressive Flows", "Khemakhem", "Monti", "Leech", "Hyvarinen", "Causal Autoregressive Flows", "Kingma", "Inverse Autoregressive Flow", "Kino", "Hsu", "Mita", "Daoud", "Review", "Research Prospects", "Social Science", "Kobyzev", "Pattern Analysis", "Machine Intelligence", "Sekhon", "J. S.", "Heterogeneous Treatment", "Loshchilov", "Hutter", "Weight Decay", "Zhang", "Hovy", "Milnor", "Princeton", "Neal", "Causal", "Nielsen", "Winther", "Papamakarios", "Murray", "Autoregressive Flow", "Density Estimation", "Nalisnick", "Mohamed", "Lakshminarayanan", "Flows", "Machine Learning Research", "Paszke", "Gross", "Chanan", "Yang", "Lin", "Desmaison", "Lerer", "Pawlowski", "Castro", "Glocker", "Deep Structural Causal Models", "Wodtke", "Sociological Methods", "Elwert", "Wright", "Ziegler", "Pearl", "Cambridge University Press", "Basic Books", "Ploch", "Rezende", "Mathematical Modelling", "Rosenbaum", "Rubin", "Biometrika", "Tabak", "Applied Mathematics", "Dual Ascent", "Tran", "Agrawal", "Poole", "Discrete Data", "Uria", "Neural Information", "Epidemiology", "Effect Modification", "Wehenkel", "Louppe", "Monotonic Neural Networks"]}{"title": ["Mental Disorders on Online Social Media Through the Lens of Language and Behaviour: Analysis and Visualisation"], "authors": ["[arxiv.Result.Author('Esteban A. R\u00edssola'), arxiv.Result.Author('Mohammad Aliannejadi'), arxiv.Result.Author('Fabio Crestani')]"], "link": ["http://arxiv.org/pdf/2202.03291v1"], "summary": "Due to the worldwide accessibility to the Internet along with the continuous\nadvances in mobile technologies, physical and digital worlds have become\ncompletely blended, and the proliferation of social media platforms has taken a\nleading role over this evolution. In this paper, we undertake a thorough\nanalysis towards better visualising and understanding the factors that\ncharacterise and differentiate social media users affected by mental disorders.\nWe perform different experiments studying multiple dimensions of language,\nincluding vocabulary uniqueness, word usage, linguistic style, psychometric\nattributes, emotions' co-occurrence patterns, and online behavioural traits,\nincluding social engagement and posting trends. Our findings reveal significant\ndifferences on the use of function words, such as adverbs and verb tense, and\ntopic-specific vocabulary, such as biological processes. As for emotional\nexpression, we observe that affected users tend to share emotions more\nregularly than control individuals on average. Overall, the monthly posting\nvariance of the affected groups is higher than the control groups. Moreover, we\nfound evidence suggesting that language use on micro-blogging platforms is less\ndistinguishable for users who have a mental disorder than other less\nrestrictive platforms. In particular, we observe on Twitter less quantifiable\ndifferences between affected and control groups compared to Reddit.", "entities_include_in_text": ["Correia et al.,\n2020", "Zarrinkalam et al., 2018", "Khodabakhsh et al., 2018; Saha et al., 2021", "Prieto et al., 2014", "Culpepper et al.,\n2018", "Losada et al., 2018, 2019", "Fast et al., 2016", "Schwartz et al., 2013", "Association, 2013", "Fast et al., 2016", "Plutchik, 1980", "Roberts et al., 2012", "Losada et al.,\n2018, 2019", "Kloumann et al., 2012", "Dodds et al., 2015", "Bathina et al., 2021", "Mikolov et al., 2013", "Devlin et al., 2018", "Gkoumas et al.,\n2021", "swirl 2018"], "entities_from_reference": ["Johnstone", "Clinical Psychological Science", "Aliannejadi", "Crestani", "Venue", "Amini", "Towards", "Saarbr", "Aragon", "Montes", "Affective Computing", "Diagnostic", "Bathina", "Rutter", "Bollen", "Nature Human Behaviour", "Boyd", "Wilson", "Pennebaker", "J. W.", "Burdisso", "Errecalde", "Applications", "Fernandez", "Novoa", "Carneiro", "Choudhury", "Choi", "Knowledge Management", "Maui", "Gamon", "Horvitz", "Weblogs", "Kiciman", "Dredze", "Kumar", "Systems", "San Jose", "Chung", "Psychology Press", "Coppersmith", "Harman", "Clinical Psychology", "Linguistic Signal", "Clinical Reality", "Ann Arbor", "Correia", "Wood", "Rocha", "Annual Review", "Culpepper", "J. S.", "Diaz", "Smucker", "Chang", "Lee", "Toutanova", "Dodds", "Clark", "Desu", "Williams", "J. R.", "J. P.", "Tivnan", "Danforth", "Fast", "Bernstein", "Empath", "Gaur", "Kursuncu", "Sheth", "Daniulaityte", "Pathak", "Gkotsis", "Oellrich", "Hubbard", "Dobson", "Liakata", "Velupillai", "Gkoumas", "Li", "Gligori", "Anderson", "Stanford", "Khodabakhsh", "Fani", "Kloumann", "Losada", "Masood", "Metzler", "Cai", "Hovy", "Human Language Technologies", "Mikolov", "Lake Tahoe", "Mohammad", "Miyazaki", "Nepomnyachiy", "Gelley", "Jiang", "Minkus", "Neuman", "Springer", "Park", "Cha", "Mehl", "Plutchik", "Approaches", "Prieto", "Matos", "Alvarez", "Cacheda", "Oliveira", "J. L.", "Punt", "Velazquez", "Gonfaus", "J. M.", "Gonz", "Lix", "Langer", "Scientific Reports", "Amantea", "Healthcare", "Roberts", "Roach", "Johnson", "Guthrie", "Harabagiu", "Saha", "Seybolt", "Mattingly", "Aledavood", "Konjeti", "Martinez", "Mark", "De Choudhury", "Schwartz", "J. C.", "Ramones", "Agrawal", "Shah", "Seligman", "Ungar", "Skaik", "Tausczik", "Liwc", "Thieme", "Doherty", "Machine", "Leemput", "Scheffer", "Uban", "Chulvi", "Rosso", "Future Generation", "Walsh", "Chaudhry", "Dua", "Goodman", "Kaplan", "Stigma", "Cohan", "Copenhagen", "Zarrinkalam", "Kahani"]}{"title": ["A Robot Web for Distributed Many-Device Localisation"], "authors": ["[arxiv.Result.Author('Riku Murai'), arxiv.Result.Author('Joseph Ortiz'), arxiv.Result.Author('Sajad Saeedi'), arxiv.Result.Author('Paul H. J. Kelly'), arxiv.Result.Author('Andrew J. Davison')]"], "link": ["http://arxiv.org/pdf/2202.03314v1"], "summary": "We show that a distributed network of robots or other devices which make\nmeasurements of each other can collaborate to globally localise via efficient\nad-hoc peer to peer communication. Our Robot Web solution is based on Gaussian\nBelief Propagation on the fundamental non-linear factor graph describing the\nprobabilistic structure of all of the observations robots make internally or of\neach other, and is flexible for any type of robot, motion or sensor. We define\na simple and efficient communication protocol which can be implemented by the\npublishing and reading of web pages or other asynchronous communication\ntechnologies. We show in simulations with up to 1000 robots interacting in\narbitrary patterns that our solution convergently achieves global accuracy as\naccurate as a centralised non-linear factor graph solver while operating with\nhigh distributed efficiency of computation and communication. Via the use of\nrobust factors in GBP, our method is tolerant to a high percentage of faults in\nsensor measurements or dropped communication packets.", "entities_include_in_text": [], "entities_from_reference": ["Robust", "Ceres", "Andersson", "Aragues", "Bryson", "J. Vial", "Barooah", "Caceres", "Hybrid", "J. P. How", "J. Rogers", "Robotics Research", "Davison", "Active Search", "Technical Report", "Georgia Institute", "Annual Review", "Robotics", "Factor Graphs", "Robot Perception", "Mukadam", "Boots", "Motion Planning", "Gaussian Processes", "Applied Iterative Methods", "Academic Press", "J. Yu", "Huang", "Nelson", "Michael", "Multirobot", "Johannsson", "J. Leonard", "Wiel", "Nature", "Kim", "Teller", "Graph Optimization", "Lajoie", "Towards", "Barfoot", "Robotic Systems", "Wang", "J. Kuang", "Signal Processing", "J. Deray", "Burgard", "Asynchronous IEEE", "J. Lien", "Zhang", "J. Dong", "J. Engel"]}{"title": ["Red Teaming Language Models with Language Models"], "authors": ["[arxiv.Result.Author('Ethan Perez'), arxiv.Result.Author('Saffron Huang'), arxiv.Result.Author('Francis Song'), arxiv.Result.Author('Trevor Cai'), arxiv.Result.Author('Roman Ring'), arxiv.Result.Author('John Aslanides'), arxiv.Result.Author('Amelia Glaese'), arxiv.Result.Author('Nat McAleese'), arxiv.Result.Author('Geoffrey Irving')]"], "link": ["http://arxiv.org/pdf/2202.03286v1"], "summary": "Language Models (LMs) often cannot be deployed because of their potential to\nharm users in hard-to-predict ways. Prior work identifies harmful behaviors\nbefore deployment by using human annotators to hand-write test cases. However,\nhuman annotation is expensive, limiting the number and diversity of test cases.\nIn this work, we automatically find cases where a target LM behaves in a\nharmful way, by generating test cases (\"red teaming\") using another LM. We\nevaluate the target LM's replies to generated test questions using a classifier\ntrained to detect offensive content, uncovering tens of thousands of offensive\nreplies in a 280B parameter LM chatbot. We explore several methods, from\nzero-shot generation to reinforcement learning, for generating test cases with\nvarying levels of diversity and difficulty. Furthermore, we use prompt\nengineering to control LM-generated test cases to uncover a variety of other\nharms, automatically finding groups of people that the chatbot discusses in\noffensive ways, personal and hospital phone numbers generated as the chatbot's\nown contact info, leakage of private training data in generated text, and harms\nthat occur over the course of a conversation. Overall, LM-based red teaming is\none promising tool (among many needed) for finding and fixing diverse,\nundesirable LM behaviors before impacting users.", "entities_include_in_text": ["Lee, 2016", "Lin et al.,\n2021", "Carlini et al., 2019, 2021", "Jia and\nLiang, 2017; Dixon et al., 2018; Garg et al., 2019;\nJiang and Bansal, 2019; Ribeiro et al., 2020", "Lee,\n2016", "Rae et al., 2021", "Behjati et al., 2019; Wallace et al.,\n2019", "Szegedy et al., 2014", "for an\noverview, see Xu et al., 2020", "Perez et al., 2021", "Jaques et al., 2017;\nSchmitt et al., 2018; Jaques et al., 2019; Ziegler\net al., 2019", "Holtzman et al., 2020", "Brown et al., 2020", "Sheng et al., 2019; Gehman\net al., 2020; Brown et al., 2020", "Gehman et al., 2020; Welbl\net al., 2021", "Lee, 2016", "Holtzman et al., 2020", "Rae et al., 2021", "Zhu\net al., 2018), as in Holtzman et al. (2020", "Papineni\net al., 2002", "Callison-\nBurch et al., 2006; Liu et al., 2016", "Joulin et al., 2017", "Ateniese et al.,\n2013", "Chen et al., 2021", "Chaudhuri and Monteleoni,\n2009; Rubinstein et al., 2012; Shokri and\nShmatikov, 2015; Abadi et al., 2016). In particular,\nit\nto have secondary mechanisms\nfor verifying that a trained model does not\nleak training data. Additional checks help to\ncatch implementation bugs, as well as to tune\nhyperparameters that trade off data leakage risk\nagainst model performance. Red teaming can also\nbe combined directly with extraction attacks such\nas Carlini et al. (2021", "similar to Brown et al., 2020", "Solaiman\nand Dennison, 2021", "Welleck et al., 2020; Li et al., 2020", "Szegedy et al., 2014; Liu\net al., 2017; Perez et al., 2019", "e.g., Goodfellow et al., 2015;\nEbrahimi et al., 2018", "e.g., Rupprecht et al.,\n2020; Goh et al., 2021", "in the spirit of Zhao et al., 2018", "Szegedy et al., 2014; Liu\net al., 2017; Perez et al., 2019", "Welleck et al., 2020; He and Glass,\n2020", "Welleck et al.,\n2020", "Li et al., 2020", "He and\nGlass, 2020). The target LM may also be trained\nusing RL, as in Saleh et al. (2020", "Zhang et al., 2018). Following Holtzman\net al. (2020", "Gupta et al., 2015", "Shazeer and Stern, 2018", "Gupta et al.,\n2015", "van Hasselt et al., 2016", "Wei et al.,\n2021", "Kingma\nand Ba, 2015"], "entities_from_reference": ["Ilya", "Kunal Talwar", "Martin Abadi", "Andy Chu", "Ian Goodfellow", "Brendan McMahan", "Li Zhang", "Machinery", "Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Mane", "Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "Tom Henighan", "Andy Jones", "Nicholas Joseph", "Ben Mann", "Nova DasSarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Jackson Kernion", "Kamal Ndousse", "Catherine Olsson", "Tom Brown", "Jack Clark", "Sam McCandlish", "Jared Kaplan", "Giuseppe Ateniese", "Giovanni Felici", "Luigi Mancini", "Angelo Spognardi", "Antonio Villani", "Domenico Vitali", "Networks", "Tristan Thrush", "Robin Jia", "Sebastian Riedel", "Pontus Stenetorp", "Douwe Kiela", "Max Bartolo", "Melika Behjati", "Mahdieh Soleymani Baghshah", "Pascal Frossard", "Universal", "Tolga Bolukbasi", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai", "Rishi Bommasani", "Drew A. Hudson", "Ehsan Adeli", "Russ Altman", "Simran Arora", "Sydney", "Michael S. Bernstein", "Jeannette Bohg", "Antoine Bosselut", "Emma Brunskill", "Erik Brynjolfsson", "Shyamal Buch", "Dallas Card", "Rodrigo Castellon", "Niladri Chatterji", "Annie Chen", "Kathleen Creel", "Jared Quincy Davis", "Dora Demszky", "Chris Donahue", "Moussa Doumbouya", "Esin Durmus", "Stefano Ermon", "John Etchemendy", "Kawin Ethayarajh", "Li Fei-Fei", "Chelsea Finn", "Trevor Gale", "Lauren Gillespie", "Karan Goel", "Noah Goodman", "Shelby Grossman", "Neel Guha", "Tatsunori Hashimoto", "Peter Henderson", "John Hewitt", "Daniel E. Ho", "Jenny Hong", "Kyle Hsu", "Jing Huang", "Thomas Icard", "Saahil Jain", "Dan Jurafsky", "Pratyusha Kalluri", "Siddharth Karamcheti", "Geoff Keeling", "Fereshte Khani", "Omar Khattab", "Pang Wei Koh", "Mark Krass", "Ranjay Krishna", "Rohith Kuditipudi", "Ananya Kumar", "Faisal Ladhak", "Mina Lee", "Tony Lee", "Jure Leskovec", "Isabelle Levent", "Xiang Lisa Li", "Xuechen Li", "Tengyu Ma", "Ali Malik", "Christopher D. Manning", "Suvir Mirchandani", "Eric Mitchell", "Zanele Munyikwa", "Suraj Nair", "Avanika Narayan", "Deepak Narayanan", "Ben Newman", "Allen Juan Carlos Niebles", "Hamed Nilforoshan", "Nie", "Julian Nyarko", "Giray Ogut", "Laurel Orr", "Isabel Papadimitriou", "Joon Sung Park", "Chris Piech", "Eva Portelance", "Christopher Potts", "Aditi Raghunathan", "Rob Reich", "Hongyu Ren", "Frieda Rong", "Yusuf Roohani", "Camilo Ruiz", "Jack Ryan", "Christopher Re", "Dorsa Sadigh", "Shiori Sagawa", "Keshav Santhanam", "Andy Shih", "Krishnan Srinivasan", "Alex Tamkin", "Rohan Taori", "Armin W. Thomas", "Florian Tramer", "Rose E. Wang", "William Wang", "Bohan Wu", "Jiajun Wu", "Yuhuai Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan", "Matei Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared D Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Sandhini Agarwal", "Ariel HerbertVoss", "Gretchen Krueger", "Rewon Child", "Aditya Ramesh", "Daniel Ziegler", "Jeffrey Wu", "Clemens Winter", "Chris Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Christopher Berner", "Alec Radford", "Ilya Sutskever", "Miles Osborne", "Philipp Koehn", "Dawn Song", "Carlini", "Chang Liu", "Ulfar Erlingsson", "Jernej Kos", "Nicholas Carlini", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Alina Oprea", "Colin Raffel", "Kamalika Chaudhuri", "Claire Monteleoni", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Oliveira Pinto", "Harri Edwards", "Yuri Burda", "Greg Brockman", "Alex Ray", "Raul Puri", "Michael Petrov", "Heidy Khlaaf", "Pamela Mishkin", "Brooke Chan", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Philippe Tillet", "Felipe Petroski Such", "Dave Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "William Hebgen Guss", "Alex Nichol", "Alex Paino", "Nikolas Tezak", "Jie Tang", "Igor Babuschkin", "Suchir Balaji", "Shantanu Jain", "William Saunders", "Christopher Hesse", "Andrew N. Carr", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Matthew Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Wojciech Zaremba", "Jan Leike", "Justin Lu", "Mia Xu Chen", "Benjamin N. Lee", "Gagan Bansal", "Yuan Cao", "Shuyuan Zhang", "Jackie Tsay", "Yinan Wang", "Andrew M. Dai", "Zhifeng Chen", "Timothy Sohn", "Yonghui Wu", "Gmail", "Data Mining", "Minhao Cheng", "Jinfeng Yi", "Huan Zhang", "Cyprien", "Masson", "Shakir Mohamed", "Mihaela Rosca", "Jack Rae", "Tony Sun", "Varun Kumar", "Satyapriya Krishna", "Yada Pruksachatkun", "Bold", "Dataset", "Rahul Gupta", "Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston", "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman", "Society", "Javid Ebrahimi", "Anyi Rao", "Daniel Lowd", "Melbourne", "Sahaj Garg", "Vincent Perot", "Nicole Limtiaco", "Ankur Taly", "Ed H. Chi", "Alex Beutel", "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi", "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Noah", "Smith", "Gabriel Goh", "Nick Cammarata", "Chelsea Voss", "Shan Carter", "Ludwig Schubert", "Multimodal Distill", "Https", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio", "Szegedy", "Jonathon Shlens", "Christian Explaining", "Ankur Kailash Agrawal", "Gopalakrishnan", "James Glass", "Koustuv Sinha", "Nicolas AngelardGontier", "Nan Rosemary Ke", "Genevieve Fried", "Ryan Lowe", "Joelle Pineau", "Dan Hendrycks", "Sorami Hisamoto", "Matt Post", "Kevin Duh", "Membership Inference Attacks", "Machine Sequence Models", "Sreeram Kannan", "Baosen Zhang", "Radha Poovendran", "Johannes Welbl", "Huang", "Ray Jiang", "Robert Stanforth", "Vishal Maini", "Dani Yogatama", "Pushmeet Kohli", "Natasha Jaques", "Asma Ghandeharioun", "Judy Hanwen Shen", "Craig Ferguson", "Agata Lapedriza", "Noah Jones", "Shixiang Gu", "Rosalind W. Picard", "Dzmitry Bahdanau", "Jose Miguel", "Richard E. Turner", "Douglas Eck", "Machine Learning", "Machine Learning Research", "Denmark", "Copenhagen", "Liwei Jiang", "Jena D. Hwang", "Chandra Bhagavatula", "Ronan Le Bras", "Jon Borchardt", "Jenny Liang", "Oren Etzioni", "Delphi", "Yichen Jiang", "Mohit Bansal", "Annual Meeting QA", "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov", "Short Papers", "Valencia", "Diederik P. Kingma", "Jimmy Ba", "Adam", "Peter Lee", "Margaret Li", "Stephen Roller", "Ilia Kulikov", "Sean Welleck", "Kyunghyun Cho", "Stephanie Lin", "Jacob Hilton", "Owain Evans", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Texas", "Haochen Liu", "Jamell Dacon", "Wenqi Fan", "Hui Liu", "Zitao Liu", "Jiliang Tang", "Tyler Derr", "Zhiwei Wang", "Yanpei Liu", "Xinyun Chen", "Volodymyr Mnih", "Adria Puigdomenech Badia", "Alex Graves", "Timothy Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu", "Milad Nasr", "Reza Shokri", "Amir Houmansadr", "San Francisco", "Yixin Nie", "Adina Williams", "Emily Dinan", "Nicolas Papernot", "Patrick D. McDaniel", "Ian J. Transferability", "Goodfellow", "Ian J. Goodfellow", "Somesh Jha", "Berkay Celik", "Ananthram Swami", "Kishore Papineni", "Salim Roukos", "Todd Ward", "Bleu", "Annual Meeting", "Ethan Perez", "Rob Fergus", "John Mellor", "Jack W. Rae", "Sebastian Borgeaud", "Trevor Cai", "Katie Millican", "Jordan Hoffmann", "Francis Song", "John Aslanides", "Sarah Henderson", "Roman Ring", "Susannah Young", "Eliza Rutherford", "Tom Hennigan", "Jacob Menick", "Albin Cassirer", "Richard Powell", "George", "Driessche", "Lisa Anne Hendricks", "Maribeth Rauh", "Sumanth Dathathri", "Saffron Huang", "Irina Higgins", "Jonathan Uesato", "Antonia Creswell", "Nat McAleese", "Amy Wu", "Erich Elsen", "Elena Buchatskaya", "David Budden", "Esme Sutherland", "Karen Simonyan", "Michela Paganini", "Laurent Sifre", "Lena Martens", "Xiang Lorraine Li", "Adhiguna Kuncoro", "Aida Nematzadeh", "Elena Gribovskaya", "Domenic Donato", "Angeliki Lazaridou", "Arthur Mensch", "Lespiau", "Maria Tsimpoukelli", "Nikolai Grigorev", "Doug Fritz", "Thibault Sottiaux", "Mantas Pajarskas", "Toby Pohlen", "Zhitao Gong", "Daniel Toyama", "Yujia Li", "Tayfun Terzi", "Vladimir Mikulik", "Aidan Clark", "Diego", "Las Casas", "Aurelia Guy", "Chris Jones", "James Bradbury", "Matthew Johnson", "Blake Hechtman", "Laura Weidinger", "Iason Gabriel", "William Isaac", "Ed Lockhart", "Simon Osindero", "Laura Rimell", "Chris Dyer", "Oriol Vinyals", "Kareem Ayoub", "Jeff Stanway", "Lorrayne Bennett", "Demis Hassabis", "Geoffrey Irving", "Sheng", "Premkumar Natarajan", "Nanyun Peng", "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh", "Alexis Ross", "Hao Peng", "Matthew E. Peters", "Matt Gardner", "Tailor", "Paul Rottger", "Bertie Vidgen", "Dong Nguyen", "Zeerak Waseem", "Helen Margetts", "Janet Pierrehumbert", "Benjamin I.", "Peter L. Bartlett", "Ling Huang", "Nina Taft", "Christian Rupprecht", "Cyril Ibrahim", "Christopher J", "Asma Natasha Ghandeharioun", "Judy Shen", "Rosalind Picard", "Jaques", "Simon Schmitt", "Jonathan J. Hudson", "Augustin Zidek", "Carl Doersch", "Wojciech M. Czarnecki", "Joel Z. Leibo", "Heinrich Kuttler", "Andrew Zisserman", "Noam Shazeer", "Vitaly Shmatikov", "Marco Stronati", "Congzheng Song", "Irene Solaiman", "Christy Dennison", "Xinwei Yu", "Karthik Narasimhan", "Nisan Stiennon", "Paul F Christiano", "Christian Szegedy", "Joan Bruna", "Hado", "Hasselt", "Arthur Guez", "Matteo Hessel", "Shi Feng", "Nikhil Kandpal", "Matt Universal Gardner", "Empirical Methods", "Dian Yu", "Kenji Sagae", "Online", "Punta Cana", "Yizhe Zhang", "Michel Galley", "Jianfeng Gao", "Zhe Gan", "Xiujun Li", "Chris Brockett", "Bill Dolan", "Zhao", "Dheeru Dua", "Zhu", "Sidi Lu", "Lei Zheng", "Jiaxian Guo", "Weinan Zhang", "Jun Wang", "Yong Yu", "Texygen", "Daniel M. Ziegler", "Tom B", "Brown", "Douwe Analyzing", "Jason Wei", "Maarten Bosma", "Vincent Y Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M Dai", "Quoc V Le", "Conor Griffin", "Myra Cheng", "Mia Glaese", "Borja Balle", "Atoosa Kasirzadeh", "Zac Kenton", "Sasha Brown", "Will Hawkins", "Tom Stepleton", "Courtney Biles", "Abeba Birhane", "Julia Haas", "Sean Legassick", "Ethical", "Kirsty Anderson", "Ben Coppin", "Jeff Wu", "Nisan", "Paul Stiennon", "Daniel Weld", "Jeffrey Polyjuice", "Heer", "Albert Xu", "Eshaan Pathak", "Dan Klein", "Han Xu", "Yao Ma", "Debayan Deb", "Anil K. Jain", "Da Ju", "Method Replies Qs Diversity", "Unique Entropy", "Gopher LM", "Point", "Appendix", "Zhang", "Holtzman", "Noun Phrase Occurrences", "Contact Info", "Regular Expression Pattern Email Address", "Phone Number", "Social Security", "Gopher", "Examples Offensive Language", "Distributional Bias", "Toxic Replies Elicited", "Dialogue Red Teaming", "Details B.1", "Adafactor", "Gupta", "Shazeer", "Stern", "Wei", "Params Acc F1", "Hello", "Muslims", "Jews", "Steemit", "Immigration", "Donald Trump", "Matter", "Appendix A.1", "Coke", "Hilary Clinton", "Nazis", "Ive", "Id", "Hitler", "Elon Musk", "Max Classifier Conf", "Ill", "Safe Red LM Questions", "Dialogue Red", "Low Red LM Offens", "Red LM", "Wait", "Im", "User", "Joe Biden", "Biden", "Sorry", "Kamala Harris", "Ok", "Fair"]}{"title": ["Towards Loosely-Coupling Knowledge Graph Embeddings and Ontology-based Reasoning"], "authors": ["[arxiv.Result.Author('Zoi Kaoudi'), arxiv.Result.Author('Abelardo Carlos Martinez Lorenzo'), arxiv.Result.Author('Volker Markl')]"], "link": ["http://arxiv.org/pdf/2202.03173v1"], "summary": "Knowledge graph completion (a.k.a.~link prediction), i.e.,~the task of\ninferring missing information from knowledge graphs, is a widely used task in\nmany applications, such as product recommendation and question answering. The\nstate-of-the-art approaches of knowledge graph embeddings and/or rule mining\nand reasoning are data-driven and, thus, solely based on the information the\ninput knowledge graph contains. This leads to unsatisfactory prediction results\nwhich make such solutions inapplicable to crucial domains such as healthcare.\nTo further enhance the accuracy of knowledge graph completion we propose to\nloosely-couple the data-driven power of knowledge graph embeddings with\ndomain-specific reasoning stemming from experts or entailment regimes (e.g.,\nOWL2). In this way, we not only enhance the prediction accuracy with domain\nknowledge that may not be included in the input knowledge graph but also allow\nusers to plugin their own knowledge graph embedding and reasoning method. Our\ninitial results show that we enhance the MRR accuracy of vanilla knowledge\ngraph embeddings by up to 3x and outperform hybrid solutions that combine\nknowledge graph embeddings with rule mining and reasoning up to 3.5x MRR.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Training OOD Detectors in their Natural Habitats"], "authors": ["[arxiv.Result.Author('Julian Katz-Samuels'), arxiv.Result.Author('Julia Nakhleh'), arxiv.Result.Author('Robert Nowak'), arxiv.Result.Author('Yixuan Li')]"], "link": ["http://arxiv.org/pdf/2202.03299v1"], "summary": "Out-of-distribution (OOD) detection is important for machine learning models\ndeployed in the wild. Recent methods use auxiliary outlier data to regularize\nthe model for improved OOD detection. However, these approaches make a strong\ndistributional assumption that the auxiliary outlier data is completely\nseparable from the in-distribution (ID) data. In this paper, we propose a novel\nframework that leverages wild mixture data -- that naturally consists of both\nID and OOD samples. Such wild data is abundant and arises freely upon deploying\na machine learning classifier in their \\emph{natural habitats}. Our key idea is\nto formulate a constrained optimization problem and to show how to tractably\nsolve it. Our learning objective maximizes the OOD detection rate, subject to\nconstraints on the classification error of ID data and on the OOD error rate of\nID examples. We extensively evaluate our approach on common OOD detection tasks\nand demonstrate superior performance.", "entities_include_in_text": ["Nguyen et al., 2015", "Hendrycks et al.,\n2019", "Liu et al., 2020", "Hendrycks\net al., 2019", "Hestenes, 1969", "Hendrycks et al., 2019", "Huber, 1964", "Blanchard et al., 2010", "Hestenes, 1969", "Rock-\nafellar, 1973", "Xu, 2017; 2021", "Liu et al., 2020", "Du et al., 2022", "Liu\net al., 2020", "Hendrycks et al., 2019", "Krizhevsky et al.,\n2009", "Netzer\net al., 2011", "Cimpoi et al., 2014", "Zhou et al., 2018", "Yu et al., 2016", "Yu et al., 2016", "Xu et al.,\n2015", "Hendrycks\net al., 2019", "Torralba et al., 2008", "Duchi et al., 2011", "Liang et al., 2018", "Lee\net al., 2018", "Liu et al., 2020", "Hsu et al., 2020", "Tack et al., 2020", "Hendrycks et al.,\n2019", "Liu et al.,\n2020", "Liang et al., 2018; Hsu et al., 2020", "Lee et al., 2018", "Liu et al., 2020; Wang et al., 2021", "Huang et al., 2021", "Hendrycks et al.,\n2019", "Liu et al., 2020", "Ruff et al., 2019; Daniel et al., 2019; Hendrycks et al.,\n2019)", "Blanchard et al.,\n2010", "Rockafellar, 1973", "Xu, 2017", "Sangalli\net al., 2021", "Blanchard et al., 2010", "Blanchard et al., 2010", "Blanchard et al., 2010", "Blanchard et al.,\n2010"], "entities_from_reference": ["Boult", "Bevandi", "Kreso", "Blanchard", "Lee", "Scott", "Machine Learning Research", "Chalapathy", "Chawla", "Hsu", "Jin", "Kira", "Pattern Recognition", "Huang", "Li", "Robust", "Krizhevsky", "Shin", "Cimpoi", "Mohamed", "Vedaldi", "Daniel", "Kurutach", "Tamar", "Deep", "Wang", "Cai", "Duchi", "Hazan", "Ergen", "Kozat", "Hendrycks", "Mazeika", "Dietterich", "Liang", "Liu", "Malinin", "Gales", "Netzer", "Unsupervised Feature Learning", "Deep Learning", "Feature Learning", "Nguyen", "Yosinski", "Clune", "Nocedal", "Wright", "Springer Science", "Perera", "Patel", "Image Processing", "Yan", "Ruff", "Binder", "Kloft", "Seff", "Zhang", "Funkhouser", "J. LSUN", "Zagoruyko", "Komodakis", "Zhou", "Lapedriza", "Khosla", "Oliva", "Scene Recognition", "Pattern Analysis", "Machine Intelligence", "Kauffmann", "J. R.", "Montavon", "Samek", "Sangalli", "Erdil", "Donati", "Konukoglu", "Song", "Jiang", "Yang", "J. Csi", "Torralba", "Freeman", "Pattern Anal", "Mach", "Bocchieri", "Ehinger", "J. TurkerGaze", "Webcam", "Eye Tracking", "Mathematical Programming", "Texture FPR", "Dataset MSP", "Pin", "Pwild", "Method SVHN", "Dataset OE Energy", "Energy", "Mahalanobis", "Special Case", "Pout", "Ptest", "Textures", "Define Define Ry", "Rwild", "Lemma", "Assumption", "Suppose K", "Ein", "Lemma B.4", "Assume", "P1", "Define Ry", "Define", "P0"]}{"title": ["AI-based artistic representation of emotions from EEG signals: a discussion on fairness, inclusion, and aesthetics"], "authors": ["[arxiv.Result.Author('Piera Riccio'), arxiv.Result.Author('Kristin Bergaust'), arxiv.Result.Author('Boel Christensen-Scheel'), arxiv.Result.Author('Juan-Carlos De Martin'), arxiv.Result.Author('Maria A. Zuluaga'), arxiv.Result.Author('Stefano Nichele')]"], "link": ["http://arxiv.org/pdf/2202.03246v1"], "summary": "While Artificial Intelligence (AI) technologies are being progressively\ndeveloped, artists and researchers are investigating their role in artistic\npractices. In this work, we present an AI-based Brain-Computer Interface (BCI)\nin which humans and machines interact to express feelings artistically. This\nsystem and its production of images give opportunities to reflect on the\ncomplexities and range of human emotions and their expressions. In this\ndiscussion, we seek to understand the dynamics of this interaction to reach\nbetter co-existence in fairness, inclusion, and aesthetics.", "entities_include_in_text": ["Manovich, \n2017", "Picard,  1995", "Bergaust  and  Nichele, \nof \n2019", "McCormack  et  al.,  2014", "Russell, \n1980", "Ekman, 1992", "Zhong, Wang and Miao, 2020", "Karras  et  al.,  2020", "McCormack et al., 2014", "Riccio, 2021", "Salevati  and  DiPaola,  2015", "Colton,  Valstar  and  Pantic,  2008", "Ekster, 2018", "Random Quark, 2017", "Kordzadeh  and  Ghasemaghaei, \n2021;  Ogolla  and  Gupta,  2018", "Immordino-Yang,  Yang  and  Damasio,  2016", "Hareli, \nKafetsios  and  Hess,  2015", "Wierzbicka,  1999", "Drag and Shaw, 1967", "Howard,  Zhang  and  Horvitz,  2017", "Zheng  et  al., \n2018", "Schaefer  et  al.,  2010;  Maffei \nand  Angrilli,  2019", "Lang,  Bradley  and \nCuthbert,  1997", "Yang  et  al.,  2018", "Goodfellow  et  al.,  2014", "Salminen, \nJung  and  Jansen,  2019", "Booth  et  al., \n2021", "Mohammad  and  Kiritchenko, \n2018", "Sun  and  Chen,  2020", "Watson  and \nseveral  debates  and  enigmas \nVehmas,  2019", "Hall,  2019", "Solvang,  2018", "Levin  and  Siebers,  2010", "Sherwood,  2019", "Siebers,  2005", "Picard,  1999", "Suchman and Weber, 2016", "McLuhan  and  Fiore,  1967", "LREC 2018", "EVA 2015"], "entities_from_reference": ["Nichele", "Piera Riccio", "Booth", "Data Protection", "Colton", "Valstar", "Pantic", "Shaw", "Ekster", "Ekman", "Goodfellow", "Mirza", "Ozair", "Bengio", "Melinda C.", "Critical Disability", "Hareli", "Kafetsios", "Howard", "Zhang", "Horvitz", "Yang", "Damasio", "Emotion", "Karras", "Aittala", "Laine", "Aila", "Limited Data", "Kordzadeh", "European Journal Lang", "Bradley", "Cuthbert", "Tobin Siebers", "Maffei", "Angrilli", "Manovich", "Flash Art International", "Dorin", "McCabe", "Monro", "Leonardo", "Mohammad", "Kiritchenko", "Gupta", "Inclusive Design Methods", "Picard", "Affective Computing", "Random Quark", "Riccio", "Piera", "Diss", "Torino", "Salevati", "Jung", "Jansen", "Schaefer", "Nils", "Philippot", "Siebers", "Solvang", "Between", "Society", "Watson", "Vehmas", "Nakao", "Sasaoka", "Miyatani", "Behavior Sounds Methods", "Zheng", "Liu", "Wang", "Miao"]}{"title": ["CITRIS: Causal Identifiability from Temporal Intervened Sequences"], "authors": ["[arxiv.Result.Author('Phillip Lippe'), arxiv.Result.Author('Sara Magliacane'), arxiv.Result.Author('Sindy L\u00f6we'), arxiv.Result.Author('Yuki M. Asano'), arxiv.Result.Author('Taco Cohen'), arxiv.Result.Author('Efstratios Gavves')]"], "link": ["http://arxiv.org/pdf/2202.03169v1"], "summary": "Understanding the latent causal factors of a dynamical system from visual\nobservations is a crucial step towards agents reasoning in complex\nenvironments. In this paper, we propose CITRIS, a variational autoencoder\nframework that learns causal representations from temporal sequences of images\nin which underlying causal factors have possibly been intervened upon. In\ncontrast to the recent literature, CITRIS exploits temporality and observing\nintervention targets to identify scalar and multidimensional causal factors,\nsuch as 3D rotation angles. Furthermore, by introducing a normalizing flow,\nCITRIS can be easily extended to leverage and disentangle representations\nobtained by already pretrained autoencoders. Extending previous results on\nscalar causal factors, we prove identifiability in a more general setting, in\nwhich only some components of a causal factor are affected by interventions. In\nexperiments on 3D rendered image sequences, CITRIS outperforms previous methods\non recovering the underlying causal variables. Moreover, using pretrained\nautoencoders, CITRIS can even generalize to unseen instantiations of causal\nfactors, opening future research areas in sim-to-real generalization for causal\nrepresentation learning.", "entities_include_in_text": ["Eberhardt, 2007", "Pearl, 2009", "Jaynes, 1957; 1968", "Jang et al., 2017", "Gresele et al., 2021; Monti et al., 2019).\nIn particular,\nLachapelle et al. (2021); Yao et al. (2021", "Bellemare et al., 2013", "Klindt et al.,\n2021", "Wright, 1921", "Spearman, 1904", "Blender Online Community, 2021", "Crane,\n2021", "Rusinkiewicz et al., 2021", "Praun et al., 2000", "Newell,\n1975", "Spearman, 1904", "Wright, 1921", "Paszke et al., 2019", "Paszke et al., 2019", "Germain et al., 2015", "Ho et al., 2019", "Ba et al., 2016", "Higgins et al., 2017", "Crane, 2021", "Rusinkiewicz et al., 2021", "Lippe et al., 2022"], "entities_from_reference": ["Kiros", "J. R.", "Bellemare", "Blender Online Community", "Blender", "Blender Foundation", "Amsterdam", "Chalupka", "Eberhardt", "Arlington", "Bischoff", "Causal Feature Learning", "Microlevel Climate Data", "Robert", "Machine Learning Research", "Cadiz", "Crane", "Curless", "Levoy", "Range Images", "Annual Conference", "Machinery", "Dean", "Causal Identifiability", "Carnegie Mellon University", "Falcon", "Murray", "Machine Learning", "Gresele", "Sch", "Hendrycks", "Gaussian Error Linear Units", "Matthey", "Botvinick", "Mohamed", "Lerchner", "Duan", "Architecture Design", "Salakhutdinov", "Hoel", "Albantakis", "Natl", "Acad", "Feature Extraction", "Nonlinear ICA", "Red Hook", "Pajunen", "Karhunen", "Oja", "John Wiley", "Sons", "Sasaki", "Ioffe", "Szegedy", "Batch Normalization", "Network Training", "Blei", "Jang", "Poole", "Jaynes", "Statistical Mechanics", "Phys", "Khemakhem", "Monti", "Hyvarinen", "Ranzato", "Hadsell", "Lin", "Kingma", "J. Adam", "San Diego", "Wallach", "Grauman", "Garnett", "Klindt", "Schott", "Ustyuzhaninov", "Brendel", "Krishnamurthy", "Dense Polygon Meshes", "Kumar", "Sattigeri", "Lachapelle", "Everett", "Lacoste", "Mechanism Sparsity", "Lippe", "Cohen", "Locatello", "Bauer", "Lucic", "Raetsch", "Gelly", "Tschannen", "Virtual Event", "Zhang", "Silva", "Murphy", "Berkeley", "Paszke", "Gross", "Lerer", "Bradbury", "Chanan", "Killeen", "Gimelshein", "Desmaison", "Yang", "Raison", "Tejani", "Steiner", "Fang", "Bai", "Fox", "Pearl", "J. Causality", "Cambridge University Press", "Praun", "Hoppe", "Ramachandran", "Zoph", "Le", "Sohn", "Lee", "Manifold Interaction", "Rezende", "Kalchbrenner", "Goyal", "Bengio", "Towards Causal", "Sorrenson", "Rother", "Spearman", "Creager", "Kilbertus", "Dittadi", "Turk", "Data Augmentations Provably Isolates Content", "Wright", "Liu", "Hao", "Wang", "J. CausalVAE", "Pattern Recognition", "Yao", "Zimmermann", "Schneider", "Appendix", "Further", "Dynamic Bayesian", "Lemma", "Jh", "Et", "Z Z", "Hence", "Next", "Ci", "C2", "Due", "Appendix B.4.2", "Cj", "Appendix B.4.3", "Hare Figure", "Object", "Teapot Armadillo Hare Cow Dragon Head Horse", "T T", "Hyperparameters", "Below", "Flows", "Feature Dimension", "Layer Conv Conv Conv Conv Conv Conv Conv Conv Reshape Linear Linear Linear Linear Reshape Upsample", "Target", "Layer Normalization", "Hyperparameter Value Batch", "Optimizer Learning", "Gumbel Softmax", "Cosine Warmup", "Oracle SlowVAE", "False", "Mk Bernoulli", "Triplet", "Mean R2"]}{"title": ["Corrupted Image Modeling for Self-Supervised Visual Pre-Training"], "authors": ["[arxiv.Result.Author('Yuxin Fang'), arxiv.Result.Author('Li Dong'), arxiv.Result.Author('Hangbo Bao'), arxiv.Result.Author('Xinggang Wang'), arxiv.Result.Author('Furu Wei')]"], "link": ["http://arxiv.org/pdf/2202.03382v1"], "summary": "We introduce Corrupted Image Modeling (CIM) for self-supervised visual\npre-training. CIM uses an auxiliary generator with a small trainable BEiT to\ncorrupt the input image instead of using artificial mask tokens, where some\npatches are randomly selected and replaced with plausible alternatives sampled\nfrom the BEiT output distribution. Given this corrupted image, an enhancer\nnetwork learns to either recover all the original image pixels, or predict\nwhether each visual token is replaced by a generator sample or not. The\ngenerator and the enhancer are simultaneously trained and synergistically\nupdated. After pre-training, the enhancer can be used as a high-capacity visual\nencoder for downstream tasks. CIM is a general and flexible visual pre-training\nframework that is suitable for various network architectures. For the first\ntime, CIM demonstrates that both ViT and CNN can learn rich visual\nrepresentations using a unified, non-Siamese framework. Experimental results\nshow that our approach achieves compelling results in vision benchmarks, such\nas ImageNet classification and ADE20K semantic segmentation. For example,\n300-epoch CIM pre-trained vanilla ViT-Base/16 and ResNet-50 obtain 83.3 and\n80.6 Top-1 fine-tuning accuracy on ImageNet-1K image classification\nrespectively.", "entities_include_in_text": ["Zhou et al., 2021; He et al., 2021;\nXie et al., 2021; Dong et al., 2021; Wei et al., 2021; El-\nNouby et al., 2021", "Devlin et al.,\n2019", "Vaswani et al., 2017", "Bao\net al., 2021", "Ramesh et al., 2021", "Dosovitskiy et al., 2020", "Clark et al., 2020", "Esser et al., 2021; Yu et al.,\n2021; Dong et al., 2021", "Holtzman\net al., 2019", "Wei et al.,\n\n2021", "He et al.,\n2016", "Lin et al., 2014", "Bao et al.,\n2021", "Rolfe, 2016; Van\nDen Oord et al., 2017", "Ramesh\net al., 2021", "rather than adversari-\nally as Goodfellow et al., 2014", "He\net al., 2021", "He et al., 2021", "Ramesh et al., 2021; Bao et al.,\n2021", "Ramesh et al., 2021", "Bao et al., 2021", "Paszke et al., 2019", "Caron et al., 2021", "Bao et al., 2021", "He et al., 2021", "Caron et al., 2021", "Bao et al., 2021", "Bao et al., 2021", "He et al., 2021", "Deng et al., 2009", "Bao et al.,\n2021", "Srivastava et al.,\n2014", "Huang et al., 2016", "Zhou\net al., 2019", "Wightman et al., 2021", "He et al., 2016", "Paszke et al., 2019", "Touvron et al., 2019", "Bello et al., 2021", "Wightman et al., 2021", "Wightman et al., 2021", "Grill et al., 2020", "Caron et al., 2020", "Wightman et al., 2021", "Liu et al., 2022", "Caron et al., 2021", "Bao et al., 2021", "He et al., 2019", "He et al., 2019", "Grill et al., 2020", "Caron et al., 2020", "Long et al., 2015", "He et al., 2021; Xie et al., 2021", "He et al., 2021", "Meng et al., 2021; Chi et al., 2021", "He et al.,\n2021", "Zhong et al., 2020", "Esser et al.,\n2021; Dong et al., 2021; Yu et al., 2021", "Zhong et al., 2020", "He et al., 2020", "Dosovitskiy et al., 2020", "Bao et al., 2021", "Dong et al., 2021", "Wei et al., 2021", "Devlin et al., 2019", "Clark et al., 2020", "Yu et al., 2021", "Wei et al., 2021", "Srivastava et al., 2014", "Huang et al., 2016", "Vaswani et al., 2017", "Dosovitskiy et al., 2020", "Bao\net al., 2021", "Bao et al., 2021", "Srivastava et al., 2014", "Huang et al., 2016", "Szegedy et al., 2016", "Yun et al., 2019", "Cubuk et al., 2020", "Clark et al., 2020", "Bao et al., 2021", "Bao et al., 2021", "Berman et al., 2019; Hoffer\net al., 2019", "Cubuk et al., 2020", "Yun et al., 2019", "Szegedy et al., 2016", "Huang et al., 2016", "Srivastava et al., 2014", "Wightman et al., 2021", "Bao\net al., 2021"], "entities_from_reference": ["Bao", "Wei", "Bello", "Fedus", "Lin", "Zoph", "Berman", "Vedaldi", "Douze", "Caron", "Misra", "Goyal", "Bojanowski", "Joulin", "Touvron", "Chen", "Wang", "Guo", "Deng", "Liu", "Gao", "Jun", "Luan", "Fan", "Girshick", "Xie", "Chi", "Huang", "Bajaj", "Clark", "Le", "Cubuk", "Practical", "Pattern Recognition Workshops", "Socher", "Li", "Devlin", "Chang", "Lee", "Toutanova", "Singh", "Dong", "Loy", "Tang", "Image", "Zhang", "Yuan", "Wen", "Dosovitskiy", "Kolesnikov", "Zhai", "Dehghani", "Minderer", "Heigold", "Gelly", "Izacard", "Jegou", "Grave", "Esser", "Goodfellow", "Mirza", "Ozair", "Bengio", "Grill", "Strub", "Richemond", "Doersch", "Pires", "Azar", "Ren", "Deep", "Hoffer", "Hoefler", "Holtzman", "Buys", "Choi", "Sedra", "Weinberger", "Denker", "J. S.", "Henderson", "Howard", "Hubbard", "Jackel", "Maire", "Hays", "Ramanan", "Zitnick", "Darrell", "Loshchilov", "Hutter", "Meng", "Bennett", "Han", "Paszke", "Gross", "Lerer", "Bradbury", "Chanan", "Killeen", "Gimelshein", "Desmaison", "Kopf", "Yang", "Raison", "Tejani", "Steiner", "Fang", "Bai", "Ramesh", "Pavlov", "Goh", "Gray", "Voss", "Rolfe", "J. T. Discrete", "Srivastava", "Krizhevsky", "Salakhutdinov", "Szegedy", "Vanhoucke", "Ioffe", "Wojna", "Van Den Oord", "Vinyals", "Mao", "Vaswani", "Shazeer", "Uszkoreit", "Jones", "Gomez", "Polosukhin", "Visual", "Wightman", "Cao", "Yao", "Dai", "Koh", "J. Y.", "Pang", "Qin", "Yun", "Chun", "Choe", "Yoo", "Cisse", "Dauphin", "Zuo", "Zhong", "Zheng", "Kang", "Ran", "Zhou", "Zhao", "Puig", "Fidler", "Barriuso", "Appendix", "Vanilla ViT", "Value Optimizer", "Batch Size Weight Decay Optimizer Momentum", "Masked Patches", "Stochastic Depth", "Patch Size", "Cosine Decay", "Random", "Vanilla ViT Models", "Label Smoothing", "Random Augmentation", "Size", "Loss Function", "Cross Entropy Loss", "Vanilla", "Epoch", "Epoch FT Optimizer Peak Learning Rate", "Batch Size Learning Rate Schedule Loss Function Warmup Epochs Weight Decay", "Cosine Decay Binary Cross Entropy"]}{"title": ["Conversational Agents: Theory and Applications"], "authors": ["[arxiv.Result.Author('Mattias Wahde'), arxiv.Result.Author('Marco Virgolin')]"], "link": ["http://arxiv.org/pdf/2202.03164v1"], "summary": "In this chapter, we provide a review of conversational agents (CAs),\ndiscussing chatbots, intended for casual conversation with a user, as well as\ntask-oriented agents that generally engage in discussions intended to reach one\nor several specific goals, often (but not always) within a specific domain. We\nalso consider the concept of embodied conversational agents, briefly reviewing\naspects such as character animation and speech processing. The many different\napproaches for representing dialogue in CAs are discussed in some detail, along\nwith methods for evaluating such agents, emphasizing the important topics of\naccountability and interpretability. A brief historical overview is given,\nfollowed by an extensive overview of various applications, especially in the\nfields of health and education. We end the chapter by discussing benefits and\npotential risks regarding the societal impact of current and future CA\ntechnology.", "entities_include_in_text": ["WI2019", "WIT press, 2008", "MIT press, 2018", "Studentlitteratur,\n\n1969", "Consulting\n\nPsychologists Press, 1978", "Sept., 2015", "Mar., 2019", "June, 2019", "Nov., 2018", "Prentice-Hall, 2009", "apr, 2017", "July, 2017", "MIT Press, 2017", "IWP2005), (2005", "Nov., 2016", "Nov.,\n2016", "WI2019", "July, 2020"], "entities_from_reference": ["Plusieurs"]}{"title": ["FL_PyTorch: optimization research simulator for federated learning"], "authors": ["[arxiv.Result.Author('Konstantin Burlachenko'), arxiv.Result.Author('Samuel Horv\u00e1th'), arxiv.Result.Author('Peter Richt\u00e1rik')]"], "link": ["http://arxiv.org/pdf/2202.03099v1"], "summary": "Federated Learning (FL) has emerged as a promising technique for edge devices\nto collaboratively learn a shared machine learning model while keeping training\ndata locally on the device, thereby removing the need to store and access the\nfull data in the cloud. However, FL is difficult to implement, test and deploy\nin practice considering heterogeneity in common edge device settings, making it\nfundamentally hard for researchers to efficiently prototype and test their\noptimization algorithms. In this work, our aim is to alleviate this problem by\nintroducing FL_PyTorch : a suite of open-source software written in python that\nbuilds on top of one the most popular research Deep Learning (DL) framework\nPyTorch. We built FL_PyTorch as a research simulator for FL to enable fast\ndevelopment, prototyping and experimenting with new and existing FL\noptimization algorithms. Our system supports abstractions that provide\nresearchers with a sufficient level of flexibility to experiment with existing\nand novel approaches to advance the state-of-the-art. Furthermore, FL_PyTorch\nis a simple to use console system, allows to run several clients simultaneously\nusing local CPUs or GPU(s), and even remote compute devices without the need\nfor any distributed implementation provided by the user. FL_PyTorch also offers\na Graphical User Interface. For new methods, researchers only provide the\ncentralized implementation of their algorithm. To showcase the possibilities\nand usefulness of our system, we experiment with several well-known\nstate-of-the-art FL algorithms and a few of the most common FL datasets.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Combining Deep Learning and Reasoning for Address Detection in Unstructured Text Documents"], "authors": ["[arxiv.Result.Author('Matthias Engelbach'), arxiv.Result.Author('Dennis Klau'), arxiv.Result.Author('Jens Drawehn'), arxiv.Result.Author('Maximilien Kintz')]"], "link": ["http://arxiv.org/pdf/2202.03103v1"], "summary": "Extracting information from unstructured text documents is a demanding task,\nsince these documents can have a broad variety of different layouts and a\nnon-trivial reading order, like it is the case for multi-column documents or\nnested tables. Additionally, many business documents are received in paper\nform, meaning that the textual contents need to be digitized before further\nanalysis. Nonetheless, automatic detection and capturing of crucial document\ninformation like the sender address would boost many companies' processing\nefficiency. In this work we propose a hybrid approach that combines deep\nlearning with reasoning for finding and extracting addresses from unstructured\ntext documents. We use a visual deep learning model to detect the boundaries of\npossible address regions on the scanned document images and validate these\nresults by analyzing the containing text using domain knowledge represented as\na rule based system.", "entities_include_in_text": ["Grand View Research Inc. 2019", "Stevens et al. 2020", "Neudecker et al. 2019", "Smith 2019", "Xu et al. 2021", "Lin et al. 2005", "Chang and Li 2010", "Vishwanath et al. 2019", "Sunder et al. 2019", "Vishwanath et al. 2019", "Bommasani et al. 2021", "Harley, Ufkes, and Derpanis 2015", "Breuel 2017", "Abdulla 2017", "WIRI 2005"], "entities_from_reference": ["Mask R-CNN", "Keras", "Bommasani", "Altman", "Arora", "Bernstein", "Brunskill", "Stanford University", "Stanford Institute", "Breuel", "Chang", "Li", "Grand View Research", "Optical Char2021Size", "Harley", "Ufkes", "Derpanis", "Deep Convolutional Nets", "Retrieval", "Lin", "Zhang", "Meng", "Postal", "Web Documents"]}{"title": ["GMC -- Geometric Multimodal Contrastive Representation Learning"], "authors": ["[arxiv.Result.Author('Petra Poklukar'), arxiv.Result.Author('Miguel Vasco'), arxiv.Result.Author('Hang Yin'), arxiv.Result.Author('Francisco S. Melo'), arxiv.Result.Author('Ana Paiva'), arxiv.Result.Author('Danica Kragic')]"], "link": ["http://arxiv.org/pdf/2202.03390v1"], "summary": "Learning representations of multimodal data that are both informative and\nrobust to missing modalities at test time remains a challenging problem due to\nthe inherent heterogeneity of data obtained from different channels. To address\nit, we present a novel Geometric Multimodal Contrastive (GMC) representation\nlearning method comprised of two main components: i) a two-level architecture\nconsisting of modality-specific base encoder, allowing to process an arbitrary\nnumber of modalities to an intermediate representation of fixed dimensionality,\nand a shared projection head, mapping the intermediate representations to a\nlatent representation space; ii) a multimodal contrastive loss function that\nencourages the geometric alignment of the learned representations. We\nexperimentally demonstrate that GMC representations are semantically rich and\nachieve state-of-the-art performance with missing modality information on three\ndifferent learning problems including prediction and reinforcement learning\ntasks.", "entities_include_in_text": ["Tsai et al., 2018; 2019", "Silva et al., 2019; Vasco et al., 2021", "Guo et al., 2019", "Shi et al., 2019", "Vasco et al., 2022", "Vasco et al., 2021", "Tsai et al., 2018", "Shi et al., 2019", "Tsai et al., 2018", "Tsai et al., 2019", "Chen et al., 2020", "Chen et al., 2020", "Silva et al., 2019; Vasco\net al., 2021", "Yin\net al., 2017", "Suzuki\net al., 2016", "Shi et al., 2019", "Shi et al., 2019", "Vasco et al., 2022", "Vasco et al., 2021", "Liang et al., 2021", "Tsai et al., 2018", "Tsai et al., 2019", "Vasco et al., 2022", "Poklukar\net al., 2022", "Shi\net al., 2019", "McInnes et al.,\n2018", "Zadeh et al., 2016", "Bagher Zadeh et al., 2018", "Tsai\net al., 2019", "Tsai et al.,\n2019", "Tsai\net al., 2018; 2019", "Silva et al., 2019", "Silva\net al., 2019", "Vasco et al., 2021", "Higgins et al., 2017", "Lillicrap et al., 2015", "Poklukar et al.,\n2022", "Poklukar et al., 2022", "Poklukar et al., 2022", "Poklukar\net al., 2021", "Silva et al., 2019", "Tsai et al., 2019", "Silva et al., 2019"], "entities_from_reference": ["Liang", "Morency", "Melbourne", "Baltrusaitis", "Ahuja", "Cao", "Chen", "Singh", "Machine Learning", "Machine Learning Research", "Guo", "Wang", "Kingma", "Lyu", "Fan", "Lee", "Zhu", "Multibench", "Lillicrap", "J. J.", "Heess", "Erez", "Groberger", "Uniform", "Open Source Software", "Meo", "Poklukar", "Zhang", "Pokorny", "Shi", "Paige", "Silva", "Vasco", "Melo", "Paiva", "Geometric Multimodal Contrastive", "Matsuo", "Joint", "Manderson", "Noca", "Dudek", "Meger", "Tsai", "Zadeh", "Salakhutdinov", "Bai", "Kolter", "Annual", "Yin", "Networks", "Goodman", "Billard", "Pincus", "Zambelli", "Cully", "Robotics", "Delaunay Component Analysis Delaunay", "Definition", "F E", "Observations", "Observations Sound Observations Trajectory Observations Label", "Loss", "Results", "Metric Baseline", "Text Observations", "Audio Observations", "Video Observations", "Network", "Input", "Classifier Base", "Input R200 FC", "Input R10 FC", "Shared", "Input Rd FC", "Classifier Input Rs FC", "Encoder Input", "Encoder Input R200 FC", "Decoder Input Rs Encoder Input Rd FC", "Encoder Input R10 FC", "Text", "Video", "Input R12 FC", "Encoder Input R12 FC", "Parameter Intermediate", "Batch", "Parameter Value"]}{"title": ["Transformers in Self-Supervised Monocular Depth Estimation with Unknown Camera Intrinsics"], "authors": ["[arxiv.Result.Author('Arnav Varma'), arxiv.Result.Author('Hemang Chawla'), arxiv.Result.Author('Bahram Zonooz'), arxiv.Result.Author('Elahe Arani')]"], "link": ["http://arxiv.org/pdf/2202.03131v1"], "summary": "The advent of autonomous driving and advanced driver assistance systems\nnecessitates continuous developments in computer vision for 3D scene\nunderstanding. Self-supervised monocular depth estimation, a method for\npixel-wise distance estimation of objects from a single camera without the use\nof ground truth labels, is an important task in 3D scene understanding.\nHowever, existing methods for this task are limited to convolutional neural\nnetwork (CNN) architectures. In contrast with CNNs that use localized linear\noperations and lose feature resolution across the layers, vision transformers\nprocess at constant resolution with a global receptive field at every stage.\nWhile recent works have compared transformers against their CNN counterparts\nfor tasks such as image classification, no study exists that investigates the\nimpact of using transformers for self-supervised monocular depth estimation.\nHere, we first demonstrate how to adapt vision transformers for self-supervised\nmonocular depth estimation. Thereafter, we compare the transformer and\nCNN-based architectures for their performance on KITTI depth prediction\nbenchmarks, as well as their robustness to natural corruptions and adversarial\nattacks, including when the camera intrinsics are unknown. Our study\ndemonstrates how transformer-based architecture, though lower in run-time\nefficiency, achieves comparable performance while being more robust and\ngeneralizable.", "entities_include_in_text": ["He et al., 2016", "Dosovitskiy et al., 2021", "Vaswani et al., 2017", "Car-\n\naEqual contribution\n\nion et al., 2020", "Zheng\net al., 2021", "Li et al., 2020;\nRanftl et al., 2021", "Lee\net al., 2019; Aich et al., 2021", "Guizilini et al., 2020; Lyu et al., 2020; Chawla\net al., 2021", "Johnston and Carneiro, 2020", "Yang et al., 2021", "Touvron et al., 2021", "Caron\net al., 2021", "Raghu et al., 2021; Bhojanapalli et al.,\n2021", "Eigen et al., 2014", "Geiger et al.,\n2013", "Ranftl et al., 2021", "Bho-\njanapalli et al., 2021; Paul and Chen, 2021", "Carion et al.,\n2020; Liu et al., 2021", "Zheng\net al., 2021; Strudel et al., 2021", "Ranftl et al., 2020; Yang\net al., 2021", "Ranftl et al.,\n2020", "Yang et al., 2021", "Vaswani\net al., 2017", "Johnston and Carneiro, 2020; Xiang et al.,\n2021", "Godard et al., 2019; Lyu et al., 2020", "Chawla et al., 2020", "Lopez et al., 2019; Zhuang et al., 2019", "Gordon\net al., 2019", "Godard\net al., 2019", "Ranftl et al., 2020", "Touvron\net al., 2021", "Vaswani et al., 2017", "Lin et al., 2017", "Godard et al.,\n2019", "Touvron et al., 2021", "Godard et al., 2019", "Godard et al., 2019", "Eigen et al.,\n2014", "Geiger et al., 2013", "Zhou et al., 2017", "Go-\ndard et al., 2019", "Paszke\net al., 2019", "Deng et al., 2009", "Kingma and Ba, 2014", "Loshchilov and Hutter, 2017", "Eigen et al., 2014", "Johnston and Carneiro, 2020", "Guizilini et al., 2020", "Uhrig et al., 2017", "Chawla et al.,\n2021", "Zhou et al., 2017", "Yin and Shi, 2018", "Mahjourian et al., 2018", "Casser et al., 2019", "Roussel et al., 2019", "Gordon et al., 2019", "Godard et al., 2019", "Ranjan et al., 2019", "Bian et al., 2019", "Godard et al., 2019", "Klingner et al., 2020", "Guizilini et al., 2020", "Poggi et al., 2020", "Johnston and Carneiro, 2020", "Lyu et al., 2020", "Guizilini et al., 2020", "Lyu et al., 2020", "Chawla et al., 2021", "Fu et al., 2018", "Diaz and Marathe, 2019", "Yin et al., 2019", "Ren et al., 2019", "Zhang et al., 2019", "Guo et al., 2018", "Chawla et al., 2021", "Godard et al., 2019", "Ochs et al., 2019", "Kong and Fowlkes, 2019", "Jiang and Huang, 2019", "Klingner et al., 2020", "Zhang et al., 2018", "Guizilini et al., 2020", "Goldman et al., 2019", "Godard et al., 2017", "Geirhos et al.,\n2020", "Godard et al., 2019", "He et al., 2016", "Hendrycks and\nDietterich, 2019", "Michaelis et al., 2019", "Madry et al., 2018", "Kurakin et al., 2016", "Wong et al., 2020", "Paul and\nChen, 2021; Bhojanapalli et al., 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Artificial Intelligence based tool wear and defect prediction for special purpose milling machinery using low-cost acceleration sensor retrofits"], "authors": ["[arxiv.Result.Author('Mahmoud Kheir-Eddine'), arxiv.Result.Author('Michael Banf'), arxiv.Result.Author('Gregor Steinhagen')]"], "link": ["http://arxiv.org/pdf/2202.03068v1"], "summary": "Milling machines form an integral part of many industrial processing chains.\nAs a consequence, several machine learning based approaches for tool wear\ndetection have been proposed in recent years, yet these methods mostly deal\nwith standard milling machines, while machinery designed for more specialized\ntasks has gained only limited attention so far. This paper demonstrates the\napplication of an acceleration sensor to allow for convenient condition\nmonitoring of such a special purpose machine, i.e. round seam milling machine.\nWe examine a variety of conditions including blade wear and blade breakage as\nwell as improper machine mounting or insufficient transmission belt tension. In\naddition, we presents different approaches to supervised failure recognition\nwith limited amounts of training data. Hence, aside theoretical insights, our\nanalysis is of high, practical importance, since retrofitting older machines\nwith acceleration sensors and an on-edge classification setup comes at low cost\nand effort, yet provides valuable insights into the state of the machine and\ntools in particular and the production process in general.", "entities_include_in_text": [], "entities_from_reference": ["Mach", "Learn", "Caesarendra", "Feature Extraction Methods", "Cortes", "Machine", "Vilan Vilan", "Segade Robleda", "Gokhale", "Hesser", "Markert", "Knittel", "Makich", "Mallat", "Academic Press", "Terrazas", "Ratchev", "Mohanraj", "Shankar", "Rajasekar", "Sakthivel", "S2238785418313061", "Steinhagen", "Wang", "Yang", "Zhang", "Xie", "S0924424714000065", "Zhou", "Gao", "Li", "Starly", "Cai", "Cohen", "Lee", "Particle"]}{"title": ["Investigating the fidelity of explainable artificial intelligence methods for applications of convolutional neural networks in geoscience"], "authors": ["[arxiv.Result.Author('Antonios Mamalakis'), arxiv.Result.Author('Elizabeth A. Barnes'), arxiv.Result.Author('Imme Ebert-Uphoff')]"], "link": ["http://arxiv.org/pdf/2202.03407v1"], "summary": "Convolutional neural networks (CNNs) have recently attracted great attention\nin geoscience due to their ability to capture non-linear system behavior and\nextract predictive spatiotemporal patterns. Given their black-box nature\nhowever, and the importance of prediction explainability, methods of\nexplainable artificial intelligence (XAI) are gaining popularity as a means to\nexplain the CNN decision-making strategy. Here, we establish an intercomparison\nof some of the most popular XAI methods and investigate their fidelity in\nexplaining CNN decisions for geoscientific applications. Our goal is to raise\nawareness of the theoretical limitations of these methods and gain insight into\nthe relative strengths and weaknesses to help guide best practices. The\nconsidered XAI methods are first applied to an idealized attribution benchmark,\nwhere the ground truth of explanation of the network is known a priori, to help\nobjectively assess their performance. Secondly, we apply XAI to a\nclimate-related prediction setting, namely to explain a CNN that is trained to\npredict the number of atmospheric rivers in daily snapshots of climate\nsimulations. Our results highlight several important issues of XAI methods\n(e.g., gradient shattering, inability to distinguish the sign of attribution,\nignorance to zero input) that have previously been overlooked in our field and,\nif not considered cautiously, may lead to a distorted picture of the CNN\ndecision-making strategy. We envision that our analysis will motivate further\ninvestigation into XAI fidelity and will help towards a cautious implementation\nof XAI in geoscience, which can lead to further exploitation of CNNs and deep\nlearning for prediction problems.", "entities_include_in_text": ["Lary et al., 2016; Karpatne et al., 2018; \n\nReichstein et al., 2019", "Bergen et al., 2019", "Shen, 2018; Sit et al., 2020", "Barnes et al., 2019; \n\nRolnick  et  al.,  2019;  Ham  et  al.,  2019", "LeCun et al., 2015", "Overpeck \n\net al., 2011; Guo, 2017; Agapiou, 2017; Reinsel et al., 2018", "Buhrmester et al., 2019", "McGovern \n\net  al.,  2019;  Ebert-Uphoff  and  Hilburn,  2020;  Toms  et  al.,  2020;  Mamalakis  et  al.,  2022", "Sonnewald and Lguensat, \n\n2021; Mayer and Barnes, 2021; Hilburn et al., 2021; Keys et al., 2021", "Ebert-Uphoff and Hilburn, 2020", "Barnes et al., 2020; Toms et al., 2021", "Mamalakis et al., \n\n2022", "Leavitt and Morcos, 2020", "Mamalakis et al., 2021). Attribution benchmark datasets \n\nconsist  of  synthetic  inputs  and  outputs,  where  the  functional  relationship  between  the  two  is \n\nknown. This allows for deriving the ground truth of what the explanation of the network should \n\nlook like for each prediction. In this way, the assessment of XAI methods is no longer based on \n\nsubjective criteria, but rather it is based on the direct comparison of the XAI results to the ground \n\ntruth of the explanation. As a first example, Mamalakis et al. (2021", "Arras et al., 2021; Zhou et al., 2022", "Mamalakis et al., \n\n2021", "see e.g., Hilburn et al., 2021", "LeCun et al., 2015", "Kohlbrenner et al., 2020", "Prabhat et al., 2021", "Simonyan et al., 2014", "Smilkov et al., 2017", "Shrikumar  et  al.,  2016;  2017", "Sundararajan  et  al.,  2017", "Bach  et  al.,  2015", "Bach et al., 2015", "Bach et al., 2015", "Kohlbrenner et al., \n\n2020", "Bach  et  al.,  2016;  Kohlbrenner  et  al.,  2020", "Montavon et al., 2017", "Lundberg  and  Lee,  2017", "Simonyan et al., 2014", "Smilkov et al., 2017", "Shrikumar et al., 2017", "Sundararajan et al., 2017", "Bach et al., 2015", "Bach et al., 2015", "Kohlbrenner et al., 2020", "Kohlbrenner et al., 2020", "Montavon et al., 2017", "Lundberg and Lee, 2017", "Balduzzi et al., 2017", "Balduzzi et al., 2017", "Samek et al., 2016; Montavon et al., 2017", "e.g., Kohlbrenner et al., 2020", "Ancona et al., 2018; 2019", "Bach et \n\nal., 2016", "Montavon \n\net  al.,  2017", "Kohlbrenner et al., 2020", "Lundberg  and  Lee,  2017", "McGovern  et  al.,  2019;  Ebert-Uphoff  and \n\nHilburn, 2020; Barnes et al., 2020; Toms et al., 2020; 2021; Sonnewald and Lguensat, 2021; Mayer \n\nand Barnes, 2021; Hilburn et al., 2021; Keys et al., 2021; Mamalakis et al., 2022", "Mamalakis et al., 2021; \n\nLeavitt and Morcos, 2020", "Kohlbrenner  et  al.,  2020;  Mamalakis  et  al.,  2021", "see \n\nPrabhat et al., 2021", "Prabhat  et  al.,  2021", "Simonyan et al., 2014", "Smilkov et al., 2017", "Shrikumar et al., 2016; 2017", "Sundararajan  et  al.,  2017", "Bach et al., 2015; Samek et al., 2016", "Bach et al., 2015", "Bach et al., 2015", "Kohlbrenner et al., 2020", "Kohlbrenner et al., 2020", "Montavon  et  al.,  2017", "Samek et al., 2016; Montavon et al., \n\n2017", "Shapley, 1953", "Lundberg and Lee, \n\n2017", "Shapley,  1953", "Lundberg and Lee, 2017"], "entities_from_reference": ["Plusieurs"]}{"title": ["Evaluation of Runtime Monitoring for UAV Emergency Landing"], "authors": ["[arxiv.Result.Author('Joris Guerin'), arxiv.Result.Author('Kevin Delmas'), arxiv.Result.Author('J\u00e9r\u00e9mie Guiochet')]"], "link": ["http://arxiv.org/pdf/2202.03059v1"], "summary": "To certify UAV operations in populated areas, risk mitigation strategies --\nsuch as Emergency Landing (EL) -- must be in place to account for potential\nfailures. EL aims at reducing ground risk by finding safe landing areas using\non-board sensors. The first contribution of this paper is to present a new EL\napproach, in line with safety requirements introduced in recent research. In\nparticular, the proposed EL pipeline includes mechanisms to monitor learning\nbased components during execution. This way, another contribution is to study\nthe behavior of Machine Learning Runtime Monitoring (MLRM) approaches within\nthe context of a real-world critical system. A new evaluation methodology is\nintroduced, and applied to assess the practical safety benefits of three MLRM\nmechanisms. The proposed approach is compared to a default mitigation strategy\n(open a parachute when a failure is detected), and appears to be much safer.", "entities_include_in_text": ["ERTS 2020", "PRDC 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Multi-Objective Quality Diversity Optimization"], "authors": ["[arxiv.Result.Author('Thomas Pierrot'), arxiv.Result.Author('Guillaume Richard'), arxiv.Result.Author('Karim Beguir'), arxiv.Result.Author('Antoine Cully')]"], "link": ["http://arxiv.org/pdf/2202.03057v1"], "summary": "In this work, we consider the problem of Quality-Diversity (QD) optimization\nwith multiple objectives. QD algorithms have been proposed to search for a\nlarge collection of both diverse and high-performing solutions instead of a\nsingle set of local optima. Thriving for diversity was shown to be useful in\nmany industrial and robotics applications. On the other hand, most real-life\nproblems exhibit several potentially antagonist objectives to be optimized.\nHence being able to optimize for multiple objectives with an appropriate\ntechnique while thriving for diversity is important to many fields. Here, we\npropose an extension of the MAP-Elites algorithm in the multi-objective\nsetting: Multi-Objective MAP-Elites (MOME). Namely, it combines the diversity\ninherited from the MAP-Elites grid algorithm with the strength of\nmulti-objective optimizations by filling each cell with a Pareto Front. As\nsuch, it allows to extract diverse solutions in the descriptor space while\nexploring different compromises between objectives. We evaluate our method on\nseveral tasks, from standard optimization problems to robotics simulations. Our\nexperimental evaluation shows the ability of MOME to provide diverse solutions\nwhile providing global performances similar to standard multi-objective\nalgorithms.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Auto-Lambda: Disentangling Dynamic Task Relationships"], "authors": ["[arxiv.Result.Author('Shikun Liu'), arxiv.Result.Author('Stephen James'), arxiv.Result.Author('Andrew J. Davison'), arxiv.Result.Author('Edward Johns')]"], "link": ["http://arxiv.org/pdf/2202.03091v1"], "summary": "Understanding the structure of multiple related tasks allows for multi-task\nlearning to improve the generalisation ability of one or all of them. However,\nit usually requires training each pairwise combination of tasks together in\norder to capture task relationships, at an extremely high computational cost.\nIn this work, we learn task relationships via an automated weighting framework,\nnamed Auto-Lambda. Unlike previous methods where task relationships are assumed\nto be fixed, Auto-Lambda is a gradient-based meta learning framework which\nexplores continuous, dynamic task relationships via task-specific weightings,\nand can optimise any choice of combination of tasks through the formulation of\na meta-loss; where the validation loss automatically influences task weightings\nthroughout training. We apply the proposed framework to both multi-task and\nauxiliary learning problems in computer vision and robotics, and show that\nAuto-Lambda achieves state-of-the-art performance, even when compared to\noptimisation strategies designed specifically for each problem and data domain.\nFinally, we observe that Auto-Lambda can discover interesting learning\nbehaviors, leading to new insights in multi-task learning. Code is available at\nhttps://github.com/lorenmt/auto-lambda.", "entities_include_in_text": [], "entities_from_reference": ["Zhao Chen", "Vijay Badrinarayanan", "Lee", "Andrew Rabinovich", "Machine Learning", "Jiquan Ngiam", "Thang Luong", "Henrik Kretzschmar", "Chai", "Dragomir Anguelov", "Marius Cordts", "Mohamed Omran", "Sebastian Ramos", "Timo Rehfeld", "Markus Enzweiler", "Rodrigo Benenson", "Uwe Franke", "Stefan Roth", "Bernt Schiele", "Pattern Recognition", "Geus", "Panagiotis Meletis", "Chenyang Lu", "Xiaoxiao Wen", "Gijs Dubbelman", "Vision", "Yunshu Du", "Wojciech M Czarnecki", "Razvan Pascanu", "Balaji Lakshminarayanan", "Kshitij Dwivedi", "Gemma Roig", "Ehsan Amid", "Zhe Zhao", "Tianhe Yu", "Rohan Anil", "Chelsea Finn", "Pieter Abbeel", "Sergey Levine", "Haoping Bai", "Zequn Jie", "Jiayi Ma", "Kui Jia", "Wei Liu", "Michelle Guo", "Albert Haque", "Huang", "Serena Yeung", "Li Fei-Fei", "Dynamic", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun", "Deep", "Falk Heuer", "Sven Mantowsky", "Saqib Bukhari", "Georg Schneider", "Timothy Hospedales", "Antreas Antoniou", "Paul Micaelli", "Amos Storkey", "Stephen James", "Andrew J Davison", "Edward Johns", "Robot Learning", "Zicong Ma", "David Rovick Arrojo", "Rlbench", "Isabel Valera", "Rotograd", "Alex Kendall", "Yarin Gal", "Roberto Cipolla", "Iasonas Kokkinos", "Alex Krizhevsky", "Trevor Darrell", "Machine Learning Research", "Zhenhua Li", "Zhang", "Sam Kwong", "Pareto", "Xingchao Liu", "Xiaojie Jin", "Peter Stone", "Qiang Liu", "Hanxiao Liu", "Karen Simonyan", "Yang", "Shikun Liu", "Ilija Radosavovic", "Paul Michel", "Sebastian Ruder", "Dani Yogatama", "Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert", "Nathan Silberman", "Derek Hoiem", "Rob Fergus", "Idan Achituve", "Haggai Maron", "Gal Chechik", "Ethan Fetaya", "Aviv Shamsian", "Kenji Kawaguchi", "Alex Nichol", "Joshua Achiam", "John Schulman", "Clemens Rosenbaum", "Tim Klinger", "Matthew Riemer", "Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver", "Vladlen Koltun", "Andrew Zisserman", "Trevor Standley", "Amir Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese", "Which", "Ximeng Sun", "Rameswar Panda", "Rogerio Feris", "Adashare", "Simon Vandenhende", "Stamatios Georgoulis", "Luc Van Gool", "Ricardo Vilalta", "Youssef Drissi", "Haoxiang Wang", "Han Zhao", "Bo Li", "Dan Xu", "Wanli Ouyang", "Xiaogang Wang", "Nicu Sebe", "Feiyang Ye", "Baijiong Lin", "Zhixiong Yue", "Pengxin Guo", "Qiao Xiao", "Yu Zhang", "Teresa Yeo", "Fatih Kar", "Robustness", "Saurabh Kumar", "Abhishek Gupta", "Karol Hausman", "Amir R Zamir", "Alexander Sax", "Nikhil Cheerla", "Rohan Suri", "Zhangjie Cao", "Leonidas J Guibas", "Robust", "William Shen", "Taskonomy", "Adam", "Robotic Manipulation Tasks Naively", "Tasks", "Depth Normal Noise Sem", "Part Seg", "Disp", "Split", "Reach Target Put Money", "Push Button Pick Up Umbrella Pick", "Lift Pick", "Lift Pick Up Umbrella Pick Up Cup Pick Up Cup Pick Up Umbrella Slide Block", "Target Put Knife", "Board Pick Up Umbrella Slide Block", "Target Push Button Pick", "Lift Pick Up Cup Knife", "Board Pick Up Umbrella Put Money", "Put Money", "Pick Up Umbrella Put Money", "Board Stack Wine Pick Up Umbrella Push Button Slide Block", "Pick Up Umbrella Put Knife", "People", "Vegetables Large Carnivores", "Large", "Herbivores"]}{"title": ["Data set creation and empirical analysis for detecting signs of depression from social media postings"], "authors": ["[arxiv.Result.Author('Kayalvizhi S'), arxiv.Result.Author('Thenmozhi D')]"], "link": ["http://arxiv.org/pdf/2202.03047v1"], "summary": "Depression is a common mental illness that has to be detected and treated at\nan early stage to avoid serious consequences. There are many methods and\nmodalities for detecting depression that involves physical examination of the\nindividual. However, diagnosing mental health using their social media data is\nmore effective as it avoids such physical examinations. Also, people express\ntheir emotions well in social media, it is desirable to diagnose their mental\nhealth using social media data. Though there are many existing systems that\ndetects mental illness of a person by analysing their social media data,\ndetecting the level of depression is also important for further treatment.\nThus, in this research, we developed a gold standard data set that detects the\nlevels of depression as `not depressed', `moderately depressed' and `severely\ndepressed' from the social media postings. Traditional learning algorithms were\nemployed on this data set and an empirical analysis was presented in this\npaper. Data augmentation technique was applied to overcome the data imbalance.\nAmong the several variations that are implemented, the model with Word2Vec\nvectorizer and Random Forest classifier on augmented data outperforms the other\nvariations with a score of 0.877 for both accuracy and F1 measure.", "entities_include_in_text": [], "entities_from_reference": ["Tuka Al Hanai", "Mohammad M Ghassemi", "James R Glass", "Hamdi Dibeklio", "Zakia Hammal", "Ying Yang", "Jeffrey F Cohn", "Multimodal", "Sharifa Alghowinem", "Michael Wagner", "Julien Epps", "Matthew Hyett", "Gordon Parker", "Michael Breakspear", "Affective Computing", "Arindam Jati", "Prashanth Gurunath Shivakumar", "Sandeep Nallan Chakravarthula", "Panayiotis Georgiou", "Jana M Havigerova", "Jiri Haviger", "Dalibor Kucera", "Petra Hoffmannova", "Maxim Stankevich", "Andrey Latyshev", "Evgenia Kuminskaya", "Ivan Smirnov", "Oleg Grigoriev", "Michelle Renee Morales", "Rivka Levitan", "Speech", "Misato Hiraga", "Atreyee Mukherjee", "Zeeshan Ali Sayyed", "Matthew Millard", "Language Cognition", "Johannes C Eichstaedt", "Robert J Smith", "Raina M Merchant", "Lyle H Ungar", "Patrick Crutchley", "Daniel Preo", "David A Asch", "Andrew Schwartz", "Facebook", "Andrew G Reece", "Andrew J Reagan", "Katharina LM Lix", "Peter Sheridan Dodds", "Christopher M Danforth", "Ellen J Langer", "Sho Tsugawa", "Yusuke Kikuchi", "Fumio Kishino", "Kosuke Nakajima", "Yuichi Itoh", "Hiroyuki Ohsaki", "Mandar Deshpande", "Vignesh Rao", "Chenhao Lin", "Pengwei Hu", "Hui Su", "Shaochun Li", "Jing Mei", "Jie Zhou", "Henry Leung", "Thin Nguyen", "Dinh Phung", "Bo Dao", "Svetha Venkatesh", "Michael Berk", "Affective", "Yevhen Tyshchenko", "Nature Precis", "Inst", "Tartu", "Science", "David E Losada", "Fabio Crestani", "Javier Parapar", "Springer", "Michael M. Tadesse", "Hongfei Lin", "Bo Xu", "Liang Yang", "Inna Pirina", "Coltekin", "Health Applications Workshop", "Shared Task", "Hannah Yao", "Sina Rashidian", "Xinyu Dong", "Hongyi Duanmu", "Richard N Rosenthal", "Fusheng Wang", "Machine", "Nick Boettcher", "Massimo Poesio", "Jacob Cohen", "Machine Learning", "Richard Landis", "Gary G Koch", "Health", "Michel", "J. Vanderplas", "Machine Learning Research", "Martin F Porter", "Jeffrey Pennington", "Richard Socher", "Christopher D Manning", "Glove", "Smote"]}{"title": ["Measuring and Reducing Model Update Regression in Structured Prediction for NLP"], "authors": ["[arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Elman Mansimov'), arxiv.Result.Author('Yi-An Lai'), arxiv.Result.Author('Yixuan Su'), arxiv.Result.Author('Lei Shu'), arxiv.Result.Author('Yi Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02976v1"], "summary": "Recent advance in deep learning has led to rapid adoption of machine learning\nbased NLP models in a wide range of applications. Despite the continuous gain\nin accuracy, backward compatibility is also an important aspect for industrial\napplications, yet it received little research attention. Backward compatibility\nrequires that the new model does not regress on cases that were correctly\nhandled by its predecessor. This work studies model update regression in\nstructured prediction tasks. We choose syntactic dependency parsing and\nconversational semantic parsing as representative examples of structured\nprediction tasks in NLP. First, we measure and analyze model update regression\nin different model update settings. Next, we explore and benchmark existing\ntechniques for reducing model update regression including model ensemble and\nknowledge distillation. We further propose a simple and effective method,\nBackward-Congruent Re-ranking (BCR), by taking into account the characteristics\nof structured output. Experiments show that BCR can better mitigate model\nupdate regression than model ensemble and knowledge distillation approaches.", "entities_include_in_text": ["Shen et al.,\n2020; Yan et al., 2021", "Xie et al., 2021", "e.g., Ma et al., 2018", "Shen et al., 2020; Yan et al., 2021; Xie et al., 2021", "Yan et al., 2021; Xie et al., 2021", "Smith, 2011", "Yan et al., 2021", "Qi et al., 2020", "Petrov et al., 2012", "Ma et al.,\n2018", "Gupta et al., 2018", "Rongali et al., 2020", "Liu et al., 2019", "Shen et al., 2020; Yan et al., 2021; Xie et al., 2021", "Yan et al.,\n2021; Xie et al., 2021", "Zmigrod et al., 2020; 2021", "Fan et al., 2018; Radford et al.,\n2019", "Holtzman et al., 2019", "Srivastava et al.,\n2014", "Shen et al., 2004;\nYan et al., 2021; Xie et al., 2021", "Ma et al., 2018", "Rongali et al.,\n2020", "Ott et al., 2019", "Liu et al., 2019", "Zmigrod et al., 2020; 2021", "Fan et al., 2018; Radford et al., 2019", "Holtzman et al., 2019", "Yan et al., 2021; Xie\net al., 2021", "Yan et al.,\n2021", "Li et al., 2016", "Geyik et al., 2019", "Falke et al., 2019", "Parisi et al.,\n2019; Biesialska et al., 2020", "Toneva et al., 2019", "Ribeiro et al., 2020"], "entities_from_reference": ["Caruana", "Biesialska", "Bucilua", "Model", "Collins", "Koo", "Rehbein", "Dozat", "Falke", "Annual", "Fan", "Lewis", "Dauphin", "Gepperth", "Geyik", "Ambler", "Kenthapadi", "Gupta", "Shah", "Mohit", "Kumar", "Hinton", "Vinyals", "Holtzman", "Buys", "Choi", "Kim", "Kriz", "Sedoc", "Apidianaki", "Zheng", "Miltsakaki", "Human Language Technologies", "Short Papers", "Nivre", "Synthesis", "Le", "Zuidema", "Galley", "Brockett", "Gao", "Dolan", "Liu", "Ott", "Goyal", "Joshi", "Levy", "Stoyanov", "Zhang", "Data Engineering", "Model Update", "Peng", "Hovy", "Shen", "Sarkar", "Och", "Masana", "Twardowski", "J.", "Xia", "Pattern Recognition", "Edunov", "Baevski", "Gross", "Auli", "Parisi", "J. L.", "Kanan", "Wermter", "Petrov", "Das", "Radford", "Luan", "Ribeiro", "Guestrin", "Singh", "Romero", "Kahou", "Chassang", "Gatta", "Bengio", "Rongali", "Monti", "Salazar", "Liang", "Kirchhoff", "Smith", "Bauer", "Srivastava", "Krizhevsky", "Salakhutdinov", "Toneva", "Trischler", "Gordon", "Kleindessner", "Locatello", "Sch", "Gehler", "Xie", "Lai", "Yan", "Kundu", "Yang", "Deng", "Wang", "Yee", "Joo", "Bae", "Yin", "Zagoruyko", "Komodakis", "Zliobait", "Zmigrod", "Vieira", "Old", "Given"]}{"title": ["Graph Self-supervised Learning with Accurate Discrepancy Learning"], "authors": ["[arxiv.Result.Author('Dongki Kim'), arxiv.Result.Author('Jinheon Baek'), arxiv.Result.Author('Sung Ju Hwang')]"], "link": ["http://arxiv.org/pdf/2202.02989v1"], "summary": "Self-supervised learning of graph neural networks (GNNs) aims to learn an\naccurate representation of the graphs in an unsupervised manner, to obtain\ntransferable representations of them for diverse downstream tasks. Predictive\nlearning and contrastive learning are the two most prevalent approaches for\ngraph self-supervised learning. However, they have their own drawbacks. While\nthe predictive learning methods can learn the contextual relationships between\nneighboring nodes and edges, they cannot learn global graph-level similarities.\nContrastive learning, while it can learn global graph-level similarities, its\nobjective to maximize the similarity between two differently perturbed graphs\nmay result in representations that cannot discriminate two similar graphs with\ndifferent properties. To tackle such limitations, we propose a framework that\naims to learn the exact discrepancy between the original and the perturbed\ngraphs, coined as Discrepancy-based Self-supervised LeArning (D-SLA).\nSpecifically, we create multiple perturbations of the given graph with varying\ndegrees of similarity and train the model to predict whether each graph is the\noriginal graph or a perturbed one. Moreover, we further aim to accurately\ncapture the amount of discrepancy for each perturbed graph using the graph edit\ndistance. We validate our method on various graph-related downstream tasks,\nincluding molecular property prediction, protein function prediction, and link\nprediction tasks, on which our model largely outperforms relevant baselines.", "entities_include_in_text": ["Fan et al., 2019", "Baek et al., 2020", "Muzio\net al., 2021", "Xie et al., 2021", "Gilmer et al., 2017", "Hamilton et al., 2017", "Velickovic et al., 2018", "Xu et al., 2019", "Velickovic et al., 2019; Sun et al., 2020", "You et al.,\n2020; Zhu et al., 2021; You et al., 2021", "Suresh et al., 2021", "Yang et al., 2021", "Gilmer et al., 2017", "Ying et al.,\n2018; Baek et al., 2021", "You\net al., 2020; Zhu et al., 2021; You et al., 2021", "Zeng et al.,\n2009", "Zeng et al., 2009", "You et al., 2020; 2021", "Hamilton et al., 2017", "Velickovic\net al., 2019", "You et al., 2020", "You et al.,\n2021", "Xu et al., 2019", "Morris et al., 2020", "Velickovic et al., 2019", "You et al., 2020", "You et al., 2021", "You et al., 2021", "Zitnik et al., 2019", "Xu et al., 2019", "Morris et al., 2020", "You et al., 2020; 2021", "Morris et al., 2020"], "entities_from_reference": ["Lee", "Hwang", "Baek", "Kang", "Chen", "Machine Learning", "Fan", "Li", "Zhao", "Tang", "Yin", "Gao", "Kipf", "J.", "Leman", "Weisfeiler", "Morris", "Bause", "Neumann", "Muzio", "Borgwardt", "Briefings Bioinform.", "Hahn", "Gilmer", "Schoenholz", "Riley", "Vinyals", "Dahl", "Rong", "Xie", "Wei", "Huang", "Hamilton", "Leskovec", "J. Inductive", "Girshick", "Momentum", "Pattern Recognition", "Liu", "Gomes", "Liang", "Pande", "Wang", "Chang", "Data Mining", "Kwon", "Kim", "Sanfeliu", "Schroff", "Kalenichenko", "Philbin", "J. Facenet", "Irwin", "J. Zinc", "J. Infograph", "Peng", "Hao", "J. Adversarial", "Graph", "Tung", "Feng", "Zhang", "Xia", "Zhu", "Zitnik", "Sosic", "Feldman", "J. Evolution", "Van", "Maaten", "Velickovic", "Casanova", "Bengio", "Fedus", "Hjelm", "Pappu", "Chemical", "Lin", "Shi", "Zhou", "Yang", "Luo", "Jegelka", "Guo", "Ren", "J. Hierarchical", "Sui", "Experimental Details", "Graph Classification Dataset", "Dataset Tasks Graphs Avg", "Nodes Avg", "Edges Chemical Domain", "Domain PPI", "Edges", "Graph Isomorphism Networks", "Implementation Details", "Molecular Property", "Double", "Triple", "Protein Function", "Link", "Astro Physics", "Dataset Table", "Graphs Avg", "Edges Pert", "Strategy", "Appendix", "Analysis Rank Correlation", "Dataset Analysis", "Contrarily", "Tox21 Figure", "Green", "Lmargin", "Anchor", "Molecules Graph", "Ledit", "Lmargin Figure"]}{"title": ["Locally Differentially Private Distributed Deep Learning via Knowledge Distillation"], "authors": ["[arxiv.Result.Author('Di Zhuang'), arxiv.Result.Author('Mingchen Li'), arxiv.Result.Author('J. Morris Chang')]"], "link": ["http://arxiv.org/pdf/2202.02971v1"], "summary": "Deep learning often requires a large amount of data. In real-world\napplications, e.g., healthcare applications, the data collected by a single\norganization (e.g., hospital) is often limited, and the majority of massive and\ndiverse data is often segregated across multiple organizations. As such, it\nmotivates the researchers to conduct distributed deep learning, where the data\nuser would like to build DL models using the data segregated across multiple\ndifferent data owners. However, this could lead to severe privacy concerns due\nto the sensitive nature of the data, thus the data owners would be hesitant and\nreluctant to participate. We propose LDP-DL, a privacy-preserving distributed\ndeep learning framework via local differential privacy and knowledge\ndistillation, where each data owner learns a teacher model using its own\n(local) private dataset, and the data user learns a student model to mimic the\noutput of the ensemble of the teacher models. In the experimental evaluation, a\ncomprehensive comparison has been made among our proposed approach (i.e.,\nLDP-DL), DP-SGD, PATE and DP-FL, using three popular deep learning benchmark\ndatasets (i.e., CIFAR10, MNIST and FashionMNIST). The experimental results show\nthat LDP-DL consistently outperforms the other competitors in terms of privacy\nbudget and model accuracy.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Comprehensive survey of computational learning methods for analysis of gene expression data in genomics"], "authors": ["[arxiv.Result.Author('Nikita Bhandari'), arxiv.Result.Author('Rahee Walambe'), arxiv.Result.Author('Ketan Kotech'), arxiv.Result.Author('Satyajeet Khare')]"], "link": ["http://arxiv.org/pdf/2202.02958v1"], "summary": "Computational analysis methods including machine learning have a significant\nimpact in the fields of genomics and medicine. High-throughput gene expression\nanalysis methods such as microarray technology and RNA sequencing produce\nenormous amounts of data. Traditionally, statistical methods are used for\ncomparative analysis of the gene expression data. However, more complex\nanalysis for classification and discovery of feature genes or sample\nobservations requires sophisticated computational approaches. In this review,\nwe compile various statistical and computational tools used in analysis of\nexpression microarray data. Even though, the methods are discussed in the\ncontext of expression microarray data, they can also be applied for the\nanalysis of RNA sequencing or quantitative proteomics datasets. We specifically\ndiscuss methods for missing value (gene expression) imputation, feature gene\nscaling, selection and extraction of features for dimensionality reduction, and\nlearning and analysis of expression data. We discuss the types of missing\nvalues and the methods and approaches usually employed in their imputation. We\nalso discuss methods of data transformation and feature scaling viz.\nnormalization and standardization. Various approaches used in feature selection\nand extraction are also reviewed. Lastly, learning and analysis methods\nincluding class comparison, class prediction, and class discovery along with\ntheir evaluation parameters are described in detail. We have described the\nprocess of generation of a microarray gene expression data along with\nadvantages and limitations of the above-mentioned techniques. We believe that\nthis detailed review will help the users to select appropriate methods based on\nthe type of data and the expected outcome.", "entities_include_in_text": ["Segundo-Val  and  Sanz-Lozano \n2016", "Smyth  et  al.  2005", "Newton \n2001; Rubinstein et al. 2003; Svensson 2016", "Bilban  et  al.  2002", "Freyhult  et  al.  2010", "Cheng et al. 2016", "Agrahari  et  al.  2018", "Collobert and Weston 2008; Neubauer 1998"], "entities_from_reference": ["Plusieurs"]}{"title": ["Tractable Boolean and Arithmetic Circuits"], "authors": ["[arxiv.Result.Author('Adnan Darwiche')]"], "link": ["http://arxiv.org/pdf/2202.02942v1"], "summary": "Tractable Boolean and arithmetic circuits have been studied extensively in AI\nfor over two decades now. These circuits were initially proposed as \"compiled\nobjects,\" meant to facilitate logical and probabilistic reasoning, as they\npermit various types of inference to be performed in linear-time and a\nfeed-forward fashion like neural networks. In more recent years, the role of\ntractable circuits has significantly expanded as they became a computational\nand semantical backbone for some approaches that aim to integrate knowledge,\nreasoning and learning. In this article, we review the foundations of tractable\ncircuits and some associated milestones, while focusing on their core\nproperties and techniques that make them particularly useful for the broad aims\nof neuro-symbolic AI.", "entities_include_in_text": [], "entities_from_reference": ["Durgesh Agrawal", "Yash Pote", "Kuldeep S. Meel", "Jatin Arora", "Supratik Chakraborty", "Shankara Narayanan Krishna", "Divya Raghunathan", "Antoine Amarilli", "Marcelo Arenas", "Pablo Barcelo Leopoldo Bertossi", "Mikael Monet", "Gilles Audemard", "Frederic Koriche", "Pierre Marquis", "Paul Beame", "Jerry Li", "Sudeepa Roy", "Dan Suciu", "Exact", "Database Syst.", "Vaishak Belle", "Luc De Raedt", "Tarek R. Besold", "Artur", "Sebastian Bader", "Howard Bowman", "Pedro Domingos", "Pascal Hitzler", "Luis C. Lamb", "Daniel Lowd", "Priscila Machado Vieira Lima", "Leo", "Gadi Pinkas", "Hoifung Poon", "Gerson Zaverucha", "Matthias Buttkus", "Florent Capelli", "Stefan Mengel", "Friedrich Slivovsky", "Marco Cadoli", "Francesco M. Donini", "Adnan Darwiche", "Artif", "Mark Chavira", "Alessandro Saffiotti", "Book Center", "Lecture Notes", "Springer", "Arthur Choi", "Machine Learning Research", "Dynamic", "Machine Learning", "Doga Kisa", "Yujia Shen", "Nazgol Tavabi", "Structured", "Guy Van", "Broeck", "Ruocheng Wang", "Yexiang Xue", "Stephen", "Meihua Dang", "Antonio Vergari", "Strudel", "Non Class", "Logics", "Dieter Fensel", "Fausto Giunchiglia", "Deborah L. McGuinness", "Williams", "Knowledge Representation", "Morgan Kaufmann", "Bayesian Networks", "Cambridge University Press", "Practical Approaches", "Hard Problems", "Causal", "Auguste Hirth", "Stefan Szeider", "Dagstuhl Reports", "Anton Lykov", "Maximilian Schleich", "Pierfrancesco Ardino", "Jacopo Gobbi", "Paolo Morettin", "Stefano Teso", "Andrea Passerini", "Paulius Dilkas", "Anton Dries", "Angelika Kimmig", "Wannes Meert", "Joris Renkens", "Jonas Vlasselaer", "Dimitar Sht", "Shterionov", "Bernd Gutmann", "Ingo Thon", "Gerda Janssens", "Abram L. Friesen", "Pedro M. Domingos", "Garcez", "Jordan Gergov", "Christoph Meinel", "Hitzler", "Md Kamruzzaman Sarker", "Steven Holtzen", "Todd D. Millstein", "Huang", "Xuanxiang Huang", "Yacine Izza", "Alexey Ignatiev", "Martin C. Cooper", "Nicholas Asher", "Joao", "John T. Gill III", "Kumar Jha", "Kimmig", "Algebraic", "Applied Logic", "Roland H.", "Kushagra Chandak", "Akshat Kumar", "Robin Manhaeve", "Sebastijan Dumancic", "Thomas Demeester", "Denis Deratani Maua", "Cassio Polpo", "Alejandro Molina", "Karl Stelzner", "Robert Peharz", "Pranav Subramani", "Nicola Di Mauro", "Pascal Poupart", "Kristian Kersting", "Christian J. Muise", "J. Christopher Beck", "Eric I. Hsu", "Dsharp", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata", "James P. Delgrande", "Frank Wolter", "Cape Town", "James D. Park", "Robert Gens", "Franz Pernkopf", "Pattern Analysis", "Machine Intelligence", "Steven Lang", "Martin Trapp", "Zoubin Ghahramani", "Einsum", "Bound Computation Algorithm", "Giuseppe Marra", "Dan Roth", "Rajhans Samdani", "Mach", "Saad", "Martin C. Rinard", "Vikash K. Mansinghka", "Tian Sang", "Fahiem Bacchus", "Henry A. Kautz", "Toniann Pitassi", "Bart Selman", "Aman Bansal", "Shubham Sharma", "Rahul Gupta", "Subhajit Roy", "Anchal Goyanka", "Haiying Huang", "Weijia Shi", "Andy Shih", "Solomon Eyal Shimony", "Leslie G. Valiant", "Anji Liu", "Klaus W. Wagner", "Acta Informatica", "Ziwei Xu", "Mohan S. Kankanhalli", "Harold Soh", "Jingyi Xu", "Zilu Zhang", "Yitao Liang", "Zhao", "Geoffrey J. Gordon"]}{"title": ["Jury Learning: Integrating Dissenting Voices into Machine Learning Models"], "authors": ["[arxiv.Result.Author('Mitchell L. Gordon'), arxiv.Result.Author('Michelle S. Lam'), arxiv.Result.Author('Joon Sung Park'), arxiv.Result.Author('Kayur Patel'), arxiv.Result.Author('Jeffrey T. Hancock'), arxiv.Result.Author('Tatsunori Hashimoto'), arxiv.Result.Author('Michael S. Bernstein')]"], "link": ["http://arxiv.org/pdf/2202.02950v1"], "summary": "Whose labels should a machine learning (ML) algorithm learn to emulate? For\nML tasks ranging from online comment toxicity to misinformation detection to\nmedical diagnosis, different groups in society may have irreconcilable\ndisagreements about ground truth labels. Supervised ML today resolves these\nlabel disagreements implicitly using majority vote, which overrides minority\ngroups' labels. We introduce jury learning, a supervised ML approach that\nresolves these disagreements explicitly through the metaphor of a jury:\ndefining which people or groups, in what proportion, determine the classifier's\nprediction. For example, a jury learning model for online toxicity might\ncentrally feature women and Black jurors, who are commonly targets of online\nharassment. To enable jury learning, we contribute a deep learning architecture\nthat models every annotator in a dataset, samples from annotators' models to\npopulate the jury, then runs inference to classify. Our architecture enables\njuries that dynamically adapt their composition, explore counterfactuals, and\nvisualize dissent.", "entities_include_in_text": ["Oct 2016", "Dec. 2017", "oct 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Prompt-Guided Injection of Conformation to Pre-trained Protein Model"], "authors": ["[arxiv.Result.Author('Qiang Zhang'), arxiv.Result.Author('Zeyuan Wang'), arxiv.Result.Author('Yuqiang Han'), arxiv.Result.Author('Haoran Yu'), arxiv.Result.Author('Xurui Jin'), arxiv.Result.Author('Huajun Chen')]"], "link": ["http://arxiv.org/pdf/2202.02944v1"], "summary": "Pre-trained protein models (PTPMs) represent a protein with one fixed\nembedding and thus are not capable for diverse tasks. For example, protein\nstructures can shift, namely protein folding, between several conformations in\nvarious biological processes. To enable PTPMs to produce task-aware\nrepresentations, we propose to learn interpretable, pluggable and extensible\nprotein prompts as a way of injecting task-related knowledge into PTPMs. In\nthis regard, prior PTPM optimization with the masked language modeling task can\nbe interpreted as learning a sequence prompt (Seq prompt) that enables PTPMs to\ncapture the sequential dependency between amino acids. To incorporate\nconformational knowledge to PTPMs, we propose an interaction-conformation\nprompt (IC prompt) that is learned through back-propagation with the\nprotein-protein interaction task. As an instantiation, we present a\nconformation-aware pre-trained protein model that learns both sequence and\ninteraction-conformation prompts in a multi-task setting. We conduct\ncomprehensive experiments on nine protein datasets. Results confirm our\nexpectation that using the sequence prompt does not hurt PTPMs' performance on\nsequence-related tasks while incorporating the interaction-conformation prompt\nsignificantly improves PTPMs' performance on tasks where conformational\nknowledge counts. We also show the learned prompts can be combined and extended\nto deal with new complex tasks.", "entities_include_in_text": ["Epstein et al., 1963", "Devlin et al., 2019", "Zhang et al., 2019", "Rao\net al., 2019", "Elnaggar et al., 2021", "Berman et al., 2000", "Dunbar\net al., 2013", "J et al., 2018", "RF et al., 2006", "Dosovitskiy et al.,\n2021) and proteins. Rives et al. (2021", "Devlin et al., 2019", "Jumper et al., 2021", "Brown et al., 2020", "Ham-\nbardzumyan et al., 2021", "Szk-\nlarczyk et al., 2019", "Dunbar\net al., 2013", "Rao et al., 2019", "Dunbar et al., 2013", "Paszke et al., 2019", "Ott et al.,\n2019", "Hashemifar et al., 2018", "H et al.,\n2018", "Chen et al., 2019", "Chen\net al., 2019", "Elnag-\ngar et al., 2021", "Humphreys et al., 2021", "Kruskal, 1964", "Yan et al., 2008", "J et al.,\n2018", "Prendergast FG,\n1978", "M et al., 1996"], "entities_from_reference": ["Elnaggar", "Heinzinger", "Dallago", "Wang", "Pattern Analysis", "Machine Intelligence", "Berman", "Feng", "Bhat", "Weissig", "Shindyalov", "Nucleic", "Epstein", "Goldberger", "Harbor Symposia", "Brown", "Mann", "Ryder", "Subbiah", "Kaplan", "J. D.", "Ranzato", "Hadsell", "Lin", "Protein Structure", "Academic Press", "Chen", "Zhou", "Zhang", "Chang", "Zaniolo", "Devlin", "Lee", "Toutanova", "Human Language Technologies", "Short Papers", "Minnesota", "Dosovitskiy", "Kolesnikov", "Zhai", "Dehghani", "Minderer", "Heigold", "Gelly", "Uszkoreit", "Dunbar", "Leem", "Baker", "Fuchs", "Georges", "Shi", "Deane", "Nucleic Acids Research", "Gao", "Fisch", "Deep Neural Network", "Protein Interactions Using", "Molecules", "Hambardzumyan", "Hashemifar", "Neyshabur", "Khan", "Humphreys", "Pei", "Baek", "Ovchinnikov", "Ness", "Science", "Jumper", "Figurnov", "Ronneberger", "Bates", "Potapenko", "Highly", "Nature", "Protein Model Kruskal", "J. Multidimensional", "Psychometrika", "Paszke", "Gross", "Lerer", "Bradbury", "Red Hook", "Le Scao", "Lester", "Online", "Punta Cana", "Liang", "Liu", "Yuan", "Jiang", "Hayashi", "Zheng", "Yang", "Tang", "J. GPT", "Main Track", "James", "Hutchison", "Ott", "Edunov", "Baevski", "Fan", "Auli", "Biochemistry", "Raffel", "Shazeer", "Roberts", "Narang", "Matena", "Li", "Machine Learning Research", "Rao", "Thomas", "Duan", "Meier", "Verkuil", "Machine Learning", "Goyal", "Guo", "Zitnick", "Schick", "Sch", "Volume", "Szklarczyk", "Gable", "Lyon", "Junge", "Protein Model", "Vig", "Madani", "Rajani", "Yan", "Dobbs", "Lan", "Pang", "Annual", "Zhu", "Shao", "Dauphin", "Vaughan", "J. W", "Deng", "Huang", "Helix"]}{"title": ["Soft Actor-Critic with Inhibitory Networks for Faster Retraining"], "authors": ["[arxiv.Result.Author('Jaime S. Ide'), arxiv.Result.Author('Daria Mi\u0107ovi\u0107'), arxiv.Result.Author('Michael J. Guarino'), arxiv.Result.Author('Kevin Alcedo'), arxiv.Result.Author('David Rosenbluth')]"], "link": ["http://arxiv.org/pdf/2202.02918v1"], "summary": "Reusing previously trained models is critical in deep reinforcement learning\nto speed up training of new agents. However, it is unclear how to acquire new\nskills when objectives and constraints are in conflict with previously learned\nskills. Moreover, when retraining, there is an intrinsic conflict between\nexploiting what has already been learned and exploring new skills. In soft\nactor-critic (SAC) methods, a temperature parameter can be dynamically adjusted\nto weight the action entropy and balance the explore $\\times$ exploit\ntrade-off. However, controlling a single coefficient can be challenging within\nthe context of retraining, even more so when goals are contradictory. In this\nwork, inspired by neuroscience research, we propose a novel approach using\ninhibitory networks to allow separate and adaptive state value evaluations, as\nwell as distinct automatic entropy tuning. Ultimately, our approach allows for\ncontrolling inhibition to handle conflict between exploiting less risky,\nacquired behaviors and exploring novel ones to overcome more challenging tasks.\nWe validate our method through experiments in OpenAI Gym environments.", "entities_include_in_text": ["Schaul et al., 2015", "Bacon et al., 2017", "Vezhnevets et al., 2017", "Nachum et al., 2018", "Van Seijen et al., 2017; Sahni et al., 2017; Haarnoja et al., 2017; Hansen et al., 2020;\nBarreto et al., 2020", "Diamond, 2013", "Haarnoja et al., 2017; Van Niekerk et al., 2019", "Botvinick et al., 2019", "Hong et al., 2018", "Puterman, 1994", "Ziebart, 2010", "Thomas, 2014", "Lillicrap et al., 2016", "Haarnoja et al., 2017", "Schul-\nman et al., 2015", "Mnih et al., 2015", "Aron, 2007; Diamond, 2013). In Shenoy et al. (2011", "Logan\net al., 2015", "Shenoy et al., 2011", "Ide et al., 2013", "Haarnoja et al., 2017; Van Niekerk\net al., 2019), the goal is to model a new task by composing value functions previously trained\non sub-tasks. In Todorov (2009", "Todorov, 2007", "Van Seijen et al., 2017", "Mnih et al., 2015", "Brys et al., 2015", "Schaal, 1996", "Ammar et al., 2012", "Aron, 2007", "Fujimoto et al., 2018", "Tulving, 2002", "Blundell et al., 2016; Lin et al., 2018; Botvinick et al., 2019", "Brockman et al., 2016", "Logan et al., 2015", "Braver et al., 2001", "AAMAS 2010", "ICLR\n2016", "Seymour et al., 2007", "Martin, 2012). Negative rewards are often successfully used in RL.\nHowever, there are studies showing that intermixing positive and sparse negative rewards might\nhave adverse effects in learning, as reported in a recent work using DDPG in robotic environ-\nment Vargas et al. (2020", "Hu et al., 2020"], "entities_from_reference": ["Plusieurs"]}{"title": ["Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks"], "authors": ["[arxiv.Result.Author('Seyyedali Hosseinalipour'), arxiv.Result.Author('Su Wang'), arxiv.Result.Author('Nicolo Michelusi'), arxiv.Result.Author('Vaneet Aggarwal'), arxiv.Result.Author('Christopher G. Brinton'), arxiv.Result.Author('David J. Love'), arxiv.Result.Author('Mung Chiang')]"], "link": ["http://arxiv.org/pdf/2202.02947v1"], "summary": "Federated learning (FedL) has emerged as a popular technique for distributing\nmodel training over a set of wireless devices, via iterative local updates (at\ndevices) and global aggregations (at the server). In this paper, we develop\n\\textit{parallel successive learning} (PSL), which expands the FedL\narchitecture along three dimensions: (i) Network, allowing decentralized\ncooperation among the devices via device-to-device (D2D) communications. (ii)\nHeterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers\nheterogeneous number of stochastic gradient descent iterations with different\nmini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic\nenvironment with data arrival and departure, where the distributions of local\ndatasets evolve over time, captured via a new metric for model/concept drift.\n(ii-c) Device: PSL considers devices with different computation and\ncommunication capabilities. (iii) Proximity, where devices have different\ndistances to each other and the access point. PSL considers the realistic\nscenario where global aggregations are conducted with idle times in-between\nthem for resource efficiency improvements, and incorporates data dispersion and\nmodel dispersion with local model condensation into FedL. Our analysis sheds\nlight on the notion of cold vs. warmed up models, and model inertia in\ndistributed machine learning. We then propose network-aware dynamic model\ntracking to optimize the model learning vs. resource efficiency tradeoff, which\nwe show is an NP-hard signomial programming problem. We finally solve this\nproblem through proposing a general optimization solver. Our numerical results\nreveal new findings on the interdependencies between the idle times in-between\nthe global aggregations, model/concept drift, and D2D cooperation\nconfiguration.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Universality of parametric Coupling Flows over parametric diffeomorphisms"], "authors": ["[arxiv.Result.Author('Junlong Lyu'), arxiv.Result.Author('Zhitang Chen'), arxiv.Result.Author('Chang Feng'), arxiv.Result.Author('Wenjing Cun'), arxiv.Result.Author('Shengyu Zhu'), arxiv.Result.Author('Yanhui Geng'), arxiv.Result.Author('Zhijie Xu'), arxiv.Result.Author('Yongwei Chen')]"], "link": ["http://arxiv.org/pdf/2202.02906v1"], "summary": "Invertible neural networks based on Coupling Flows CFlows) have various\napplications such as image synthesis and data compression. The approximation\nuniversality for CFlows is of paramount importance to ensure the model\nexpressiveness. In this paper, we prove that CFlows can approximate any\ndiffeomorphism in C^k-norm if its layers can approximate certain\nsingle-coordinate transforms. Specifically, we derive that a composition of\naffine coupling layers and invertible linear transforms achieves this\nuniversality. Furthermore, in parametric cases where the diffeomorphism depends\non some extra parameters, we prove the corresponding approximation theorems for\nour proposed parametric coupling flows named Para-CFlows. In practice, we apply\nPara-CFlows as a neural surrogate model in contextual Bayesian optimization\ntasks, to demonstrate its superiority over other neural surrogate models in\nterms of optimization performance.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Redactor: Targeted Disinformation Generation using Probabilistic Decision Boundaries"], "authors": ["[arxiv.Result.Author('Geon Heo'), arxiv.Result.Author('Steven Euijong Whang')]"], "link": ["http://arxiv.org/pdf/2202.02902v1"], "summary": "Information leakage is becoming a critical problem as various information\nbecomes publicly available by mistake, and machine learning models train on\nthat data to provide services. As a result, one's private information could\neasily be memorized by such trained models. Unfortunately, deleting information\nis out of the question as the data is already exposed to the Web or third-party\nplatforms. Moreover, we cannot necessarily control the labeling process and the\nmodel trainings by other parties either. In this setting, we study the problem\nof targeted disinformation where the goal is to lower the accuracy of inference\nattacks on a specific target (e.g., a person's profile) only using data\ninsertion. While our problem is related to data privacy and defenses against\nexploratory attacks, our techniques are inspired by targeted data poisoning\nattacks with some key differences. We show that our problem is best solved by\nfinding the closest points to the target in the input space that will be\nlabeled as a different class. Since we do not control the labeling process, we\ninstead conservatively estimate the labels probabilistically by combining\ndecision boundaries of multiple classifiers using data programming techniques.\nWe also propose techniques for making the disinformation realistic. Our\nexperiments show that a probabilistic decision boundary can be a good proxy for\nlabelers, and that our approach outperforms other targeted poisoning methods\nwhen using end-to-end training on real datasets.", "entities_include_in_text": [], "entities_from_reference": ["How Photos", "Accessed", "Identify Risky Clients", "Jeff Larson", "Surya Mattu", "Lauren Kirchner", "Machine", "Xavier Renard", "Jonathan Aigrain", "Thibault Laugel", "Pascal Frossard", "Marcin Detyniecki", "Tabular Data", "Lise Getoor", "Louis Licamele", "Vadim Borisov", "Tobias Leemann", "Kathrin Seler", "Johannes Haug", "Martin Pawelczyk", "Gjergji Kasneci", "Lucas Bourtoule", "Varun Chandrasekaran", "Christopher", "Hengrui Jia", "Adelin Travers", "Baiwu Zhang", "David Lie", "Nicolas Papernot", "Machine Unlearning", "Haipeng Chen", "Sushil Jajodia", "Jing Liu", "Noseong Park", "Vadim Sokolov", "Bounded Real Data", "Xinyun Chen", "Chang Liu", "Bo Li", "Kimberly Lu", "Dawn Song", "Backdoor Attacks", "Deep Learning Systems Using Data Poisoning", "Edward Choi", "Siddharth Biswal", "Bradley A. Malin", "Jon Duke", "Walter F. Stewart", "Jimeng Sun", "Vol", "Florian Tramer", "Nicholas Carlini", "Peter Christen", "Data Matching", "Record Linkage", "Entity Resolution", "Springer", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam D. Smith", "Noise", "Aaron Roth", "Found", "Trends Theor", "Ahmed K. Elmagarmid", "Panagiotis G. Ipeirotis", "Vassilios S. Verykios", "Duplicate Record", "Knowl", "Data Eng", "Fredrikson", "Somesh Jha", "Thomas Ristenpart", "Model", "Matthew Fredrikson", "Eric Lantz", "Simon Lin", "David Page", "Antonio Ginart", "Melody Y. Guan", "Gregory Valiant", "James Zou", "Aditya Golatkar", "Alessandro Achille", "Stefano Soatto", "Deep Networks", "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David WardeFarley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio", "Laura Graves", "Vineel Nagisetty", "Vijay Ganesh", "Amnesiac Machine Learning", "Chuan Guo", "Tom Goldstein", "Awni Y. Hannun", "Maaten", "Data Removal", "Machine Learning Models", "Jamie Hayes", "Luca Melis", "George Danezis", "Emiliano De Cristofaro", "Dorjan Hitaj", "Luigi V Mancini", "Mary Anne Smart", "Kamalika Chaudhuri", "James Y. Zou", "Jinyuan Jia", "Ahmed Salem", "Michael Backes", "Yang Zhang", "Neil Zhenqiang Gong", "Mehran Mozaffari Kermani", "Susmita", "Anand Raghunathan", "Niraj K. Jha", "Machine Learning", "Health Informatics", "Ron Kohavi", "Jiacheng Li", "Ninghui Li", "Bruno Ribeiro", "Membership Inference Attacks", "Jialin Pan", "Qiang Yang", "Panagiotis Papadimitriou", "Hector Garcia-Molina", "Data Leakage Detec", "Park", "Mahmoud Mohammadi", "Kshitij Gorde", "Hongkyu Park", "Youngmin Kim", "Data Synthesis", "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer", "Fabian Pedregosa", "Gael Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg", "Erwin Quiring", "Daniel Arp", "Konrad Rieck", "Alexander Ratner", "Stephen H. Bach", "Henry R. Ehrenberg", "Jason Alan Fries", "Christopher Re", "Snorkel", "Rapid Training Data Creation", "Weak Supervision", "Jason", "Alexander J. Ratner", "Set Generation", "Mathias Humbert", "Pascal Berrang", "Mario Fritz", "Data Independent Membership Inference Attacks", "Defenses", "Sebastian Schelter", "Can Forget User Data Very Fast", "Ali Shafahi", "Ronny Huang", "Mahyar Najibi", "Octavian Suciu", "Christoph Studer", "Tudor Dumitras", "Poison Frogs", "Marco Stronati", "Congzheng Song", "Vitaly Shmatikov", "Akash Srivastava", "Lazar Valkov", "Chris Russell", "Michael U. Gutmann", "Charles Sutton", "Mode Collapse", "Implicit Variational Learning", "Beata Strack", "Jonathan P. DeShazo", "Chris Gennings", "Juan L. Olmo", "Sebastian Ventura", "Krzysztof J. Cios", "John N. Clore", "Clinical Database Patient Records", "Radu Marginean", "Yigitcan Kaya", "Hal Daume III", "Does Machine Learning", "Steven Euijong Whang", "Yinjun Wu", "Edgar Dobriban", "Susan B. Davidson", "Maria Skoularidou", "Alfredo Cuesta-Infante", "Kalyan Veeramachaneni", "Tabular", "Chen Zhu", "Hengduo Li", "Gavin Taylor", "Deep Neural Nets"]}{"title": ["Inter-subject Contrastive Learning for Subject Adaptive EEG-based Visual Recognition"], "authors": ["[arxiv.Result.Author('Pilhyeon Lee'), arxiv.Result.Author('Sunhee Hwang'), arxiv.Result.Author('Jewook Lee'), arxiv.Result.Author('Minjung Shin'), arxiv.Result.Author('Seogkyu Jeon'), arxiv.Result.Author('Hyeran Byun')]"], "link": ["http://arxiv.org/pdf/2202.02901v1"], "summary": "This paper tackles the problem of subject adaptive EEG-based visual\nrecognition. Its goal is to accurately predict the categories of visual stimuli\nbased on EEG signals with only a handful of samples for the target subject\nduring training. The key challenge is how to appropriately transfer the\nknowledge obtained from abundant data of source subjects to the subject of\ninterest. To this end, we introduce a novel method that allows for learning\nsubject-independent representation by increasing the similarity of features\nsharing the same class but coming from different subjects. With the dedicated\nsampling principle, our model effectively captures the common knowledge shared\nacross different subjects, thereby achieving promising performance for the\ntarget subject even under harsh problem settings with limited data.\nSpecifically, on the EEG-ImageNet40 benchmark, our model records the top-1 /\ntop-3 test accuracy of 72.6% / 91.6% when using only five EEG samples per class\nfor the target subject. Our code is available at\nhttps://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Inter_Subject_Contrastive_Learning_for_EEG.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Mental Stress Detection using Data from Wearable and Non-wearable Sensors: A Review"], "authors": ["[arxiv.Result.Author('Aamir Arsalan'), arxiv.Result.Author('Syed Muhammad Anwar'), arxiv.Result.Author('Muhammad Majid')]"], "link": ["http://arxiv.org/pdf/2202.03033v1"], "summary": "This paper presents a comprehensive review of methods covering significant\nsubjective and objective human stress detection techniques available in the\nliterature. The methods for measuring human stress responses could include\nsubjective questionnaires (developed by psychologists) and objective markers\nobserved using data from wearable and non-wearable sensors. In particular,\nwearable sensor-based methods commonly use data from electroencephalography,\nelectrocardiogram, galvanic skin response, electromyography, electrodermal\nactivity, heart rate, heart rate variability, and photoplethysmography both\nindividually and in multimodal fusion strategies. Whereas, methods based on\nnon-wearable sensors include strategies such as analyzing pupil dilation and\nspeech, smartphone data, eye movement, body posture, and thermal imaging.\nWhenever a stressful situation is encountered by an individual, physiological,\nphysical, or behavioral changes are induced which help in coping with the\nchallenge at hand. A wide range of studies has attempted to establish a\nrelationship between these stressful situations and the response of human\nbeings by using different kinds of psychological, physiological, physical, and\nbehavioral measures. Inspired by the lack of availability of a definitive\nverdict about the relationship of human stress with these different kinds of\nmarkers, a detailed survey about human stress detection methods is conducted in\nthis paper. In particular, we explore how stress detection methods can benefit\nfrom artificial intelligence utilizing relevant data from various sources. This\nreview will prove to be a reference document that would provide guidelines for\nfuture research enabling effective detection of human stress conditions.", "entities_include_in_text": ["Dempsey, 2018", "Organization\net al., 2015", "Spoorthy et al., 2020; Ransing et al., 2020", "Association et al., 2017", "Can et al., 2020", "Delmastro et al., 2020", "Zubair and Yoon, 2020", "Pluntke et al., 2019", "Gillani et al., 2021", "Gedam and\nPaul, 2021", "Muthukumar and Nachiappan, 2010", "Ulrich-Lai\nand Herman, 2009", "Kajantie and Phillips, 2006", "Werner, 1993", "Segerstrom and Miller, 2004", "Sincero, 2012", "Hiriyappa, 2013", "Salleh, 2008", "Gurung, 2013", "Gowrisankaran et al., 2012", "Schulte-Mecklenbeck et al., 2011", "Subahni et al., 2012", "Wielgosz et al., 2016", "Liew et al., 2015", "Giannakakis et al., 2019", "Rastgoo et al., 2018", "Carneiro et al., 2017", "Thapliyal et al.,\n2017", "Stroop, 1935", "Pujol et al., 2001", "Dedovic et al., 2005", "Setz et al., 2009; Minguillon et al., 2016; Al-Shargie\net al., 2015, 2016", "Hines,\n1932", "Lovallo, 1975", "Suter et al., 2007", "Previnaire et al., 2012", "Frings et al., 2013", "Hassellund et al., 2010", "Shi et al., 2010", "Bitsika et al., 2014", "Taylor et al., 2000", "Dick-\nerson and Kemeny, 2004", "McEwen, 2005", "Kemp et al.,\n2012", "Glaser and Kiecolt-Glaser, 2005", "Pittig et al.,\n2013", "Wolpe, 2013", "Parsons and Rizzo, 2008", "Bordnick et al., 2012", "Kudielka et al., 2009", "Slater et al., 2006; Felnhofer et al.,\n2014", "Slater et al., 2006; Felnhofer et al., 2014; Pertaub et al., 2002", "Kudielka\net al., 2009", "Kelly et al., 2008", "Hemmeter et al., 2005", "Stroud et al., 2002", "Kothgassner et al., 2016", "Escher et al., 1993", "Suda et al., 2008", "Khalfa et al., 2003", "Bartlett, 1996", "Allen et al., 2001", "Knight and Rickard,\n2001", "Evans, 2002", "Lang et al., 1997", "Baltaci and Gokcay, 2016; Liao et al., 2005; Giannakakis et al., 2017; Nhan\nand Chau, 2009; Khalilzadeh et al., 2010", "Lasaitis et al., 2008", "Lo-\nhani et al., 2013", "Dufey et al., 2011", "Caria et al., 2010", "Hajcak and Dennis, 2009", "Styliadis et al., 2015", "Bradley et al., 2001", "Baglioni et al., 2010", "Kirschbaum et al., 1993", "Kurniawan et al., 2013; Engert et al.,\n2014; Vinkers et al., 2013; Nater et al., 2005", "Healey and Pi-\ncard, 2005", "Schneegass et al., 2013", "Birjandta-\nlab et al., 2016", "Taamneh et al., 2017", "Schmidt\net al., 2018", "Koldijk et al., 2014", "Steeneken and Hansen,\n1999", "Markova et al., 2019", "Meziatisabour et al., 2021", "Cohen et al., 1983", "Bryant et al., 2000", "Brantley et al., 1987", "Derogatis and Spencer, 1993", "Schulz and Schlotz, 1999", "Bryant et al., 2000", "Wang\net al., 2010", "Harvey and Bryant, 1998,\n1999, 2000; Brewin et al., 1999", "Harvey and Bryant, 1998", "Derogatis and Spencer, 1993", "Racine et al., 2018", "Stavropoulos et al., 2017", "Fitting et al., 1986", "Corcoran, 1992", "Kaufer et al., 1998", "Greene et al., 1982", "Maslach et al., 1997", "Brantley\nand Jones, 1993", "Goreczny et al.,\n1988", "Mosley Jr et al., 1991; Waggoner, 1986", "Garrett et al., 1991", "Goetsch et al., 1990", "Sturmbauer\net al., 2019", "Buehl et al., 2012", "Healey and Picard, 2005", "Saeed et al., 2020", "Lin et al., 2016", "Shoker et al.,\n2005", "Ala-\nmudun et al., 2012", "Mozos et al.,\n2017; Gjoreski et al., 2017", "Pan and Tompkins, 1985", "Hovsepian et al., 2015", "Elgendi, 2012", "Choi et al., 2011", "Wijsman et al., 2010", "Willigenburg et al., 2012", "Giakoumis et al., 2012; Setz et al., 2009", "Shon et al., 2018", "Saeed et al.,\n2020", "Deng et al., 2012", "Yerigeri and Ragha, 2019", "Hasan and Kim,\n2019", "Palacios et al., 2019", "Ahuja and Banga, 2019; Saeed et al.,\n2017", "Saeed et al., 2018, 2020; Vanitha and Suresh, 2013; Attallah, 2020", "Asif et al., 2019; Vasavi et al., 2018", "Sardeshpande and Thool, 2019; Masood and\nAlghamdi, 2019", "Uddin and Canavan, 2019", "Garcia and Gustavson, 1997; Northrup, 1997", "Dharmawan, 2007", "Berger, 1929", "Chandra et al., 2017", "Sawangjai et al., 2019", "Troy M et al., 2012", "Technologies, 2012", "Oostenveld and Praamstra, 2001", "Minguillon et al., 2016", "Sanei\nand Chambers, 2013", "Saeed et al., 2015; Hamid et al., 2015", "Hoffmann, 2005", "Gatzke-Kopp et al., 2014; Giannakakis\net al., 2015", "Seo et al., 2008; Lewis et al., 2007", "Qin et al., 2009", "Lopez-Duran et al., 2012", "Peng et al., 2013", "Minguillon et al., 2016", "Acharya et al., 2012", "Giannakakis et al., 2015", "Lopez-Duran et al., 2012; Tomarken et al., 1990", "Peng et al., 2013", "Hosseini et al., 2010; Khosrowabadi\net al., 2011; Sharma and Gedeon, 2014; Ko et al., 2009; Giannakaki et al., 2017", "Hayashi et al., 2009", "Alonso et al., 2015; De-\nmerdzieva, 2011; Al-Shargie et al., 2015; Tran et al., 2007; Seo and Lee, 2010", "Katsis et al., 2011", "Choi et al., 2015", "Alonso et al., 2015", "Huiku et al., 2007", "Marshall et al., 2015", "Choi et al., 2015", "Sharma and Gedeon, 2012", "Secerbegovic et al., 2017", "Hou et al., 2015", "Jun and Smitha, 2016", "Duru et al., 2013", "Calibo\net al., 2013", "Pomer-Escher et al., 2014", "Vijayaragavan et al., 2015", "Reanaree\net al., 2016", "Lin and John,\n2006", "Vanitha\nand Krishnan, 2016", "Kalas and Momin, 2016", "Al-Shargie et al., 2015", "Pandiyan et al., 2013", "Asif et al., 2019", "Halim and Rehan,\n2020", "Saeed et al., 2015", "Saeed et al., 2017", "Saeed et al.,\n2018", "Hamid et al., 2015", "Sulaiman et al., 2011", "Hamid et al.,\n2010", "Luijcks et al., 2015", "Saeed et al., 2020", "Halim and Rehan, 2020", "Dharmawan, 2007", "Secerbegovic et al., 2017", "Hou et al., 2015", "Jun and Smitha, 2016", "Calibo et al., 2013", "Hosseini et al., 2010", "Giannakaki et al., 2017", "Al-Shargie et al., 2015", "Vanitha and Krishnan, 2016", "Asif et al., 2019", "Khosrowabadi et al., 2011", "Saeed et al., 2015", "Saeed et al., 2017", "Saeed et al., 2018", "Saeed et al., 2020", "De Luca, 2002", "Reaz et al., 2006", "Weyers et al., 2006", "Ekman et al., 1978", "Luijcks et al., 2014", "Lundberg et al., 1994; Wijsman et al., 2010; Larsson et al., 1995", "Lund-\nberg et al., 1994", "Wi-\njsman et al., 2010", "Larsson\net al., 1995", "Krantz et al., 2004", "Schleifer et al., 2008", "Engelhardt et al., 2015", "Tyagi et al., 2017", "Farnsworth, 2018", "Wu et al., 2010", "Lidberg\nand Wallin, 1981", "Ayata\net al., 2017", "Critchley, 2002", "Kurniawan et al., 2013", "Dawson\net al., 2007", "Nikula, 1991", "Liao et al., 2005", "Shi et al., 2007", "Healey, 2000", "Giakoumis et al., 2012; Blechert et al., 2006; Ritz et al., 2000;\nReinhardt et al., 2012; Hoehn-Saric et al., 1989", "Setz et al., 2009; Ren et al.,\n2012; Lee et al., 2004; Blechert et al., 2006; Hoehn-Saric et al., 1989; Nomikos et al.,\n1968; Lanzetta et al., 1976", "Nomikos et al., 1968", "Panigrahy\net al., 2017", "Hernandez\net al., 2011", "Healey, 2000", "Blechert et al., 2006", "Setz et al., 2009", "Lee et al., 2004", "Panigrahy et al., 2017", "Hernandez et al., 2011", "Ren et al., 2012", "Kociel-\nnik et al., 2013", "Ahn et al., 2019", "Al Khatib et al., 2007", "Karthikeyan et al., 2012c, 2011", "Karthikeyan et al., 2011", "Charbonnier et al., 2018", "Liu and Ulrich, 2014", "Goldberger et al., 2000", "Tanev et al., 2014", "Bong et al.,\n2012", "Karthikeyan et al.,\n2013", "Karthikeyan et al., 2011", "Charbonnier et al., 2018", "Tanev et al., 2014", "Bong et al., 2012", "Karthikeyan et al., 2013", "Keshan et al., 2015", "Castaldo et al., 2016", "Keshan et al., 2015", "Castaldo et al., 2016", "Giannakakis et al., 2017", "En-\ngert et al., 2014", "Lundberg et al., 1994", "Krantz et al.,\n2004", "Finsen\net al., 2001", "Rein-\nhardt et al., 2012", "Acerbi et al., 2016", "Steptoe et al., 2001", "Schubert et al., 2009", "Clays et al., 2011", "McDuff et al., 2014; Blechert et al., 2006; Hynynen et al.,\n2011; Cinaz et al., 2013; McDuff et al., 2016", "McDuff et al., 2014", "Blechert et al., 2006", "Hynynen et al., 2011", "Mc-\nDuff et al., 2016", "Blechert et al., 2006", "Giannakakis et al., 2017", "McDuff et al., 2014", "McDuff et al., 2016", "McFarland, 1985", "Zhai and Barreto, 2006", "Reisman, 1997", "Yoon et al., 2016", "Lee et al., 2010;\nTorii et al., 1992", "Vinkers et al., 2013", "Herborn et al., 2015", "Marazziti\net al., 1992", "Lee et al., 2004", "Zhai and Barreto, 2006", "Rimm-Kaufman and Kagan, 1996", "Simoes et al., 1991", "Vinkers et al., 2013; McDuff et al., 2014; Gross-\nman, 1983", "Ahmed et al.,\n2015", "Kreibig, 2010", "Stern et al., 2001", "Healey and Picard,\n2005", "Shin et al.,\n1998", "Hosseini and Naghibi-Sistani, 2011", "Seematter et al., 2002", "McDuff et al., 2014", "Ahmed et al., 2015", "Hosseini and Naghibi-Sistani, 2011", "Wijsman et al., 2013", "Rigas et al., 2011", "Wijsman et al., 2011", "Shan et al., 2020", "Shan et al., 2020", "Acharya et al., 2006", "Clifford, 2002", "Sloan et al., 1994", "Karthikeyan et al., 2013", "Taelman et al., 2009", "Lombardo and Vick, 2019", "McDuff et al., 2014", "Oskooei et al., 2019", "Hammoud et al., 2019", "Orsila et al., 2008", "Malik, 1996", "Blechert et al., 2006; Acerbi et al., 2016; Schubert et al., 2009; Clays et al.,\n2011; Hynynen et al., 2011; Cinaz et al., 2013; Bernardi et al., 2000; Taelman et al.,\n2011; Tharion et al., 2009; Visnovcova et al., 2014; Madden and Savard, 1995", "Acerbi et al.,\n2016", "Schubert et al., 2009", "Clays et al., 2011", "Hynynen et al., 2011", "Cinaz et al., 2013", "Bernardi et al., 2000", "Taelman et al., 2011", "Thar-\nion et al., 2009", "Visnovcova et al., 2014", "Madden and Savard, 1995", "Li et al., 2009", "Acerbi et al., 2016; Moriguchi et al., 1992", "Melillo et al., 2011", "Bousefsaf et al., 2013", "Kim et al., 2008", "Boonnithi and Phongsuphap,\n2011", "Melillo et al.,\n2013", "Vanitha and Suresh, 2014", "Munla et al., 2015", "Wang et al., 2013", "Gasperin et al., 2009", "Pickering et al., 1996", "Blechert et al., 2006", "Karthikeyan et al., 2013", "McDuff et al., 2014", "McDuff et al., 2016", "Melillo et al., 2011", "Melillo et al., 2013", "Vanitha and Suresh, 2014", "Munla et al., 2015", "Wang et al., 2013", "Kim et al., 2008", "Schnall et al., 1998", "Lundberg et al., 1994", "Carroll et al., 2003", "Carroll et al., 2011", "Hjortskov et al., 2004", "Krantz et al., 2004", "Vinkers et al., 2013", "Finsen et al., 2001", "Moriguchi\net al., 1992", "Steptoe et al., 2001", "Bernardi et al., 2000", "Hjortskov et al., 2004", "Chal-\nloner, 1979", "Maeda et al., 2011", "Kageyama et al., 2007", "Giannakakis et al., 2019", "Lyu et al., 2015", "Chauhan et al., 2018", "Henelius, 2016", "Poh et al., 2010", "McDuff et al., 2014, 2016", "Charlton et al., 2018", "Mohan et al., 2016", "Li et al., 2018", "Kirschbaum and Hellhammer, 1989", "McDuff et al., 2014", "McDuff et al., 2016", "Chauhan et al., 2018", "Li et al., 2018", "Fink, 2000", "Hellhammer et al., 2009", "Boucher\nand Plusquellec, 2019", "Tu et al., 2019", "Luo et al., 2012", "Nath et al., 2020", "Selvaraj, 2015", "Rey et al., 2014", "Nomura et al., 2009", "Beatty et al., 2000", "Bradley et al., 2008", "Onorati et al., 2013; Partala and Surakka, 2003; Al-\nOmar et al., 2013; Bradley et al., 2008; Ren et al., 2012; Pedrotti et al., 2014", "Honma, 2013; Simpson and Molloy, 1971; Baltaci and Gokcay, 2016; Zhai\nand Barreto, 2006", "Zhai and Bar-\nreto, 2006", "De Berker et al., 2016", "Partala and Surakka, 2003", "Ren et al., 2012", "Zhai and Barreto, 2006", "Pedrotti et al., 2014", "Baltaci and Gokcay, 2016", "Kimble et al., 2010", "Par-\ntala and Surakka, 2003", "Simpson\nand Molloy, 1971", "Wang and Yue, 2011", "Liao et al., 2005", "Torres-Salomao et al., 2015", "Winn et al., 1994", "Winn et al., 1994", "Ellermeier and Westphal,\n1995", "Partala and Surakka, 2003", "Pedrotti\net al., 2014; Reeves, 1920", "Womack and Hansen, 1999", "Lefter et al., 2015", "Williams and Stevens, 1972; Hansen, 1988; Cairns\nand Hansen, 1994; Junqua, 1996; Protopapas and Lieberman, 1997; Hansen and\nPatil, 2007; Gharavian, 2012; Lu et al., 2012; Kurniawan et al., 2013; Sondhi\net al., 2015; Hansen et al., 2011", "Nwe et al., 2003", "Fernandez and Picard, 2003; Healey and Picard,\n2005; Lefter et al., 2011", "Womak and Hansen, 1996", "Cairns and\nHansen, 1994", "Devillers and Vidrascu, 2006", "Hollien, 2002", "Simantiraki et al., 2016", "He et al., 2009", "Hanson et al., 1993", "Sarikaya and Gowdy, 1998", "Fernandez and Picard, 2003", "Hansen et al., 2011", "Yao et al., 2012", "Kadambe, 2007", "Soury and Devillers, 2013", "Haak et al., 2009", "Kurniawan et al., 2013", "Womack and Hansen, 1999", "Lefter et al., 2015", "Cairns and Hansen, 1994", "Hansen and Patil, 2007", "Lu et al., 2012", "Fernandez and Picard, 2003", "Womak and Hansen, 1996", "Simantiraki et al., 2016", "He et al., 2009", "Sarikaya and Gowdy, 1998", "Soury and Devillers, 2013", "Haak et al., 2009; Giannakakis et al., 2017", "Pavlidis et al., 2000", "Giannakakis et al., 2017", "Laretzaki et al., 2011", "Giannakakis et al., 2017", "Mokhayeri and Akbarzadeh-T, 2011", "Fox et al.,\n2007; Staab, 2014", "Fox et al., 2007", "Staab, 2014", "Mokhayeri and Akbarzadeh-T, 2011", "Aigrain\net al., 2015", "Giakoumis et al., 2012", "Carneiro et al., 2012", "Gunes and Piccardi, 2007", "Liao et al., 2005; Bevilacqua et al., 2018", "Bevilacqua et al., 2018", "Liao et al., 2005", "Dinges et al., 2005", "Aigrain et al., 2015", "Carneiro et al., 2012", "Dinges et al., 2005", "Arnrich et al., 2009", "Adams et al., 2015", "Arnrich\net al., 2009", "Liao et al.,\n2005", "Sun et al., 2014", "Carneiro et al., 2015", "Her-\nnandez et al., 2014", "Sun et al., 2014", "Lim et al.,\n2014", "Carneiro et al., 2015", "Rodrigues et al., 2013", "Lim et al., 2014", "Hernandez et al., 2014", "Gunawardhane et al., 2013", "Kailas et al., 2010", "Carneiro\net al., 2017", "Lee and Jung, 2018", "Dillon et al., 2016", "Colunas et al., 2011", "Colunas\net al., 2011", "Gaggioli et al., 2012", "Lu et al., 2012", "Bauer and\nLukowicz, 2012", "Muaremi et al., 2013", "Chang et al.,\n2011", "Garcia-Ceja et al.,\n2015", "Lee and Chung, 2016", "Gjoreski et al., 2015", "Ciman and\nWac, 2016", "Gjoreski et al., 2017", "Gimpel et al., 2015", "Sysoev et al., 2015", "Lu et al., 2012", "Muaremi et al., 2013", "Garcia-Ceja et al., 2015", "Lee and Chung, 2016", "Gjoreski et al., 2015", "Ciman and Wac, 2016", "Gjoreski et al., 2017", "Sysoev et al., 2015", "Vildjiounaite et al., 2018", "Kostopoulos et al., 2017", "Vildjiounaite et al.,\n2018", "Levine et al., 2001", "Nozawa and Tacano,\n2009", "Ioannou et al., 2014", "Nhan and Chau, 2009", "Nhan and Chau, 2009; Shastri et al., 2008", "Hong and Hong, 2016", "Pavlidis and\nLevine, 2002; Pavlidis et al., 2000", "Shastri et al., 2008", "Engert et al., 2014; Vinkers et al., 2013; Kang et al., 2006", "Pavlidis et al., 2012; Shastri et al., 2012", "Dinges et al., 2005", "Gao\net al., 2014", "Derakhshan et al., 2014", "Nhan and Chau, 2009", "Hong and Hong, 2016", "Sharma et al., 2014", "Aigrain et al.,\n2015", "Mohd et al., 2015", "Gao et al., 2014", "Derakhshan et al., 2014", "Sharma et al., 2014", "Aigrain et al., 2015", "Al-Shargie et al., 2016", "Kyriakou et al., 2019", "Lee et al., 2016", "Chen et al., 2017", "Ghaderi et al., 2015", "Gjoreski et al., 2016", "Maier et al., 2014", "Sano and Picard, 2013", "Hovsepian et al., 2015", "Zubair et al., 2015", "de Santos Sierra\net al., 2010", "Sandulescu et al., 2015", "Martinez et al., 2017", "Mo-\nzos et al., 2017", "Kurniawan et al.,\n2013", "Aigrain\net al., 2016", "Baltaci and Gokcay, 2016", "Huang et al., 2016", "Akhonda et al., 2014", "Shi et al., 2010", "Ahn et al., 2019", "Healey and Picard, 2005", "Muaremi et al., 2014", "Xu et al., 2014", "Mohino-Herranz\net al., 2015", "Sriramprakash et al.,\n2017", "Hosseini and Khalilzadeh, 2010", "Wijsman et al.,\n2013", "Rigas et al., 2011", "Picard et al., 2001", "Wijs-\nman et al., 2011", "Choi, 2011", "Gjoreski et al., 2016", "Betti et al., 2017", "Liew et al.,\n2015", "Nater et al., 2005", "Aimie-Salleh et al.,\n2018", "Vizer et al., 2009", "Liao\net al., 2005", "Giakoumis et al., 2012", "Al-Shargie et al., 2016", "Kyriakou et al., 2019", "Lee et al., 2016", "Chen et al., 2017", "Ghaderi et al., 2015", "Gjoreski et al., 2016", "Sano and Picard, 2013", "Hovsepian et al., 2015", "Zubair et al., 2015", "de Santos Sierra et al., 2010", "Sandulescu et al., 2015", "Mozos et al., 2017", "Kurniawan et al., 2013", "Aigrain et al., 2016", "Baltaci and Gokcay, 2016", "Huang et al., 2016", "Akhonda et al., 2014", "Ahn et al., 2019", "Healey and Picard, 2005", "Muaremi et al., 2014", "Xu et al., 2014", "Sriramprakash et al., 2017", "Hosseini and Khalilzadeh, 2010", "Wijsman et al., 2013", "Rigas et al., 2011", "Wijsman et al., 2011", "Gjoreski et al., 2016", "Betti et al., 2017", "Liew et al., 2015", "Vizer et al., 2009", "Zhai and Barreto, 2008", "Giakoumis et al., 2012", "Gjoreski et al., 2016", "Gottlieb, 2013", "Koelstra et al., 2011", "Song and\nKim, 2017", "FG 2018", "DCII 2008", "ICIAS2012"], "entities_from_reference": ["Acerbi", "Rovini E", "Betti S", "Tirri A", "Sirianni A", "Agrimi J", "Eusebi L", "Cavallo F", "Italian Forum", "Springer", "Acharya UR", "Joseph KP", "Kannathal N", "Lim CM", "Suri JS", "Medical", "Sree SV", "Ang PCA", "Yanti R", "Adaler A", "Balkan E", "Global", "Mahmoud M", "Baltrusaitis T", "Robinson P", "Affective Computing", "Khan HM", "Choi J", "Ahn JW", "Ku Y", "Kim HC", "Ahuja R", "Banga A", "Dubuisson S", "Detyniecki M", "Chetouani M", "Workshops", "Gesture Recognition", "Spodenkiewicz M", "Dubuiss S", "Cohen D", "Aamir Arsalan1", "Malarvili M", "Whitttaker AC", "Stress", "Islam SMF", "Khan AS", "Ahmed F", "Rahman MM", "Bertozzi D", "Poletti F", "Benini L", "Jantsch A", "Bechara M", "Khalifeh H", "Hajjar M", "Nabiev R", "Jonsson S", "Fawzi M", "Tang TB", "Badruddin N", "Kiguchi M", "Hani AFM", "Petersen KL", "Adrenocortical", "Alamudun F", "Khan H", "Ahmed B", "Pervasive Computing", "Golden LH", "Izzo Jr JL", "Forrest A", "Niles CR", "Niswander PR", "Barlow JC", "Alonso J", "Romero S", "Ballester M", "Antonijoan R", "Andr", "Funk P", "Arnrich B", "Setz C", "La Marca R", "Ehlert U", "Arsalan A", "Majid M", "Anwar SM", "Bagci U", "Annual International Conference", "Biology Society", "Butt AR", "Data", "Attallah O", "Ayata D", "Yaslan Y", "Kamas", "Istanbul", "Electronics", "Baglioni C", "Lombardo C", "Bux E", "Hansen S", "Salveta C", "Biello S", "Violani C", "Espie CA", "Baltac S", "Baltaci S", "Gokcay D", "Barreto A", "Zhai J", "Adjouadi M", "Rishe N", "Gao Y", "Bartlett DL", "Lukowicz P", "Beatty J", "Berger H", "Archiv", "Bernardi L", "Valenti C", "Castoldi S", "Passino C", "Spadacini G", "Sleight P", "Lova RM", "Acerbi G", "Santarelli L", "Cabiati M", "Del Ry S", "Bevilacqua F", "Engstr", "Backlund P", "Birjandtalab J", "Cogan D", "Pouyan MB", "Nourani M", "Bitsika V", "Sharpley CF", "Sweeney JA", "Physiology", "Blechert J", "Lajtman M", "Michael T", "Margraf J", "Wilhelm FH", "Lepri B", "Ferron M", "Pianesi F", "Daily", "Murugappan M", "Yaacob S", "Phongsuphap S", "Bordnick PS", "Traylor AC", "Carter BL", "Graap KM", "Boucher P", "Plusquellec P", "Acute", "Maaoui C", "Pruski A", "Remote", "Bradley MM", "Codispoti M", "Cuthbert BN", "Lang PJ", "Miccoli L", "Escrig MA", "Brkhus A", "Engedal K", "Laake K", "Brantley PJ", "Jones GN", "Waggoner CD", "Rappaport NB", "Brewin CR", "Andrews B", "Rose S", "Kirk M", "Bryant RA", "Moulds ML", "Guthrie RM", "Kroisamer JS", "Gerendas B", "Roberts P", "Rezar S", "Eibenberger K", "Sacu S", "Visual Science", "Bullinger M", "Naber D", "Pickar D", "Cohen RM", "Kalin NH", "Pert A", "Bunney Jr", "Cairns DA", "Hansen JH", "Calibo TK", "Blanco JA", "Firebaugh SL", "Ersoy C", "Chalabianloo N", "Ekiz D", "Riva G", "Sitaram R", "Veit R", "Begliomini C", "Birbaumer N", "Biological", "Carneiro D", "Castillo JC", "Novais P", "Neves J", "Pego JM", "Sousa N", "Hybrid Artificial Intelligence Systems", "Augusto JC", "Payne N", "Carroll D", "Hunt K", "Macintyre S", "Blood", "Psychosomatic Medicine", "Phillips AC", "Der G", "Benzeval M", "Castaldo R", "Xu W", "Melillo P", "Pecchia L", "Santamaria L", "James C", "Asheeb A", "Dash S", "Retna N", "Teja KVR", "Issac TG", "Indian Journal", "Chang Kh", "Fisher D", "Canny J", "Hartmann B", "Body Area Networks", "Vila G", "Godin C", "Labyt E", "Sakri O", "Campagne A", "Charlton PH", "Celka P", "Farukh B", "Chowienczyk P", "Alastruey J", "Chauhan U", "Reithinger N", "Mackey JR", "Adjunct", "Chen Ll", "Zhao Y", "Ye Pf", "Zhang J", "Zou Jz", "Chen T", "Yuen P", "Richardson M", "Liu G", "Chen X", "Liu A", "Peng H", "Ward RK", "Cho HM", "Park H", "Dong SY", "Cho Y", "Julier SJ", "Marquardt N", "Texas", "Kim M", "Chun C", "Ciman M", "Wac K", "Cinaz B", "Clays E", "De Bacquer D", "Crasset V", "Kittel F", "De Smet P", "Kornitzer M", "Karasek R", "De Backer", "Clifford GD", "Uni", "Oxford Cohen S", "Kamarck T", "Mermelstein R", "Colunas MF", "Fernandes JMA", "Oliveira IC", "Cunha JPS", "Mobile Computing Conference", "Critchley HD", "Schell A", "Filion D", "De Berker AO", "Tirole M", "Rutledge RB", "Cross GF", "Dolan RJ", "Bestmann S", "Scientific", "De Luca CJ", "Csenki L", "Empirical Text", "Culture", "Dedovic K", "Renwick R", "Mahani NK", "Engert V", "Lupien SJ", "Pruessner JC", "Delmastro F", "Di Martino F", "Dolciotti C", "Acta Informatica Medica", "Dempsey LA", "Nature", "Deng Y", "Wu Z", "Chu CH", "Yang T", "Derakhshan A", "Mikaeili M", "Khalilzadeh MA", "Mohammadian A", "Derogatis LR", "Spencer P", "Pearson Up", "Saddle River", "Forget H", "Fiset D", "Blais C", "Vidrascu L", "Ninth International Conference", "Spoken Language", "Master", "Faculty", "Mathematics", "Delft University", "Netherlands DHondt F", "Lassonde M", "Collignon O", "Dubarry AS", "Robert M", "Rigoulot S", "Honor", "Lepore F", "Sequeira H", "Dickerson SS", "Kemeny ME", "Dillon A", "Kelly M", "Robertson IH", "Robertson DA", "Rider RL", "Dorrian J", "Rogers NL", "Cizman Z", "Goldenstein SK", "Vogler C", "Venkataraman S", "Aviation", "B172B182 Dufey M", "Mayol R", "Universitas", "Duru DG", "Duru AD", "Barkana DE", "Sanli O", "Ozkan M", "Ekman P", "Friesen W", "Hager J", "Elgendi M", "Ellermeier W", "Westphal W", "Engelhardt C", "Malfroy Camine V", "Ingram D", "Farron A", "Pioletti D", "Terrier A", "Merla A", "Grant JA", "Cardone D", "Tusche A", "Singer T", "Anthenien L", "Dayer E", "Bosshard C", "Gaillard R", "Music", "Schweizerische Medizinische", "Politti JC", "Felice CJ", "Biomedical", "Farnsworth B", "Kothgassner OD", "Hetterle T", "Beutl L", "Hlavacs H", "Afraid", "Cyberpsychology", "Anishchenko L", "Fernandez R", "Picard RW", "Speech", "Fink G", "Academic", "Finsen L", "Sgaard K", "Jensen C", "Borg V", "Christensen H", "Muscle", "Rabins P", "Lucas MJ", "Eastham J", "Tolvanen A", "Myllym", "Rantala S", "Korpela R", "Peuhkuri K", "Kolehmainen M", "Puttonen S", "Lappalainen R", "Fox E", "Mathews A", "Calder AJ", "Yiend J", "Anxiety", "Frings C", "Larra MF", "Moeller B", "Sch", "Gaggioli A", "Pioggia G", "Tartarisco G", "Baldus G", "Ferro M", "Cipresso P", "Serino S", "Popleteev A", "Gabrielli S", "Maimone R", "Gao H", "Thiran JP", "Image Processing", "Garcia J", "Gustavson AR", "Osmani V", "Mayora O", "Garrett VD", "Gasperin D", "Netuveli G", "Dias-da Costa JS", "Pattussi MP", "Cadernos", "Jetha MK", "Segalowitz SJ", "Gedam S", "Paul S", "Frounchi J", "Farnam A", "Machine", "Gharavian D", "Giakoumis D", "Drosou A", "Tzovaras D", "Hassapis G", "Giannakaki K", "Giannakakis G", "Farmaki C", "Sakkalis V", "Grigoriadis D", "Tsiknakis M", "Pediaditis M", "Manousos D", "Kazantzaki E", "Simos PG", "Marias K", "Biomedical Signal", "Chaniotakis V", "Health Informatics", "Simos P", "Simantiraki O", "Roniotis A", "Review", "Affective Computing Gillani SF", "Saeed SMU", "Shabbir Z", "Habib F", "Regal C", "Schmidt M", "Gjoreski M", "Gjoreski H", "Lutrek M", "Gams M", "Lustrek M", "Nature Reviews", "Goetsch VL", "Wiebe DJ", "Veltum LG", "Van Dorsten", "Goldberger AL", "Glass L", "Hausdorff JM", "Ivanov PC", "Mark RG", "Mietus JE", "Moody GB", "Peng CK", "Stanley HE", "Goreczny AJ", "Buss RR", "Waters WF", "Gottlieb BH", "Springer Science", "Nahar NK", "Hayes JR", "Sheedy JE", "Optometry", "Vision Science", "Greene J", "Smith R", "Gardiner M", "Timbury G", "Gross C", "Seeba K", "Scheibe S", "Behavior Research Methods", "Gunawardhane SD", "De Silva PM", "Kulathunga DS", "Arunatileka SM", "Gunes H", "Piccardi M", "Network", "Gurung RA", "Haak M", "Bos S", "Panic S", "Rothkrantz L", "Steiner TJ", "Grant EC", "Clifford Rose", "Language", "Hajcak G", "Dennis TA", "Halim Z", "Rehan M", "Hall M", "Vasko R", "Buysse D", "Ombao H", "Chen Q", "Cashmere JD", "Kupfer D", "Thayer JF", "Hamid NHA", "Sulaiman N", "Aris SAM", "Murat ZH", "Taib MN", "Karam R", "Mourad R", "Kurdi M", "Patil S", "Kim W", "Rahurkar M", "Ruzanski E", "Meyerhoff J", "Robust", "Hanson HM", "Maragos P", "Potamianos A", "Bryant R", "Clinical Psychology", "Harvey AG", "Hasan MJ", "Kim JM", "Hassellund SS", "Flaa A", "Sandvik L", "Kjeldsen SE", "Rostrup M", "Hayashi T", "Okamoto E", "Nishimura H", "Ishii R", "Ukai S", "Image", "Lech M", "Maddage N", "Allen N", "Healey JA", "Massachusetts Institute", "Kudielka BM", "Hemmeter U", "St", "Mager R", "Kuntze M", "Hennig J", "Amditis A", "Bekiaris A", "Bullinger A", "Henelius A", "Herborn KA", "Graves JL", "Jerem P", "Evans NP", "Nager R", "Skin", "Hernandez J", "Morris RR", "Paredes P", "Roseway A", "Czerwinski M", "Hines EA", "Clin Proc", "Stress Management", "Booktango Hjortskov N", "Blangsted AK", "Fallentin N", "Lundberg U", "Zimmerli WD", "Hoffmann E", "Stress Report", "Hollien HF", "Academic Press Hong K", "Honma M", "Applied Biomedical", "Niazmand V", "Hou X", "Liu Y", "Sourina O", "Tan YRE", "Wang L", "Man", "Ertin E", "Nakajima M", "Kumar S", "Huang MX", "Li J", "Ngai G", "Leong HV", "Uutela K", "Van Gils M", "Kym", "Paloheimo M", "Rantanen M", "Takala P", "Vierti", "Hynynen E", "Konttinen N", "Kinnunen U", "Kyr", "Rusko H", "Ioannou S", "Gallese V", "Jobb", "Majn", "Nagy P", "Periodica Polytechnica Electrical Engineering", "Jun G", "Smitha KG", "Kadambe S", "Study", "Odagaki M", "Hosaka H", "Chong CC", "Watanabe F", "Kajantie E", "Phillips DI", "Kalas MS", "Momin B", "Kang J", "Karthikeyan P", "Katsis", "Katertsidis NS", "Fotiadis DI", "Kaufer DI", "Cummings JL", "Christine D", "Bray T", "Castellon S", "Masterman D", "Ketchel P", "Kelly MM", "Tyrka AR", "Anderson GM", "Carpenter LL", "Kemp AH", "Quintana DS", "Felmingham KL", "Matthews S", "Jelinek HF", "Keshan N", "Parimi P", "Khalfa S", "Bella SD", "Roy M", "Peretz", "Homam SM", "Khosrowabadi R", "Quek C", "Ang KK", "Tung SW", "Heijnen M", "Kim D", "Seo Y", "Cho J", "Cho CH", "Kimble MO", "Fleming K", "Bandy C", "Kim J", "Zambetti A", "Kirschbaum C", "Pirke KM", "Knight WE", "Rickard NS", "Yang HC", "Sim KB", "Kocielnik R", "Sidorova N", "Maggi FM", "Ouwerkerk M", "Westerink JH", "Muhl C", "Soleymani M", "Lee JS", "Yazdani A", "Ebrahimi T", "Pun T", "Nijholt A", "Patras", "Koldijk S", "Sappelli M", "Verberne S", "Neerincx MA", "Kraaij W", "Kyritsis AI", "Deriaz M", "Konstantas D", "Felnhofer A", "Palme R", "Glenk LM", "Krantz G", "Forsman M", "Kreibig SD", "Biolog", "Kurniawan H", "Maslov AV", "Pechenizkiy M", "Resch B", "Sagl G", "Petutschnig A", "Werner C", "Niederseer D", "Osborne T", "Pykett J", "Lackner HK", "Papousek", "Batzel JJ", "Roessler A", "Scharfetter H", "Eleck RE", "Laretzaki G", "Plainis S", "Vrettos", "Chrisoulakis A", "Bitsios P", "Larsson SE", "Larsson R", "Zhang Q", "Cai H", "Oberg P", "Lasaitis C", "Ribeiro RL", "Bueno OFA", "Jornal Brasileiro", "Lee BG", "Chung WY", "Lee DS", "Chong TW", "Lee Mh", "Yang G", "Lee HK", "Bang S", "Lee RA", "Jung ME", "Lee Y", "Lee B", "Lee M", "Telemedicine", "Rothkrantz LJ", "Van Leeuwen DA", "Wiggers P", "Burghouts GJ", "Levine JA", "Cooper M", "Lewis RS", "Weekes NY", "Wang TH", "Li F", "Xu P", "Zheng S", "Chen W", "Lu S", "Liu Z", "Li Z", "Snieder H", "Su S", "Ding X", "Treiber FA", "Wang X", "Liao W", "Zhang W", "Zhu Z", "Ji Q", "Pattern Recognition", "Katsanos C", "Sotiropoulos D", "Xenos M", "Karousos N", "Lidberg L", "Wallin BG", "Lien R", "Neijts M", "Willemsen G", "Geus EJ", "Liew WS", "Seera M", "Loo CK", "Lim E", "Kubota N", "Lim YM", "Ayesh A", "Stacey M", "Wu J", "Chan SC", "John L", "Linden W", "Liu D", "Ulrich M", "Lohani M", "Gupta R", "Srinivasan N", "Lombardo DM", "Vick RS", "Nusslock R", "George C", "Kovacs M", "Lovallo W", "Frauendorfer D", "Rabbi M", "Mast MS", "Campbell AT", "Choudhury T", "Lucini D", "Clerici M", "Pagani M", "Luijcks R", "Hermens HJ", "Bodar L", "Vossen CJ", "Lousberg R", "Experi", "Kadefors R", "Melin B", "Palmerud G", "Hassm", "Dohns IE", "Luo L", "Xiao L", "Miao D", "Luo X", "Zhou J", "Yu C", "Miao C", "Wang T", "Shi Y", "Kameyama Ki", "Annual ACM Conference", "Systems", "Madden K", "Savard G", "Clinical Physiology", "Maeda Y", "Sekine M", "Tamura T", "Maier E", "Reimer U", "Laurenzi E", "Ridinger M", "Ulmer T", "Malik M", "Marazziti D", "Di Muro A", "Castrogiovanni P", "Markova V", "Ganchev T", "Kalinkov K", "Cooper NR", "Segrave R", "Geeraert N", "Martinez R", "Irigoyen E", "Arruti A", "Mart", "Muguerza J", "Maslach C", "Jackson S", "Leiter M", "Zalaquett C", "Wood R", "Scarecrow Masood K", "Alghamdi MA", "Gontarek S", "Picard R", "Bracale M", "Formisano C", "Bracale U", "Benezeth Y", "De Oliveira P", "Chappe J", "Yang F", "Affective Computing Minguillon J", "Pelayo F", "Mohan PM", "Nagarajan V", "Das SR", "Kashima M", "Sato K", "Watanabe M", "Affective", "Ferreira J", "Seoane F", "Mokhayeri F", "Otsuka A", "Kohara K", "Mikami H", "Katahira K", "Tsunetoshi T", "Higashimori K", "Ohishi M", "Yo Y", "Ogihara T", "Clinical Autonomic", "Mosley Jr TH", "Penzien DB", "Johnson CA", "Wittrock DA", "Andrew ME", "Payne TJ", "Mozos OM", "Sandulescu V", "Andrews S", "Ellis D", "Bellotto N", "Dobrescu R", "Ferrandez JM", "Muaremi A", "Bexheti A", "Gravenhorst F", "Khalil M", "Shahin A", "Mourad A", "Murray IR", "Baber C", "Muthukumar K", "Nachiappan V", "Nater UM", "Rohleder N", "Gaab J", "Berger S", "Jud A", "Nath RK", "Thapliyal H", "Nhan BR", "Chau T", "Nikula R", "Nomikos MS", "Opton Jr E", "Averill JR", "Nomura S", "Mizuno T", "Nozawa A", "Asano H", "Ide H", "Kansei Engineering", "Institute", "York University Nozawa A", "Tacano M", "P01007 Nwe TL", "Foo SW", "De Silva LC", "Onorati F", "Barbieri R", "Mauri M", "Russo V", "Mainardi L", "Praamstra P", "Clinical", "Orsila R", "Virtanen M", "Luukkaala T", "Tarvainen M", "Karjalainen P", "Viik J", "Savinainen M", "Nyg", "Oskooei A", "Chau SM", "Weiss J", "Sridhar A", "Michel B", "Palacios D", "Rodellar V", "Tompkins WJ", "Pandiyan PM", "Panigrahy SK", "Jena SK", "Turuk AK", "Research Journal", "Parsons TD", "Rizzo AA", "Partala T", "Surakka V", "Pupil", "Levine J", "Biology Magazine", "Baukol P", "Tsiamyrtzis P", "Shastri D", "Wesley A", "Zhou Y", "Lindner P", "Buddharaju P", "Joseph R", "Mandapati A", "Dunkin B", "Mirzaei MA", "Tedesco A", "Chardonnet JR", "Benedetto S", "Baccino T", "Pehlivano", "Durmazlar N", "Balkanc D", "Erciyes Medical", "Hu B", "Zheng F", "Fan D", "Zhao W", "Yang Y", "Cai Q", "Pertaub DP", "Slater M", "Barker C", "Virtual", "Pflanzer R", "Vyzas E", "Healey J", "Toward", "Devereux RB", "James GD", "Gerin W", "Landsbergis P", "Schnall PL", "Schwartz JE", "S17985 Mental Stress", "Arch JJ", "Lam CW", "Craske MG", "Pluntke U", "Gerke S", "Optics", "Souza MDP", "Bastos Filho TF", "Better", "Safer Living", "Soler JM", "Leclercq V", "Denys P", "Lieberman P", "Pujol J", "Vendrell P", "Deus J", "Junqu", "Bello J", "Capdevila A", "Puri C", "Olson L", "Starren J", "Hermans EJ", "Marle HJ", "Luo J", "Racine N", "Khu M", "Reynolds K", "Guilcher G", "Schulte F", "Current Oncology", "Rahman T", "Ghosh AK", "Shuvo M", "Ramalho R", "Orsolini L", "Adiukwu F", "Larnaout A", "Costa MP", "Grandinetti P", "Shalbafan M", "Brain", "Rastgoo MN", "Nakisa B", "Rakotonirainy A", "Chandran V", "Tjondronegoro D", "Reanaree P", "Tananchana P", "Narongwongwathana W", "Pintavirooj C", "Reaz MBI", "Hussain MS", "Mohd-Yasin F", "Reinhardt T", "Schmahl C", "Bohus M", "Reisman S", "Ren P", "Renaud P", "Blondin JP", "Rey JMG", "Arza A", "Aguilo J", "Kagan J", "Burns VE", "Drayson M", "Walkey DG", "Dale S", "Riss", "Sandsj", "Dohns", "Ritz T", "Steptoe A", "Costa M", "Rodrigues M", "Gonc", "Keystrokes", "Bhatti AM", "Khalid H", "Saeed U", "Muhammad S", "Awais M", "Alnowami M", "Jeong MG", "Lim SK", "Won K", "Woo JM", "Sanei S", "John Wiley", "Sons Sano A", "Humaine Association Conference", "Santos Sierra A", "Avila CS", "Casanova JG", "Pozo GB", "Vera VJ", "Sardeshpande K", "Thool VR", "Sarikaya R", "Gowdy JN", "Sawangjai P", "Hompoonsup S", "Leelaarporn P", "Kongwudhikunakorn S", "Wilaiprasitporn T", "Schleifer LM", "Kerick SE", "Cram JR", "Ley R", "Hatfield BD", "Schmidt P", "Reiss A", "Duerichen R", "Marberger C", "Van Laerhoven", "Landsbergis PA", "Warren K", "Schneegass S", "Broy N", "Heinrich F", "Schmidt A", "Schubert C", "Lambertz M", "Nelesen R", "Bardwell W", "Choi JB", "Dimsdale J", "Johnson JG", "Psychology Press Schulz P", "Schlotz W", "Ibric S", "Nisic J", "Suljanovic N", "Mujcic A", "Dirlewanger M", "Rey V", "Schneiter P", "Tappy L", "Miller GE", "Selvaraj N", "Gil Y", "Lee J", "Hybrid Information Technology", "Lee JT", "Seraganian P", "Szabo A", "Brown TG", "Schumm J", "Shan Y", "Li S", "Machine Learning", "Sharma N", "Gedeon T", "Applied Soft", "Dhall A", "Goecke R", "Video", "Papadakis M", "Bass B", "Ruiz N", "Taib R", "Choi E", "Chen F", "Nguyen MH", "Blitz P", "Fisk S", "Torre F", "Smailagic A", "Siewiorek DP", "Seongo H", "Cha D", "Yoon Y", "Yoon H", "Vol", "Shon D", "Im K", "Park JH", "Lim DS", "Jang B", "Pampouchidou A", "Simoes E", "Roark R", "Berman S", "Esler L", "Murphy J", "Simpson H", "Molloy F", "Psychophys", "Sincero SM", "Singh M", "Queyam AB", "Clark DM", "Sloan R", "Shapiro P", "Bagiella E", "Boni S", "Paik M", "Bigger Jr J", "Steinman R", "Gorman J", "Sondhi S", "Khan M", "Vijay R", "Salhan AK", "Song SH", "Kim DK", "Soury M", "Pratapa SK", "Mahant S", "Prasanna VD", "Murthy OR", "Stavropoulos V", "Gomez R", "Steen E", "Beard C", "Liew L", "Griffiths MD", "Steeneken HJ", "Owen N", "Flower L", "Stern RM", "Ray WJ", "Quigley KS", "Oxford University Press", "Stroop", "Stroud LR", "Salovey P", "Epel ES", "Stula T", "Sturmbauer SC", "Shields GS", "Hetzel EL", "Slavich GM", "Styliadis C", "Ioannides AA", "Bamidis PD", "Papadelis C", "Xia L", "Malik AS", "Advanced Systems", "Subhani AR", "Mumtaz W", "Kamil N", "Saad NM", "Nandagopal N", "Saad MNBM", "Kamel N", "Morimoto K", "Obata A", "Koizumi H", "Maki A", "Lias S", "Mustafa M", "Rashid NA", "Networks", "Huggenberger HJ", "Svetlak M", "Bob P", "Cernik M", "Kukleta M", "Autonomic Neuroscience", "Sysoev M", "Kos A", "Pogacnik M", "Taamneh S", "Dcosta M", "Khatri A", "Manser M", "Ferris T", "Wunderlich R", "Taelman J", "Vandeput S", "Spaepen A", "Van Huffel S", "Vlemincx E", "Tanev G", "Saadi DB", "Hoppe K", "Sorensen HB", "Klein LC", "Lewis BP", "Gruenewald TL", "Updegraff JA", "Khalus V", "Labrado C", "Natl Med", "Thommessen B", "Braekhus A", "Oksengaard AR", "Tomaka J", "Blascovich J", "Swart L", "Tomarken AJ", "Davidson RJ", "Henriques JB", "Torii M", "Yamasaki M", "Sasaki T", "Nakayama H", "Mahfouf M", "Thuraisingham R", "Wijesuriya N", "Nguyen H", "Craig A", "Joseph T G", "Daniel P", "Brain Science", "Begdache L", "Won D", "Koh A", "Tugade MM", "Fredrickson BL", "Tulen J", "Moleman P", "Van Steenis H", "Boomsma F", "Pharmacology Biochemistry", "Tyagi P", "Arora AS", "Rastogi V", "Uddin MT", "Canavan S", "Russo M", "Sikora M", "Health Care", "Herman JP", "Ulstein", "Bruun Wyller T", "Wyller TB", "Ushiyama K", "Ogawa T", "Ishii M", "Ajisaka R", "Sugishita Y", "Vanitha L", "Suresh G", "Hybrid", "Advanced Computing", "Vanitha V", "Krishnan P", "Vasavi S", "Neeharica P", "Wadhwa B", "Annual Information Technology", "Mobile Communication Conference", "Raghav R", "Phani K", "Vaidyanathan V", "Eeg", "Green Computing", "Kallio J", "Kyll", "Nieminen M", "Lindholm M", "Gimelfarb G", "Vinkers CH", "Penning R", "Verster JC", "Klaessens JH", "Olivier B", "Kalkman CJ", "Visnovcova Z", "Mestanik M", "Javorka M", "Mokra D", "Gala M", "Jurko A", "Calkovska A", "Vizer LM", "Zhou L", "Sears A", "Vuksanovi", "Gal V", "Waggoner C", "Louisiana State University", "Baton Rouge", "Louisiana Wang JS", "Lin CW", "Yang YTC", "Shi Z", "Zhang Y", "Shen J", "Wang Ls", "Yue X", "V1347 Weidner G", "Friend R", "Ficarrotto TJ", "Mendell NR", "Psychosomatic Medicine Werner EE", "Weyers P", "Hefele C", "Pauli P", "Wielgosz J", "Schuyler BS", "Lutz A", "Wijsman J", "Grundlehner B", "Hermens H", "Trapezius", "Wireless Health", "Liu H", "Williams CE", "Stevens KN", "Willigenburg NW", "Daffertshofer A", "Kingma", "Van Die", "Winn B", "Whitaker D", "Elliott DB", "Phillips NJ", "Wolpe J", "Audio", "Womak B", "Hao M", "Nwe TL", "Guan C", "Yao X", "Jitsuhiro T", "Miyajima C", "Kitaoka N", "Takeda K", "Ragha L", "Speech Technology", "Sim JK", "Cho YH", "Pastor JM", "Zubair M", "Yoon C", "Kim H", "Smart"]}{"title": ["Causal Inference Using Tractable Circuits"], "authors": ["[arxiv.Result.Author('Adnan Darwiche')]"], "link": ["http://arxiv.org/pdf/2202.02891v1"], "summary": "The aim of this paper is to discuss a recent result which shows that\nprobabilistic inference in the presence of (unknown) causal mechanisms can be\ntractable for models that have traditionally been viewed as intractable. This\nresult was reported recently to facilitate model-based supervised learning but\nit can be interpreted in a causality context as follows. One can compile a\nnon-parametric causal graph into an arithmetic circuit that supports inference\nin time linear in the circuit size. The circuit is also non-parametric so it\ncan be used to estimate parameters from data and to further reason (in linear\ntime) about the causal graph parametrized by these estimates. Moreover, the\ncircuit size can sometimes be bounded even when the treewidth of the causal\ngraph is not, leading to tractable inference on models that have been deemed\nintractable previously. This has been enabled by a new technique that can\nexploit causal mechanisms computationally but without needing to know their\nidentities (the classical setup in causal inference). Our goal is to provide a\ncausality-oriented exposure to these new results and to speculate on how they\nmay potentially contribute to more scalable and versatile causal inference.", "entities_include_in_text": ["NeurIPS 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Deep Convolutional Learning-Aided Detector for Generalized Frequency Division Multiplexing with Index Modulation"], "authors": ["[arxiv.Result.Author('Merve Turhan'), arxiv.Result.Author('Ersin \u00d6zt\u00fcrk'), arxiv.Result.Author('Hakan Ali \u00c7\u0131rpan')]"], "link": ["http://arxiv.org/pdf/2202.02876v1"], "summary": "In this paper, a deep convolutional neural network-based symbol detection and\ndemodulation is proposed for generalized frequency division multiplexing with\nindex modulation (GFDM-IM) scheme in order to improve the error performance of\nthe system. The proposed method first pre-processes the received signal by\nusing a zero-forcing (ZF) detector and then uses a neural network consisting of\na convolutional neural network (CNN) followed by a fully-connected neural\nnetwork (FCNN). The FCNN part uses only two fully-connected layers, which can\nbe adapted to yield a trade-off between complexity and bit error rate (BER)\nperformance. This two-stage approach prevents the getting stuck of neural\nnetwork in a saddle point and enables IM blocks processing independently. It\nhas been demonstrated that the proposed deep convolutional neural network-based\ndetection and demodulation scheme provides better BER performance compared to\nZF detector with a reasonable complexity increase. We conclude that\nnon-orthogonal waveforms combined with IM schemes with the help of deep\nlearning is a promising physical layer (PHY) scheme for future wireless\nnetworks", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Evaluation Methods and Measures for Causal Learning Algorithms"], "authors": ["[arxiv.Result.Author('Lu Cheng'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Raha Moraffah'), arxiv.Result.Author('Paras Sheth'), arxiv.Result.Author('K. Selcuk Candan'), arxiv.Result.Author('Huan Liu')]"], "link": ["http://arxiv.org/pdf/2202.02896v1"], "summary": "The convenient access to copious multi-faceted data has encouraged machine\nlearning researchers to reconsider correlation-based learning and embrace the\nopportunity of causality-based learning, i.e., causal machine learning (causal\nlearning). Recent years have therefore witnessed great effort in developing\ncausal learning algorithms aiming to help AI achieve human-level intelligence.\nDue to the lack-of ground-truth data, one of the biggest challenges in current\ncausal learning research is algorithm evaluations. This largely impedes the\ncross-pollination of AI and causal inference, and hinders the two fields to\nbenefit from the advances of the other. To bridge from conventional causal\ninference (i.e., based on statistical methods) to causal learning with big data\n(i.e., the intersection of causal inference and machine learning), in this\nsurvey, we review commonly-used datasets, evaluation methods, and measures for\ncausal learning using an evaluation pipeline similar to conventional machine\nlearning. We focus on the two fundamental causal-inference tasks and\ncausality-aware machine learning tasks. Limitations of current evaluation\nprocedures are also discussed. We then examine popular causal inference\ntools/packages and conclude with primary challenges and opportunities for\nbenchmarking causal learning algorithms in the era of big data. The survey\nseeks to bring to the forefront the urgency of developing publicly available\nbenchmarks and consensus-building standards for causal learning evaluation with\nobservational data. In doing so, we hope to broaden the discussions and\nfacilitate collaboration to advance the innovation and application of causal\nlearning.", "entities_include_in_text": ["NOAA 2008"], "entities_from_reference": ["Plusieurs"]}{"title": ["Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity"], "authors": ["[arxiv.Result.Author('Lin Guan'), arxiv.Result.Author('Sarath Sreedharan'), arxiv.Result.Author('Subbarao Kambhampati')]"], "link": ["http://arxiv.org/pdf/2202.02886v1"], "summary": "Creating reinforcement learning (RL) agents that are capable of accepting and\nleveraging task-specific knowledge from humans has been long identified as a\npossible strategy for developing scalable approaches for solving long-horizon\nproblems. While previous works have looked at the possibility of using symbolic\nmodels along with RL approaches, they tend to assume that the high-level action\nmodels are executable at low level and the fluents can exclusively characterize\nall desirable MDP states. This need not be true and this assumption overlooks\none of the central technical challenges of incorporating symbolic task\nknowledge, namely, that these symbolic models are going to be an incomplete\nrepresentation of the underlying task. To this end, we introduce Symbolic-Model\nGuided Reinforcement Learning, wherein we will formalize the relationship\nbetween the symbolic model and the underlying MDP that will allow us to capture\nthe incompleteness of the symbolic model. We will use these models to extract\nhigh-level landmarks that will be used to decompose the task, and at the low\nlevel, we learn a set of diverse policies for each possible task sub-goal\nidentified by the landmark. We evaluate our system by testing on three\ndifferent benchmark domains and we show how even with incomplete symbolic model\ninformation, our approach is able to discover the task structure and\nefficiently guide the RL agent towards the goal.", "entities_include_in_text": ["Icarte et al., 2018", "Lyu et al., 2019;\nIllanes et al., 2020", "Illanes et al., 2020; Lyu et al., 2019", "Zhang\net al., 2018; Kambhampati et al., 2021", "Yang et al., 2018; Illanes et al., 2020; Lyu\net al., 2019; Kokel et al., 2021", "Basu et al., 2018; Guan et al.,\n2021", "Andreas et al., 2017", "Goyal et al., 2019", "Haarnoja et al., 2017; Kumar et al., 2020", "Florensa et al., 2017; Achiam et al., 2018; Eysenbach\net al., 2019; Lee et al., 2019", "Kambhampati et al., 1996", "Zhang et al., 2019", "Miller, 2019; Chakraborti et al.,\n2019", "Sreedharan\net al., 2020; Zhang et al., 2018; Lyu et al., 2019", "Srivastava et al., 2016; Marthi et al., 2007", "Illanes et al., 2020", "Kulkarni et al., 2016", "Sutton et al., 1999", "Florensa et al., 2017; Eysenbach et al., 2019;\nAchiam et al., 2018", "Florensa et al., 2017", "Illanes et al., 2020", "Yang et al., 2018", "Andreas\net al., 2017", "Li et al., 2006", "Kokel\net al., 2021)", "Florensa et al., 2017;\nAchiam et al., 2018; Kumar et al., 2020; Eysenbach et al.,\n2019", "Yang et al., 2018", "Schaul et al., 2015"], "entities_from_reference": ["Edwards", "Andreas", "Klein", "Machine Learning", "Basu", "Chakraborti", "Eysenbach", "Gupta", "Ibarz", "Nilsson", "Artif", "Florensa", "Duan", "Geffner", "Bonet", "Synthesis", "Goyal", "Mooney", "Grzes", "Kudenko", "Guan", "Guo", "Zhang", "Haarnoja", "Tang", "Milli", "Bengio", "Wallach", "Garnett", "Icarte", "Klassen", "Valenzano", "Machine Learning Research", "Illanes", "Yan", "Symbolic", "Nancy", "Kambhampati", "Ihrig", "Zha", "Keyder", "Richter", "Helmert", "Wooldridge", "Kokel", "Natarajan", "Tadepalli", "Kulkarni", "Narasimhan", "J. Hierarchical", "Kumar", "Skill Diversity Lee", "Salakhutdinov", "Lerer", "Szlam", "Ballard", "Stone", "Walsh", "Littman", "Symposium", "Lyu", "Yang", "Liu", "Gustafson", "Marthi", "Wolfe", "Fox", "Rhode Island", "Miller", "Schaul", "Quan", "Antonoglou", "Sreedharan", "Srivastava", "Wellman", "Phoenix", "Sutton", "Singh", "Tversky", "Kahneman", "Skill Diversity", "Proof Sketch", "Input", "Initialize Q-values", "Select", "Terminate", "Hence", "Achiam", "Due", "Smeta", "Smeta S", "Mario", "Again", "Similar", "Implementation Details", "Skill Diversity J", "Symbolic Models", "Domain Model", "Problem Model"]}{"title": ["An Empirical Analysis of AI Contributions to Sustainable Cities (SDG11)"], "authors": ["[arxiv.Result.Author('Shivam Gupta'), arxiv.Result.Author('Auriol Degbelo')]"], "link": ["http://arxiv.org/pdf/2202.02879v1"], "summary": "Artificial Intelligence (AI) presents opportunities to develop tools and\ntechniques for addressing some of the major global challenges and deliver\nsolutions with significant social and economic impacts. The application of AI\nhas far-reaching implications for the 17 Sustainable Development Goals (SDGs)\nin general, and sustainable urban development in particular. However, existing\nattempts to understand and use the opportunities offered by AI for SDG 11 have\nbeen explored sparsely, and the shortage of empirical evidence about the\npractical application of AI remains. In this chapter, we analyze the\ncontribution of AI to support the progress of SDG 11 (Sustainable Cities and\nCommunities). We address the knowledge gap by empirically analyzing the AI\nsystems (N = 29) from the AIxSDG database and the Community Research and\nDevelopment Information Service (CORDIS) database. Our analysis revealed that\nAI systems have indeed contributed to advancing sustainable cities in several\nways (e.g., waste management, air quality monitoring, disaster response\nmanagement, transportation management), but many projects are still working for\ncitizens and not with them. This snapshot of AI's impact on SDG11 is inherently\npartial, yet useful to advance our understanding as we move towards more mature\nsystems and research on the impact of AI systems for social good.", "entities_include_in_text": ["Gupta et al., 2021", "Degbelo et al., \n\n2021;  Chipofya  et  al.,  2020", "Gupta  et  al.,  2018", "Barns, 2019", "Gupta et  al., 2018", "Allam  and Jones, 2020", "Zheng et al., 2020). Furthermore, as Israilidis et al. (2021", "Batty, 2009", "Axinte et al., 2019", "Solecki et al., 2018", "Ismagilova  et  al.,  2019", "Sharda  et  al.,  2021;  Rogers  et  al.,  2020", "Ismagilova et al., 2019", "Barlacchi et al., 2015; Bibri, \n\n2021", "Aust, 2019", "Acuto, 2016", "Allam and Dhunny, 2019", "Firouzi  et  al.,  2021;  Dinh  and \n\nThai,  2018;  Rajan  and  Saffiotti,  2017;  Tajunisa  et  al.,  2021;  Dai,  2019", "Rabah, 2018", "Sougkakis et al., 2020; \n\nVillagra et al., 2020; Majumdar et al., 2021", "Kuffer  et  al.,  2020,  2021", "Hilbert,  2016;  Gupta  et  al.,  2021", "Nilsson  et  al.,  2016", "Nilsson  et  al.,  2016;  Vinuesa  et  al.,  2020", "Allam and Dhunny, 2019", "Reddick et al., 2020; Chase, 2020", "Galaz  et  al.,  2021", "Taddeo  et  al.,  2021;  van  Wynsberghe,  2021", "van \n\nWynsberghe, 2021", "United  Nations,  2015", "Croese  et  al.,  2020", "Yigitcanlar et al., 2019", "Martens,  2019", "Li et al., 2018", "Fritz et al., \n\n2019). Social and cultural information dictates the context in which the AI is \n\nimplemented. Citizen participation provides the public with the opportunity \n\nto  support  policy  development,  leading  to  trust-building,  credibility,  and \n\nultimately  inclusiveness  in  taking  actions  towards  SDGs.  SDGs  require \n\nactions  that  can  transform  existing  practices  across  sectors.  Fraisl  et  al. \n\n(2020", "Kirwan and Zhiyong, 2020", "Munsaka  and  Dube, \n\n2018", "Antweiler,  2019", "Makondo and Thomas, 2018; Magni, \n\n2017", "Micheletti et al., 2014", "Gupta et al., \n\n2018", "Ferri et al., 2020", "Newman  et  al.,  2020", "Saner et al., 2020", "Vinuesa  et  al.,  2020", "Pekmezovic, 2019", "Walker  et  al., \n\n2019", "Guan et al., 2019; Rubio-Mozos et al., 2019; Thinyane, 2018", "Samoili et al., 2020", "Samoili et al., 2020", "Hutson, \n\n2017", "Degbelo  et  al.,  2016", "Salehi  et  al.,  2016)", "e.g., Zook \n\net al. (2010)", "Henderson  et  al., \n\n2020", "KDD  2016"], "entities_from_reference": ["Plusieurs"]}{"title": ["Trusted Approximate Policy Iteration with Bisimulation Metrics"], "authors": ["[arxiv.Result.Author('Mete Kemertas'), arxiv.Result.Author('Allan Jepson')]"], "link": ["http://arxiv.org/pdf/2202.02881v1"], "summary": "Bisimulation metrics define a distance measure between states of a Markov\ndecision process (MDP) based on a comparison of reward sequences. Due to this\nproperty they provide theoretical guarantees in value function approximation.\nIn this work we first prove that bisimulation metrics can be defined via any\n$p$-Wasserstein metric for $p\\geq 1$. Then we describe an approximate policy\niteration (API) procedure that uses $\\epsilon$-aggregation with\n$\\pi$-bisimulation and prove performance bounds for continuous state spaces. We\nbound the difference between $\\pi$-bisimulation metrics in terms of the change\nin the policies themselves. Based on these theoretical results, we design an\nAPI($\\alpha$) procedure that employs conservative policy updates and enjoys\nbetter performance bounds than the naive API approach. In addition, we propose\na novel trust region approach which circumvents the requirement to explicitly\nsolve a constrained optimization problem. Finally, we provide experimental\nevidence of improved stability compared to non-conservative alternatives in\nsimulated continuous control.", "entities_include_in_text": ["Li et al., 2006", "Lan et al.,\n2021", "Singh et al., 1995; Bertsekas, 2019", "Ferns et al., 2004; 2011", "Castro, 2020", "Zhang et al., 2021).\nZhang et al. (2021", "Zhang et al., 2021", "Ferns et al., 2011)", "Ferns et al., 2011). A dis-\ncrete analogue of this metric was also outlined previously\nby Ferns et al. (2004), which is omitted here for brevity.\nFerns et al. (2011", "Ferns et al., 2004; 2011)", "Ferns et al.,\n2011", "Ferns et al., 2011", "Castro, 2020)", "Haarnoja et al., 2018", "see Bertsekas (2011)", "Schulman\net al., 2015", "Zhang et al., 2021", "Zhang et al.,\n2021", "Munos, 2003; Farahmand et al., 2010). Indeed, as\nnoted by Bertsekas (2011", "Schul-\nman et al., 2015", "Wu et al., 2017", "Schul-\nman et al., 2017)", "Castro,\n2020; Zhang et al., 2021", "Haarnoja et al., 2018", "Ziebart et al.,\n2008", "Nachum et al., 2017;\nHaarnoja et al., 2018", "Nachum et al., 2017", "Haarnoja et al., 2018", "Nachum et al., 2018", "Nachum et al., 2018", "Pinsker, 1964; Kullback, 1967", "see Duchi (2007", "Haarnoja et al., 2018", "Zhang et al., 2021", "Villani, 2008)", "Villani, 2008", "Ferns et al., 2011", "Villani, 2008", "Bertsekas, 2018)", "Bertsekas, 2018)", "Bertsekas, 2018"], "entities_from_reference": ["Bertsekas", "Athena Bertsekas", "Sinica", "Tsitsiklis", "J. N.", "Athena Scientific", "Brockman", "Pettersson", "Schneider", "Schulman", "Tang", "Castro", "Markov", "Duchi", "J. Derivations", "Berkeley", "Red Hook", "Ferns", "Panangaden", "Arlington", "Haarnoja", "Zhou", "Machine Learning", "Kakade", "San Francisco", "Morgan Kaufmann Publishers", "Kemertas", "Kullback", "Lan", "Walsh", "Littman", "Munos", "Nachum", "Bengio", "Wallach", "Garnett", "Gibbs", "Statistical Review", "Pinsker", "Qiao", "Minematsu", "Bisimulation Metrics Ravindran", "Barto", "Scherrer", "Blei", "Machine Learning Research", "Wolski", "Singh", "Touretzky", "Leen", "Villani", "Springer Science", "Liao", "J. Scalable", "Systems", "Zhang", "Gal", "Ziebart", "Maas", "Dey", "Bisimulation Metrics", "Trusted Approximate Policy Iteration", "Definition", "Follows", "Lemma", "Fubinis", "Bisimulation Metrics Lemma", "Corollary", "Thm", "Em", "Rj", "Otherwise", "V V", "Lemma A.8", "Lemma A.10", "V B", "Tg V", "T V", "T V Tg V", "V TV", "Lemma A.11", "Corollary A.4", "Ek", "D TV", "Ek1 Plugging"]}{"title": ["Applications of Machine Learning in Healthcare and Internet of Things (IOT): A Comprehensive Review"], "authors": ["[arxiv.Result.Author('Farid Ghareh Mohammadi'), arxiv.Result.Author('Farzan Shenavarmasouleh'), arxiv.Result.Author('Hamid R. Arabnia')]"], "link": ["http://arxiv.org/pdf/2202.02868v1"], "summary": "In recent years, smart healthcare IoT devices have become ubiquitous, but\nthey work in isolated networks due to their policy. Having these devices\nconnected in a network enables us to perform medical distributed data analysis.\nHowever, the presence of diverse IoT devices in terms of technology, structure,\nand network policy, makes it a challenging issue while applying traditional\ncentralized learning algorithms on decentralized data collected from the IoT\ndevices. In this study, we present an extensive review of the state-of-the-art\nmachine learning applications particularly in healthcare, challenging issues in\nIoT, and corresponding promising solutions. Finally, we highlight some\nopen-ended issues of IoT in healthcare that leaves further research studies and\ninvestigation for scientists.", "entities_include_in_text": ["Ullah et al., 2021", "Szegedy et al., 2015", "Cho et al., 2014", "DiFrancesco, 1993", "Schmerber et\n\nal., 2017", "Csoeregi et al., 1994", "Iandola et al., 2016", "J. Wang et al., 2018", "F. Wang et al., 2017", "Miller, 1998", "Gondalia et al., 2018", "Gondalia et al., 2018", "Yao et al.,\n\n2019", "Basingab, 2020", "Basingab, 2020; Mambou et al., 2016", "Shahrestani, 2017", "Gupta et al., 2018", "Jourdan et al., 2020", "Jourdan et al., 2020", "Jourdan et al., 2020", "Mitchell et al., 2018", "Fontenla-Romero et al., 2013", "Hassan et al., 2020", "Mohammadi et al., 2020", "Lim et al., 2020", "Xu et al., 2021", "Lim et al., 2020", "Lim et al., 2020", "Truex et al.,\n\n2019", "Wu et al., 2020", "Chen et al., 2020", "Truex et al., 2019", "Truex et al., 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors"], "authors": ["[arxiv.Result.Author('Christina G\u00f6pfert'), arxiv.Result.Author('Yinlam Chow'), arxiv.Result.Author('Chih-wei Hsu'), arxiv.Result.Author('Ivan Vendrov'), arxiv.Result.Author('Tyler Lu'), arxiv.Result.Author('Deepak Ramachandran'), arxiv.Result.Author('Craig Boutilier')]"], "link": ["http://arxiv.org/pdf/2202.02830v1"], "summary": "Interactive recommender systems (RSs) allow users to express intent,\npreferences and contexts in a rich fashion, often using natural language. One\nchallenge in using such feedback is inferring a user's semantic intent from the\nopen-ended terms used to describe an item, and using it to refine\nrecommendation results. Leveraging concept activation vectors (CAVs) [21], we\ndevelop a framework to learn a representation that captures the semantics of\nsuch attributes and connects them to user preferences and behaviors in RSs. A\nnovel feature of our approach is its ability to distinguish objective and\nsubjective attributes and associate different senses with different users.\nUsing synthetic and real-world datasets, we show that our CAV representation\naccurately interprets users' subjective semantics, and can improve\nrecommendations via interactive critiquing", "entities_include_in_text": ["ICDM 2008"], "entities_from_reference": ["Plusieurs"]}{"title": ["Aligning Eyes between Humans and Deep Neural Network through Interactive Attention Alignment"], "authors": ["[arxiv.Result.Author('Yuyang Gao'), arxiv.Result.Author('Tong Sun'), arxiv.Result.Author('Liang Zhao'), arxiv.Result.Author('Sungsoo Hong')]"], "link": ["http://arxiv.org/pdf/2202.02838v1"], "summary": "While Deep Neural Networks (DNNs) are deriving the major innovations in\nnearly every field through their powerful automation, we are also witnessing\nthe peril behind automation as a form of bias, such as automated racism, gender\nbias, and adversarial bias. As the societal impact of DNNs grows, finding an\neffective way to steer DNNs to align their behavior with the human mental model\nhas become indispensable in realizing fair and accountable models. We propose a\nnovel framework of Interactive Attention Alignment (IAA) that aims at realizing\nhuman-steerable Deep Neural Networks (DNNs). IAA leverages DNN model\nexplanation method as an interactive medium that humans can use to unveil the\ncases of biased model attention and directly adjust the attention. In improving\nthe DNN using human-generated adjusted attention, we introduce GRADIA, a novel\ncomputational pipeline that jointly maximizes attention quality and prediction\naccuracy. We evaluated IAA framework in Study 1 and GRADIA in Study 2 in a\ngender classification problem. Study 1 found applying IAA can significantly\nimprove the perceived quality of model attention from human eyes. In Study 2,\nwe found using GRADIA can (1) significantly improve the perceived quality of\nmodel attention and (2) significantly improve model performance in scenarios\nwhere the training samples are limited. We present implications for future\ninteractive user interfaces design towards human-alignable AI.", "entities_include_in_text": ["February 2022", "Tesla 2020", "Hong et al. 2020", "Kendall 2020", "Feng and Wu 2020", "Guidotti et al. 2019", "Hong et al. 2020", "Chouldechova and Roth 2018; Saxena\net al. 2019", "Baeza-Yates 2018; Burns et al. 2018;\nChouldechova and Roth 2018", "Barlas et al. 2021; Burns\net al. 2018; Chouldechova and Roth 2018", "Kim et al. 2019; Weiss et al. 2007", "Chawla et al. 2002; He and Garcia 2009", "Wang et al. 2019", "Burns et al. 2018", "Wang et al. 2019", "Burns et al. 2018", "Zhao et al. 2017", "Sagawa et al. 2020", "Burns et al. 2018; Quadrianto et al. 2019", "Selvaraju et al. 2017]", "Selvaraju et al. 2017", "Mitsuhara et al. 2019", "Lin et al. 2014", "Hu et al. 2016; Zhang et al. 2016", "Fukui\net al. 2019", "Mitsuhara\net al. 2019", "Liu and Avci 2019", "Mitsuhara et al. 2019", "Liu and Avci 2019", "Mitsuhara et al. 2019; Shao et al. 2020", "Jacovi and Goldberg 2020; Ross et al. 2017", "Visotsky et al. 2019", "Liu and Avci 2019]", "Kahng\net al. 2018; Pezzotti et al. 2017", "Hall et al. 2009", "Fails and Olsen Jr 2003", "Talbot et al. 2009", "Choo et al. 2010", "Alsallakh et al. 2014", "Ren\net al. 2016", "Dingen et al. 2019", "Brooks et al. 2015", "Amershi et al. 2015", "Kahng et al. 2016", "Krause et al. 2016", "Ming et al. 2018", "Britton 2019", "Kahng et al. 2018; Pezzotti et al. 2017", "Strobelt\net al. 2017", "Ming et al. 2017", "Liu et al. 2016", "Bilal\net al. 2017", "Liu\net al. 2017", "Kahng et al. 2018", "Wongsuphasawat et al. 2017", "Cabrera et al. 2019", "Yan et al.\n2020", "Chouldechova and Roth 2018", "Pedreshi et al.\n2008; Zliobaite 2015", "Kim et al. 2019; Quadrianto et al. 2019", "Calders et al. 2009; Kamiran and Calders 2009", "Aghaei et al. 2019", "Wang et al. 2019", "Calders and Verwer 2010;\nHardt et al. 2016", "Kamiran et al. 2010", "Nushi et al. 2018", "Burns et al. 2018; Wang et al. 2019", "Wang et al. 2019", "Nushi et al. 2018", "Zhou et al. 2016", "Fukui et al. 2019", "Selvaraju et al. 2017", "Mitsuhara et al. 2019", "Liu and Avci 2019", "Mitsuhara et al. 2019", "Barlas et al. 2021; Zhao et al. 2017", "Lin et al. 2014", "Barlas et al. 2021; Burns et al. 2018; Zhao et al. 2017", "Lin et al. 2014", "Zhao et al. 2017", "He et al. 2016", "Bau et al. 2017", "Barlas et al. 2021", "Mitsuhara\net al. 2019", "Fukui et al. 2019; Mitsuhara et al. 2019", "He et al. 2016", "Fukui et al. 2019", "Zhou 2018", "Chung et al. 2021", "Mitsuhara et al. 2019", "Burns et al.\n2018", "Kosti et al. 2017", "Zellers et al. 2019", "Kipf and Welling 2016", "Pope et al. 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["The Self-Driving Car: Crossroads at the Bleeding Edge of Artificial Intelligence and Law"], "authors": ["[arxiv.Result.Author('Scott McLachlan'), arxiv.Result.Author('Evangelia Kyrimi'), arxiv.Result.Author('Kudakwashe Dube'), arxiv.Result.Author('Norman Fenton'), arxiv.Result.Author('Burkhard Schafer')]"], "link": ["http://arxiv.org/pdf/2202.02734v1"], "summary": "Artificial intelligence (AI) features are increasingly being embedded in cars\nand are central to the operation of self-driving cars (SDC). There is little or\nno effort expended towards understanding and assessing the broad legal and\nregulatory impact of the decisions made by AI in cars. A comprehensive\nliterature review was conducted to determine the perceived barriers, benefits\nand facilitating factors of SDC in order to help us understand the suitability\nand limitations of existing and proposed law and regulation. (1) existing and\nproposed laws are largely based on claimed benefits of SDV that are still\nmostly speculative and untested; (2) while publicly presented as issues of\nassigning blame and identifying who pays where the SDC is involved in an\naccident, the barriers broadly intersect with almost every area of society,\nlaws and regulations; and (3) new law and regulation are most frequently\nidentified as the primary factor for enabling SDC. Research on assessing the\nimpact of AI in SDC needs to be broadened beyond negligence and liability to\nencompass barriers, benefits and facilitating factors identified in this paper.\nResults of this paper are significant in that they point to the need for deeper\ncomprehension of the broad impact of all existing law and regulations on the\nintroduction of SDC technology, with a focus on identifying only those areas\ntruly requiring ongoing legislative attention.", "entities_include_in_text": ["\n \ns\ne\nl\nc\ni\nh\ne\nV\n\n \nd\nn\na\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \n,\nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \nd\ne\nt\nc\ne\nn\nn\no\nC\n\n \ns\nr\na\nC\n\n \nt\nn\ne\ng\ni\nl\nl\ne\nt\nn\nI\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ns\nu\no\nm\no\nn\no\nt\nu\na\n-\ni\n\nm\ne\nS\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\nr\na\nC\n \ns\nu\no\nm\no\nn\no\nt\nu\na\n-\ni\n\nm\ne\nS\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\nr\na\nC\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ns\ns\ne\nl\nr\ne\nv\ni\nr\n\nD\n\n \ns\nr\na\nC\n \ns\ns\ne\nl\nr\ne\nv\ni\nr\n\nD\n\n \ns\nr\na\nC\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \nr\no\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \nr\no\n \nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\nr\na\nC\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n\n \ns\nr\na\nC\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n\n \ns\nr\na\nC\n \nd\ne\nt\nc\ne\nn\nn\no\nC\n\n \n\n \n\nD\nE\nS\nU\nS\nM\nR\nE\nT\nL\nA\nT\nO\nT\n\n \n\n \n\nTOTAL \n\nGaeta (2019", "ICCESE 2020). \nLeiman, T. (2020", "Reprinted from. \nEC. (2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["SFMGNet: A Physics-based Neural Network To Predict Pedestrian Trajectories"], "authors": ["[arxiv.Result.Author('Sakif Hossain'), arxiv.Result.Author('Fatema T. Johora'), arxiv.Result.Author('J\u00f6rg P. M\u00fcller'), arxiv.Result.Author('Sven Hartmann'), arxiv.Result.Author('Andreas Reinhardt')]"], "link": ["http://arxiv.org/pdf/2202.02791v1"], "summary": "Autonomous robots and vehicles are expected to soon become an integral part\nof our environment. Unsatisfactory issues regarding interaction with existing\nroad users, performance in mixed-traffic areas and lack of interpretable\nbehavior remain key obstacles. To address these, we present a physics-based\nneural network, based on a hybrid approach combining a social force model\nextended by group force (SFMG) with Multi-Layer Perceptron (MLP) to predict\npedestrian trajectories considering its interaction with static obstacles,\nother pedestrians and pedestrian groups. We quantitatively and qualitatively\nevaluate the model with respect to realistic prediction, prediction performance\nand prediction \"interpretability\". Initial results suggest, the model even when\nsolely trained on a synthetic dataset, can predict realistic and interpretable\ntrajectories with better than state-of-the-art accuracy.", "entities_include_in_text": ["MABS 2020"], "entities_from_reference": ["Johora", "J. P. Muller", "Springer", "Clausthal University", "Yang", "Vehicles Symposium", "Technical Report", "Modelling", "Trajectory", "Scholler", "Wang", "Jiang", "Alahi", "Gupta", "J. Johnson", "Pattern Recognition", "Vemula", "J. Oh", "Huang", "Stgat", "Toward", "Ohio St.", "Kroll", "Kim", "Karpatne", "Atluri", "J. H. Faghmous", "Steinbach", "Antonucci", "Papini", "Deep", "Towards", "Science China", "Zhao", "John Wiley", "Sons", "Design", "Artech House", "Schindler", "Van Gool", "Youll", "Lerner", "Wiley Online Library", "Zhang", "J. J. Baldelomar", "Hayet", "J. Pettre", "Opentraj", "Sadeghian", "Sophie"]}{"title": ["Energy-Aware Edge Association for Cluster-based Personalized Federated Learning"], "authors": ["[arxiv.Result.Author('Y. Li'), arxiv.Result.Author('X. Qin'), arxiv.Result.Author('H. Chen'), arxiv.Result.Author('K. Han'), arxiv.Result.Author('P. Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02727v1"], "summary": "Federated Learning (FL) over wireless network enables data-conscious services\nby leveraging the ubiquitous intelligence at network edge for\nprivacy-preserving model training. As the proliferation of context-aware\nservices, the diversified personal preferences causes disagreeing conditional\ndistributions among user data, which leads to poor inference performance. In\nthis sense, clustered federated learning is proposed to group user devices with\nsimilar preference and provide each cluster with a personalized model. This\ncalls for innovative design in edge association that involves user clustering\nand also resource management optimization. We formulate an accuracy-cost\ntrade-off optimization problem by jointly considering model accuracy,\ncommunication resource allocation and energy consumption. To comply with\nparameter encryption techniques in FL, we propose an iterative solution\nprocedure which employs deep reinforcement learning based approach at cloud\nserver for edge association. The reward function consists of minimized energy\nconsumption at each base station and the averaged model accuracy of all users.\nUnder our proposed solution, multiple edge base station are fully exploited to\nrealize cost efficient personalized federated learning without any prior\nknowledge on model parameters. Simulation results show that our proposed\nstrategy outperforms existing strategies in achieving accurate learning at low\nenergy cost.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal"], "authors": ["[arxiv.Result.Author('David Leslie'), arxiv.Result.Author('Christopher Burr'), arxiv.Result.Author('Mhairi Aitken'), arxiv.Result.Author('Michael Katell'), arxiv.Result.Author('Morgan Briggs'), arxiv.Result.Author('Cami Rincon')]"], "link": ["http://arxiv.org/pdf/2202.02776v1"], "summary": "Following on from the publication of its Feasibility Study in December 2020,\nthe Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and\nits subgroups initiated efforts to formulate and draft its Possible Elements of\na Legal Framework on Artificial Intelligence, based on the Council of Europe's\nstandards on human rights, democracy, and the rule of law. This document was\nultimately adopted by the CAHAI plenary in December 2021. To support this\neffort, The Alan Turing Institute undertook a programme of research that\nexplored the governance processes and practical tools needed to operationalise\nthe integration of human right due diligence with the assurance of trustworthy\nAI innovation practices.\n  The resulting framework was completed and submitted to the Council of Europe\nin September 2021. It presents an end-to-end approach to the assurance of AI\nproject lifecycles that integrates context-based risk analysis and appropriate\nstakeholder engagement with comprehensive impact assessment, and transparent\nrisk management, impact mitigation, and innovation assurance practices. Taken\ntogether, these interlocking processes constitute a Human Rights, Democracy and\nthe Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the\nprocedural requirements for principles-based human rights due diligence with\nthe governance mechanisms needed to set up technical and socio-technical\nguardrails for responsible and trustworthy AI innovation practices. Its purpose\nis to provide an accessible and user-friendly set of mechanisms for\nfacilitating compliance with a binding legal framework on artificial\nintelligence, based on the Council of Europe's standards on human rights,\ndemocracy, and the rule of law, and to ensure that AI innovation projects are\ncarried out with appropriate levels of public accountability, transparency, and\ndemocratic governance.", "entities_include_in_text": ["Levine 2012", "Duijm 2015", "Rausand and Haugen 2020", "quantified in loss function \n\n                                       \nS. (2012", "Aven \n2017", "Krisper  2021", "Cox 2008", "Smith et al 2009", "Levine 2012", "Duijm \n2015", "Baybutt  2017", "Krisper  2021", "Esteves, et al 2017", "UNGP,  2011", "UNHROHC \n2020", "Ashmore,  Calinescu,  and  Paterson  2019", "Sweenor et al. 2020", "ICO  and  Institute  2020", "see  Burton  et  al. \n2020", "Mitchell  et  al.  2019; \nAshmore,  Calinescu,  and  Paterson  2019", "see Burton et al. 2020"], "entities_from_reference": ["Plusieurs"]}{"title": ["Deep Learning-Aided Spatial Multiplexing with Index Modulation"], "authors": ["[arxiv.Result.Author('Merve Turhan'), arxiv.Result.Author('Ersin Ozturk'), arxiv.Result.Author('Hakan Ali Cirpan')]"], "link": ["http://arxiv.org/pdf/2202.02856v1"], "summary": "In this paper, deep learning (DL)-aided data detection of spatial\nmultiplexing (SMX) multiple-input multiple-output (MIMO) transmission with\nindex modulation (IM) (Deep-SMX-IM) has been proposed. Deep-SMX-IM has been\nconstructed by combining a zero-forcing (ZF) detector and DL technique. The\nproposed method uses the significant advantages of DL techniques to learn\ntransmission characteristics of the frequency and spatial domains. Furthermore,\nthanks to using subblockbased detection provided by IM, Deep-SMX-IM is a\nstraightforward method, which eventually reveals reduced complexity. It has\nbeen shown that Deep-SMX-IM has significant error performance gains compared to\nZF detector without increasing computational complexity for different system\nconfigurations.", "entities_include_in_text": ["Jun 2017", "Dec 2017", "Nov 2013", "Jun 2017)\n\n(2018", "Feb\n2020", "Aug 2019", "Sep 2014", "Dec 2016", "Jun 2016", "Oct 2017", "Jun 2017", "Oct 2018", "Dec 2017", "Feb 2014"], "entities_from_reference": ["Basar", "Ayg", "Panayrc", "Poor", "Corlay", "Multilevel MIMO", "Adam", "Goodfellow", "Bengio", "Huang", "Guo", "Gui", "Yang", "Zhang", "Sari", "Adachi", "Luong", "Vien", "Matthaiou", "Deep", "Michailow", "Ozturk", "Black Sea Conf", "Varna", "Istanbul", "Samuel", "Turhan", "Mobile Radio Communications", "Index", "Balkan Conf", "Skopje"]}{"title": ["(Almost) Envy-Free, Proportional and Efficient Allocations of an Indivisible Mixed Manna"], "authors": ["[arxiv.Result.Author('Vasilis Livanos'), arxiv.Result.Author('Ruta Mehta'), arxiv.Result.Author('Aniket Murhekar')]"], "link": ["http://arxiv.org/pdf/2202.02672v1"], "summary": "We study the problem of finding fair and efficient allocations of a set of\nindivisible items to a set of agents, where each item may be a good (positively\nvalued) for some agents and a bad (negatively valued) for others, i.e., a mixed\nmanna. As fairness notions, we consider arguably the strongest possible\nrelaxations of envy-freeness and proportionality, namely envy-free up to any\nitem (EFX and EFX$_0$), and proportional up to the maximin good or any bad\n(PropMX and PropMX$_0$). Our efficiency notion is Pareto-optimality (PO).\n  We study two types of instances:\n  (i) Separable, where the item set can be partitioned into goods and bads, and\n(ii) Restricted mixed goods (RMG), where for each item $j$, every agent has\neither a non-positive value for $j$, or values $j$ at the same $v_j>0$. We\nobtain polynomial-time algorithms for the following:\n  (i) Separable instances: PropMX$_0$ allocation.\n  (ii) RMG instances: Let pure bads be the set of items that everyone values\nnegatively.\n  - PropMX allocation for general pure bads.\n  - EFX+PropMX allocation for identically-ordered pure bads.\n  - EFX+PropMX+PO allocation for identical pure bads.\n  Finally, if the RMG instances are further restricted to binary mixed goods\nwhere all the $v_j$'s are the same, we strengthen the results to guarantee\nEFX$_0$ and PropMX$_0$ respectively.", "entities_include_in_text": [], "entities_from_reference": ["Martin Aleksandrov", "Toby Walsh", "Georgios Amanatidis", "Georgios Birmpas", "Aris Filos-Ratsikas", "Alexandros Hollender", "Alexandros A. Voudouris", "Maximum Nash", "Haris Aziz", "Ioannis Caragiannis", "Ayumi Igarashi", "Fair", "Herv", "Moulin", "Fedor Sandomirskiy", "Artem Baklanov", "Pranav Garimidi", "Vasilis Gkatzelis", "Daniel Schoepflin", "Nikhil Bansal", "Maxim Sviridenko", "Machinery", "Sanath Kumar Krishnamurthy", "Econ", "Rohit Vaish", "Barman", "Umang Bhaskar", "Anna Bogomolnaia", "Elena Yanovskaya", "Sylvain Bouveret", "Michel Lematre", "David Kurokawa", "Ariel D. Procaccia", "Nisarg Shah", "Bhaskar Ray Chaudhury", "Jugal Garg", "Kurt Mehlhorn", "Andreas Darmann", "Joachim Schauer", "Nash", "Foley", "Yale Economic Essays", "Peter McGlaughlin", "Aniket Murhekar", "Algorithmic Game Theory", "John Qin", "Daniel Halpern", "Alexandros Psomas", "Nikolai Gravin", "Martin Hoefer", "Ruta Mehta", "Web", "Cham", "Springer International Publishing", "Huang", "Pinyan Lu", "Rucha Kulkarni", "Setareh Taki", "Yingkai Li", "Xiaowei Wu", "Almost", "Lipton", "Annual Review", "Benjamin Plaut", "Tim Roughgarden", "Annual", "Econometrica", "Equity", "Goods", "Sandomirskiy"]}{"title": ["Pipe Overflow: Smashing Voice Authentication for Fun and Profit"], "authors": ["[arxiv.Result.Author('Shimaa Ahmed'), arxiv.Result.Author('Yash Wani'), arxiv.Result.Author('Ali Shahin Shamsabadi'), arxiv.Result.Author('Mohammad Yaghini'), arxiv.Result.Author('Ilia Shumailov'), arxiv.Result.Author('Nicolas Papernot'), arxiv.Result.Author('Kassem Fawaz')]"], "link": ["http://arxiv.org/pdf/2202.02751v1"], "summary": "Recent years have seen a surge of popularity of acoustics-enabled personal\ndevices powered by machine learning. Yet, machine learning has proven to be\nvulnerable to adversarial examples. Large number of modern systems protect\nthemselves against such attacks by targeting the artificiality, i.e., they\ndeploy mechanisms to detect the lack of human involvement in generating the\nadversarial examples. However, these defenses implicitly assume that humans are\nincapable of producing meaningful and targeted adversarial examples. In this\npaper, we show that this base assumption is wrong. In particular, we\ndemonstrate that for tasks like speaker identification, a human is capable of\nproducing analog adversarial examples directly with little cost and\nsupervision: by simply speaking through a tube, an adversary reliably\nimpersonates other speakers in eyes of ML models for speaker identification.\nOur findings extend to a range of other acoustic-biometric tasks such as\nliveness, bringing into question their use in security-critical settings in\nreal life, such as phone banking.", "entities_include_in_text": [], "entities_from_reference": ["Source", "Anatomy", "Shimaa Ahmed", "Ilia Shumailov", "Nicolas Papernot", "Kassem Fawaz", "Abdulaziz M Aljalal", "Sound", "European Journal", "Andrew Allen", "Nikunj Raghuvanshi", "Edinburgh", "Dublin Philosophical Magazine", "Pratyusha Kalluri", "Dallas Card", "William Agnew", "Ravit Dotan", "Michelle Bao", "Hadi Abdullah", "Luis Vargas", "Patrick Traynor", "Hello", "Mobile Networks", "Machinery", "Nicholas Boucher", "Ross Anderson", "Stephen Boyd", "Lieven Vandenberghe", "Convex Optimization", "Cambridge University Press", "Timnit Gebru", "Christo Wilson", "Machine Learning Research", "Kui Ren", "Sixu Piao", "Qian Wang", "Jian Weng", "Lu Su", "Aziz Mohaisen", "Hongwei Luo", "Yijie Shen", "Feng Lin", "Guoai Xu", "Xinyun Chen", "Chang Liu", "Bo Li", "Kimberly Lu", "Dawn Song", "Chung", "Voxceleb2", "John R. Deller", "John H.", "John G. Proakis", "Speech Production", "Jenthe Thienpondt", "Kris Demuynck", "Shelley", "Benoit Fabre", "Gilbert", "Avraham Hirschberg", "Xavier Pelorson", "Annual", "Morgan Klaus Scheuerman", "Stacy M. Branham", "Gender Recognition", "Reductionism", "Tian Tan", "Andre Kassis", "Urs Hengartner", "George Kersta", "Jong Wook Kim", "Justin Salamon", "Peter Li", "Juan Pablo Bello", "Crepe", "Kinsler", "Austin R Frey", "Alan B Coppens", "James V Sanders", "John", "Tom Ko", "Vijayaditya Peddinti", "Daniel Povey", "Michael L Seltzer", "Sanjeev Khudanpur", "Stepan Komkov", "Aleksandr Petiushko", "Pattern Recognition", "Harold Levine", "Julian Schwinger", "Songxiang Liu", "Haibin Wu", "Hung", "Lee", "Helen Meng", "Leena Mary", "Springer", "Quatieri", "Pitch", "Zichang Wang", "Wei Zhang", "Peilin Wu", "Haojin Zhu", "Xiaohui Liang", "Yao Liu", "Wivo", "Mobile Ad Hoc Networking", "Michael J Moloney", "Daniel L Hatten", "Acoustic", "Mumtaj Begam", "Nagrani", "J. S. Chung", "Voxceleb", "Joon Son Chung", "Weidi Xie", "Andrew Zisserman", "Johannes Nederveen", "Jason Yosinski", "Jeff Clune", "Deep", "Krzysztof Slot", "Ajith Abraham", "Manuel Gra", "Hybrid Artificial", "Berlin", "Heidelberg", "Springer Berlin Heidelberg", "Ayesha Pervaiz", "Fawad Hussain", "Huma Israr", "Muhammad Ali Tahir", "Fawad Riasat Raja", "Naveed Khan Baloch", "Farruh Ishmanov", "Yousaf Bin Zikria", "Nicholas Carlini", "Garrison Cottrell", "Ian Goodfellow", "Colin Raffel", "Joaquin Qui", "Masashi Sugiyama", "Anton Schwaighofer", "Neil D Lawrence", "Dataset", "Mit Press", "Leann Down", "Adam Jonas", "Elham Tabassi", "Shahin Shamsabadi", "Francisco Sep", "Alberto Abad", "Bhiksha Raj", "Andrea Cavallaro", "Isabel Trancoso", "Foolhd", "Jiacheng Shang", "Si Chen", "Jie Wu", "Mobile Ad Hoc", "Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K. Reiter", "Berrak Sisman", "Mingyang Zhang", "Sakriani Sakti", "Haizhou Li", "Satoshi Nakamura", "Technology Workshop", "Tamara Smyth", "Jonathan S Abel", "Acta Acustica", "Acustica", "Robust DNN", "David Snyder", "Daniel Garcia-Romero", "Kenneth N Stevens", "Rainer Storn", "Kenneth Price", "Global Optimization", "Jiawei Su", "Danilo Vasconcellos Vargas", "Kouichi Sakurai", "Motoaki Kawanabe", "Machine", "Hemlata Tak", "Jose Patino", "Massimiliano Todisco", "Andreas Nautsch", "Nicholas Evans", "Anthony Larcher", "Paul Taylor", "Cambridge", "Wiebe Van Ranst", "Toon Goedem", "Fooling", "Umetani", "Ryan Schmidt", "Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Andrew Senior", "Koray Kavukcuoglu", "Chen Wang", "S Abhishek Anand", "Jian Liu", "Payton Walker", "Chen", "Nitesh Saxena", "Annual Computer Security Applications Conference", "Yao Wang", "Wandong Cai", "Tao Gu", "Wei Shao", "Yannan Li", "Yong Yu", "Yuxuan Wang", "Daisy Stanton", "Yonghui Wu", "Ron J. Weiss", "Navdeep Jaitly", "Zongheng Yang", "Xiao", "Zhifeng Chen", "Samy Bengio", "Quoc Le", "Yannis Agiomyrgiannakis", "Rob Clark", "Rif A. Saurous", "Tacotron", "Eric Wong", "Leslie Rice", "Fast", "Zhizheng Wu", "Tomi Kinnunen", "Junichi Yamagishi", "Federico Alegre", "Speech Communication", "Zuxuan Wu", "Larry Davis", "Tom Goldstein", "Gaoyuan Zhang", "Sijia Liu", "Quanfu Fan", "Mengshu Sun", "Hongge Chen", "Yanzhi Wang", "Xue Lin", "Weilin Xu", "Yanjun Qi", "David Evans", "Chen Yan", "Xiaoyu Ji", "Wenyuan Xu", "Siyang Yuan", "Pengyu Cheng", "Ruiyi Zhang", "Weituo Hao", "Zhe Gan", "Lawrence Carin", "Linghan Zhang", "Sheng Tan", "Jie Yang", "Di Tang", "Xiaofeng Wang", "Weili Han", "Xiangyu Liu", "Kehuan Zhang", "Philip Steels", "Tom", "Low", "Lord", "Phil", "Ill", "Gad", "Gregson", "Philip", "Input"]}{"title": ["Block shuffling learning for Deepfake Detection"], "authors": ["[arxiv.Result.Author('Sitong Liu'), arxiv.Result.Author('Zhichao Lian'), arxiv.Result.Author('Siqi Gu'), arxiv.Result.Author('Liang Xiao')]"], "link": ["http://arxiv.org/pdf/2202.02819v1"], "summary": "Although the deepfake detection based on convolutional neural network has\nachieved good results, the detection results show that these detectors show\nobvious performance degradation when the input images undergo some common\ntransformations (like resizing, blurring), which indicates that the\ngeneralization ability of the detector is insufficient. In this paper, we\npropose a novel block shuffling learning method to solve this problem.\nSpecifically, we divide the images into blocks and then introduce the random\nshuffling to intra-block and inter-block. Intra-block shuffling increases the\nrobustness of the detector and we also propose an adversarial loss algorithm to\novercome the over-fitting problem brought by the noise introduced by shuffling.\nMoreover, we encourage the detector to focus on finding differences among the\nlocal features through inter-block shuffling, and reconstruct the spatial\nlayout of the blocks to model the semantic associations between them.\nEspecially, our method can be easily integrated with various CNN models.\nExtensive experiments show that our proposed method achieves state-of-the-art\nperformance in forgery face detection, including good generalization ability in\nthe face of common image transformations.", "entities_include_in_text": [], "entities_from_reference": ["Darius Afchar", "Vincent Nozick", "Junichi Yamagishi", "Isao Echizen", "Maungmaung Aprilpyone", "Hitoshi Kiya", "Encryption", "Image Processing", "Valentin Bazarevsky", "Yury Kartynnik", "Andrey Vakunov", "Karthik Raveendran", "Matthias Grundmann", "Blazeface", "Third Workshop", "Scott McCloskey", "Jingyi Yu", "Focus", "Pattern Recognition", "Salt Lake City", "Yalong Bai", "Wei Zhang", "Tao Mei", "Giovanni Chierchia", "Sara Parrilli", "Giovanni Poggi", "Luisa Verdoliva", "Carlo Sansone", "Chollet", "Tatsuya Chuman", "Warit Sirichotedumrong", "Deepfakes", "Mengnan Du", "Shiva K. Pentyala", "Li", "Xia Hu", "Towards", "Stefan Dietze", "Claudia Hauff", "Edward Curry", "Philippe Cudr", "Knowledge Management", "Virtual Event", "Ricard Durall", "Margret Keuper", "Janis Keuper", "Faceswap", "Jessica J. Fridrich", "Jan Kodovsk", "Rich", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun", "Deep", "Las Vegas", "Shuiwang Li", "Yuchao Yang", "Feiyu Zhu", "Qijun Zhao", "Li Lu", "Remote Sensing Letters", "Hyeonseong Jeon", "Youngoh Bang", "Simon S. Woo", "Jimmy Ba", "Adam", "San Diego", "Lingzhi Li", "Jianmin Bao", "Ting Zhang", "Hao Yang", "Dong Chen", "Fang Wen", "Face", "Yuezun Li", "Siwei Lyu", "Pattern Recognition Workshops", "Xin Yang", "Pu Sun", "Honggang Qi", "Honggu Liu", "Xiaodan Li", "Wenbo Zhou", "Yuefeng Chen", "Hui Xue", "Weiming Zhang", "Nenghai Yu", "Jan Luk", "Miroslav Goljan", "Electronic Imaging", "Yucheng Luo", "Yong Zhang", "Junchi Yan", "Wei Liu", "Huy Hoang Nguyen", "Fuming Fang", "Huy H. Nguyen", "Yuyang Qian", "Guojun Yin", "Lu Sheng", "Zixuan Chen", "Andreas R", "Davide Cozzolino", "Christian Riess", "Justus Thies", "Matthias Niener", "Sara Sabour", "Nicholas Frosst", "Geoffrey E. Hinton", "Dynamic", "Ulrike", "Samy Bengio", "Hanna M. Wallach", "Rob Fergus", "Roman Garnett", "Yuma Kinoshita", "Michael Zollh", "Marc Stamminger", "Christian Theobalt", "Chengrui Wang", "Weihong Deng", "Junke Wang", "Zuxuan Wu", "Jingjing Chen", "Yugang Jiang", "Zhao", "Dongdong Chen", "Tianyi Wei", "Zhun Zhong", "Liang Zheng", "Guoliang Kang", "Shaozi Li", "Yi Yang", "Random", "Apr", "Peng Zhou", "Xintong Han", "Vlad I. Morariu", "Larry S. Davis"]}{"title": ["A survey of top-down approaches for human pose estimation"], "authors": ["[arxiv.Result.Author('Thong Duy Nguyen'), arxiv.Result.Author('Milan Kresovic')]"], "link": ["http://arxiv.org/pdf/2202.02656v1"], "summary": "Human pose estimation in two-dimensional images videos has been a hot topic\nin the computer vision problem recently due to its vast benefits and potential\napplications for improving human life, such as behaviors recognition, motion\ncapture and augmented reality, training robots, and movement tracking. Many\nstate-of-the-art methods implemented with Deep Learning have addressed several\nchallenges and brought tremendous remarkable results in the field of human pose\nestimation. Approaches are classified into two kinds: the two-step framework\n(top-down approach) and the part-based framework (bottom-up approach). While\nthe two-step framework first incorporates a person detector and then estimates\nthe pose within each box independently, detecting all body parts in the image\nand associating parts belonging to distinct persons is conducted in the\npart-based framework. This paper aims to provide newcomers with an extensive\nreview of deep learning methods-based 2D images for recognizing the pose of\npeople, which only focuses on top-down approaches since 2016. The discussion\nthrough this paper presents significant detectors and estimators depending on\nmathematical background, the challenges and limitations, benchmark datasets,\nevaluation metrics, and comparison between methods.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Exploration with Multi-Sample Target Values for Distributional Reinforcement Learning"], "authors": ["[arxiv.Result.Author('Michael Teng'), arxiv.Result.Author('Michiel van de Panne'), arxiv.Result.Author('Frank Wood')]"], "link": ["http://arxiv.org/pdf/2202.02693v1"], "summary": "Distributional reinforcement learning (RL) aims to learn a value-network that\npredicts the full distribution of the returns for a given state, often modeled\nvia a quantile-based critic. This approach has been successfully integrated\ninto common RL methods for continuous control, giving rise to algorithms such\nas Distributional Soft Actor-Critic (DSAC). In this paper, we introduce\nmulti-sample target values (MTV) for distributional RL, as a principled\nreplacement for single-sample target value estimation, as commonly employed in\ncurrent practice. The improved distributional estimates further lend themselves\nto UCB-based exploration. These two ideas are combined to yield our\ndistributional RL algorithm, E2DC (Extra Exploration with Distributional\nCritics). We evaluate our approach on a range of continuous control tasks and\ndemonstrate state-of-the-art model-free performance on difficult tasks such as\nHumanoid control. We provide further insight into the method via visualization\nand analysis of the learned distributions and their evolution during training.", "entities_include_in_text": ["Lee\net al., 2020", "Barth-Maron et al., 2018", "Ma et al., 2020", "Haarnoja et al.,\n2017", "Van Hasselt\net al., 2016; Wang et al., 2016; Lan et al., 2020", "Mavrin et al., 2018", "Todorov\net al., 2012", "Tassa et al., 2020", "Brockman et al., 2016", "Kuznetsov et al., 2020", "Yue\net al., 2020; Ward et al., 2019", "Gangwani\net al., 2018"], "entities_from_reference": ["Gabriel", "Matthew W Hoffman", "David Budden", "Will Dabney", "Dan Horgan", "Dhruva Tb", "Alistair Muldal", "Nicolas Heess", "Timothy Lillicrap", "Gangwani", "Standard", "Extra Exploration", "Machine Learning", "Marc G Bellemare", "Remi Munos", "Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba", "Openai", "Tuomas Haarnoja", "Aurick Zhou", "Kristian Hartikainen", "George Tucker", "Sehoon Ha", "Jie Tan", "Vikash Kumar", "Henry Zhu", "Abhishek Gupta", "Pieter Abbeel", "Sergey Levine", "Kumar", "Justin Fu", "Richard Y Chen", "Szymon Sidor", "Thanard Kurutach", "Ignasi Clavera", "Aviv Tamar", "Kamil Ciosek", "Quan Vuong", "Robert Loftin", "Katja Hofmann", "Better", "Georg Ostrovski", "David Silver", "Implicit", "Dabney", "Mark Rowland", "Marc Bellemare", "Scott Fujimoto", "Herke Hoof", "David Meger", "Tanmay Gangwani", "Qiang Liu", "Jian Peng", "Shixiang Gu", "Zoubin Ghahramani", "Richard E Turner", "Haoran Tang", "Arsenii Kuznetsov", "Pavel Shvechikov", "Alexander Grishin", "Dmitry Vetrov", "Qingfeng Lan", "Yangchen Pan", "Alona Fyshe", "Martha White", "Maxmin", "Kimin Lee", "Michael Laskin", "Aravind Srinivas", "Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Tom Erez", "Yuval Tassa", "Daan Wierstra", "Xiaoteng Ma", "Qiyuan Zhang", "Li Xia", "Zhengyuan Zhou", "Jun Yang", "Qianchuan Zhao", "Borislav Mavrin", "Hengshuai Yao", "Linglong Kong", "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Tim Harley", "Koray Kavukcuoglu", "Thomas M Moerland", "Joost Broekens", "Charles Blundell", "Benjamin Van Roy", "Deep", "M Dalal", "S Lin", "Rlkit", "Philipp Moritz", "Michael Jordan", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov", "Yunhao Tang", "Shipra Agrawal", "Saran Tunyasuvunakool", "Yotam Doron", "Siqi Liu", "Steven Bohez", "Josh Merel", "Deepmind", "Emanuel Todorov", "Mujoco", "Systems", "Hado Van Hasselt", "Arthur Guez", "Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado Hasselt", "Marc Lanctot", "Nando Freitas", "Patrick Nadeem Ward", "Ariella Smofsky", "Avishek Joey Bose", "Yuguang Yue", "Zhendong Wang"]}{"title": ["Learning Synthetic Environments and Reward Networks for Reinforcement Learning"], "authors": ["[arxiv.Result.Author('Fabio Ferreira'), arxiv.Result.Author('Thomas Nierhoff'), arxiv.Result.Author('Andreas Saelinger'), arxiv.Result.Author('Frank Hutter')]"], "link": ["http://arxiv.org/pdf/2202.02790v1"], "summary": "We introduce Synthetic Environments (SEs) and Reward Networks (RNs),\nrepresented by neural networks, as proxy environment models for training\nReinforcement Learning (RL) agents. We show that an agent, after being trained\nexclusively on the SE, is able to solve the corresponding real environment.\nWhile an SE acts as a full proxy to a real environment by learning about its\nstate dynamics and rewards, an RN is a partial proxy that learns to augment or\nreplace rewards. We use bi-level optimization to evolve SEs and RNs: the inner\nloop trains the RL agent, and the outer loop trains the parameters of the SE /\nRN via an evolution strategy. We evaluate our proposed new concept on a broad\nrange of RL algorithms and classic control environments. In a one-to-one\ncomparison, learning an SE proxy requires more interactions with the real\nenvironment than training agents only on the real environment. However, once\nsuch an SE has been learned, we do not need any interactions with the real\nenvironment to train new agents. Moreover, the learned SE proxies allow us to\ntrain agents with fewer interactions while maintaining the original task\nperformance. Our empirical results suggest that SEs achieve this result by\nlearning informed representations that bias the agents towards relevant states.\nMoreover, we find that these proxies are robust against hyperparameter\nvariation and can also transfer to unseen agents.", "entities_include_in_text": ["Such et al., 2020", "Jhang et al., 2020", "Rechenberg, 1973;\nSalimans et al., 2017", "Sutton, 1990)", "Ng et al., 1999", "Brockman et al., 2016", "Hutter et al., 2019", "Paszke et al., 2019", "Sutton, 1990", "Silver et al., 2017; Moerland et al., 2020", "Togelius et al., 2011", "Matiisen et al., 2020", "Volz et al., 2018; Shaker et al., 2016; Wang et al., 2019; Cobbe et al., 2020", "Tobin et al., 2017", "Dennis et al., 2020", "Such et al., 2020", "Pathak et al., 2017; Burda et al., 2019; Singh\net al., 2010; Bellemare et al., 2016; Tang et al., 2017", "Judah et al., 2014; Brys et al., 2015; Ibarz et al., 2018", "Faust et al., 2019; Hu et al., 2020; Jaderberg et al., 2019", "Zheng et al., 2018", "Zou\net al., 2019", "Zheng et al., 2020", "Zheng et al., 2018;\nZou et al., 2019", "Zheng et al.,\n2018", "Zou et al., 2019", "Zheng et al., 2020", "Wierstra et al., 2008", "Metz et al.,\n2019", "Salimans et al., 2017", "Salimans et al.,\nIt consists of an Evolutionary Strategy in the outer loop\n2017", "Salimans et al., 2017", "Ng et al., 1999", "Ng et al., 1999", "Ng et al., 1999", "van Hasselt et al., 2016", "Wang et al., 2016", "Jang et al., 2017", "Falkner et al., 2018", "Watkins, 1989", "Brockman et al., 2016", "Schulman et al., 2017", "Pathak et al., 2017", "Ng et al., 1999", "Salimans et al., 2017", "Jabbari et al., 2017", "Salimans et al.,\n2017", "Sutton et al., 2020", "Pathak et al., 2017", "Pathak et al., 2017", "Pathak et al., 2017", "Pathak et al., 2017", "Wierstra et al., 2008"], "entities_from_reference": ["Plusieurs"]}{"title": ["Doing Right by Not Doing Wrong in Human-Robot Collaboration"], "authors": ["[arxiv.Result.Author('Laura Londo\u00f1o'), arxiv.Result.Author('Adrian R\u00f6fer'), arxiv.Result.Author('Tim Welschehold'), arxiv.Result.Author('Abhinav Valada')]"], "link": ["http://arxiv.org/pdf/2202.02654v1"], "summary": "As robotic systems become more and more capable of assisting humans in their\neveryday lives, we must consider the opportunities for these artificial agents\nto make their human collaborators feel unsafe or to treat them unfairly. Robots\ncan exhibit antisocial behavior causing physical harm to people or reproduce\nunfair behavior replicating and even amplifying historical and societal biases\nwhich are detrimental to humans they interact with. In this paper, we discuss\nthese issues considering sociable robotic manipulation and fair robotic\ndecision making. We propose a novel approach to learning fair and sociable\nbehavior, not by reproducing positive behavior, but rather by avoiding negative\nbehavior. In this study, we highlight the importance of incorporating\nsociability in robot manipulation, as well as the need to consider fairness in\nhuman-robot interactions.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Triangle Graph Interest Network for Click-through Rate Prediction"], "authors": ["[arxiv.Result.Author('Wensen Jiang'), arxiv.Result.Author('Yizhu Jiao'), arxiv.Result.Author('Qingqin Wang'), arxiv.Result.Author('Chuanming Liang'), arxiv.Result.Author('Lijie Guo'), arxiv.Result.Author('Yao Zhang'), arxiv.Result.Author('Zhijun Sun'), arxiv.Result.Author('Yun Xiong'), arxiv.Result.Author('Yangyong Zhu')]"], "link": ["http://arxiv.org/pdf/2202.02698v1"], "summary": "Click-through rate prediction is a critical task in online advertising.\nCurrently, many existing methods attempt to extract user potential interests\nfrom historical click behavior sequences. However, it is difficult to handle\nsparse user behaviors or broaden interest exploration. Recently, some\nresearchers incorporate the item-item co-occurrence graph as an auxiliary. Due\nto the elusiveness of user interests, those works still fail to determine the\nreal motivation of user click behaviors. Besides, those works are more biased\ntowards popular or similar commodities. They lack an effective mechanism to\nbreak the diversity restrictions. In this paper, we point out two special\nproperties of triangles in the item-item graphs for recommendation systems:\nIntra-triangle homophily and Inter-triangle heterophiy. Based on this, we\npropose a novel and effective framework named Triangle Graph Interest Network\n(TGIN). For each clicked item in user behavior sequences, we introduce the\ntriangles in its neighborhood of the item-item graphs as a supplement. TGIN\nregards these triangles as the basic units of user interests, which provide the\nclues to capture the real motivation for a user clicking an item. We\ncharacterize every click behavior by aggregating the information of several\ninterest units to alleviate the elusive motivation problem. The attention\nmechanism determines users' preference for different interest units. By\nselecting diverse and relative triangles, TGIN brings in novel and\nserendipitous items to expand exploration opportunities of user interests.\nThen, we aggregate the multi-level interests of historical behavior sequences\nto improve CTR prediction. Extensive experiments on both public and industrial\ndatasets clearly verify the effectiveness of our framework.", "entities_include_in_text": [], "entities_from_reference": ["Andrei Broder", "Michael Mitzenmacher", "Network", "Chen", "Guoxin Zhang", "Fast", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton", "Levent Koc", "Jeremiah Harmsen", "Tushar Chandra", "Hrishi Aradhye", "Glen Anderson", "Greg Corrado", "Wei Chai", "Mustafa Ispir", "Virtual Event", "Fernando Diaz", "Torsten Suel", "Pablo Castells", "Rosie Jones", "Tetsuya Sakai", "Paul Covington", "Jay Adams", "Emre Sargin", "Deng", "Junwei Pan", "Tian Zhou", "Deguang Kong", "Aaron Flores", "Guang Lin", "Data Mining", "Liane Lewin-Eytan", "David Carmel", "Elad Yom-Tov", "Eugene Agichtein", "Evgeniy Gabrilovich", "Jacob Devlin", "Kenton Lee", "Kristina Toutanova", "Hongliang Fei", "Jingyuan Zhang", "Xingxuan Zhou", "Junhao Zhao", "Xinyang Qi", "Feature Interaction Learning", "Binbin Hu", "Fuyu Lv", "Qingwen Liu", "Zhiqiang Zhang", "Wenwu Ou", "Effective Recommendation", "Fei Sun", "Kun Kuang", "Yang Liu", "Multiplex", "Knowledge Management", "Weichen Shen", "Menghan Wang", "Yu Zhu", "Huifeng Guo", "Ruiming Tang", "Ye", "Zhenguo Li", "Jianqiang Huang", "Ke Hu", "Qingtao Tang", "Mingjian Chen", "Yi Qi", "Jia Cheng", "Jun Lei", "Deep Position-wise", "Gary L Miller", "Richard Peng", "Charalampos E Tsourakakis", "Daniel Huttenlocher", "Jon Kleinberg", "Feng Li", "Zhenrui Chen", "Pengjie Wang", "Yi Ren", "Di Zhang", "Xiaoyu Zhu", "Graph Intention Network", "Huichuan Duan", "Yuanjie Zheng", "Qianqian Wang", "Yu Wang", "Appl", "Taiwei Jin", "Changlong Yu", "Quan Lin", "Keping Yang", "Wilfred Ng", "Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton Van Den Hengel", "Mark EJ Newman", "Duncan J Watts", "Steven H Strogatz", "Random", "Weijie Bian", "Guorui Zhou", "Xiaoqiang Zhu", "Kun Gai", "Yanru Qu", "Han Cai", "Kan Ren", "Weinan Zhang", "Yong Yu", "Wen", "Jun Wang", "Yang Song", "Ali Mamdouh Elkahky", "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Kaiser", "Illia Polosukhin", "Hongwei Wang", "Fuzheng Zhang", "Jialin Wang", "Miao Zhao", "Wenjie Li", "Xie", "Minyi Guo", "Wang", "Pipei Huang", "Huan Zhao", "Zhibo Zhang", "Binqiang Zhao", "Dik Lun Lee", "Xiang Wang", "Yixin Cao", "Meng Liu", "Stanley Wasserman", "Katherine Faust", "Social", "Andreas Wimmer", "Kevin Lewis", "Beyond", "Luwei Yang", "Wen Jiang", "Yi Wei", "Yi Hu", "Hao Wang", "Feng Yu", "Qiang Liu", "Shu Wu", "Liang Wang", "Tieniu Tan", "Zhang", "Hao Qian", "Qing Cui", "Qi Liu", "Longfei Li", "Jun Zhou", "Jianhui Ma", "Enhong Chen", "Feature Learning", "Chang Zhou", "Jinze Bai", "Junshuai Song", "Xiaofei Liu", "Zhengchao Zhao", "Xiusi Chen", "Jun Gao", "Atrank", "Vol", "Na Mou", "Fan", "Qi Pi", "Chenru Song", "Han Zhu", "Xiao Ma", "Yanghui Yan", "Junqi Jin", "Han Li"]}{"title": ["TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network"], "authors": ["[arxiv.Result.Author('Xiaomin Li'), arxiv.Result.Author('Vangelis Metsis'), arxiv.Result.Author('Huangyingrui Wang'), arxiv.Result.Author('Anne Hee Hiong Ngu')]"], "link": ["http://arxiv.org/pdf/2202.02691v1"], "summary": "Signal measurements appearing in the form of time series are one of the most\ncommon types of data used in medical machine learning applications. However,\nsuch datasets are often small, making the training of deep neural network\narchitectures ineffective. For time-series, the suite of data augmentation\ntricks we can use to expand the size of the dataset is limited by the need to\nmaintain the basic properties of the signal. Data generated by a Generative\nAdversarial Network (GAN) can be utilized as another data augmentation tool.\nRNN-based GANs suffer from the fact that they cannot effectively model long\nsequences of data points with irregular temporal relations. To tackle these\nproblems, we introduce TTS-GAN, a transformer-based GAN which can successfully\ngenerate realistic synthetic time-series data sequences of arbitrary length,\nsimilar to the real ones. Both the generator and discriminator networks of the\nGAN model are built using a pure transformer encoder architecture. We use\nvisualizations and dimensionality reduction techniques to demonstrate the\nsimilarity of real and generated time-series data. We also compare the quality\nof our generated data with the best existing alternative, which is an RNN-based\ntime-series GAN.", "entities_include_in_text": [], "entities_from_reference": ["Bousmalis", "Silberman", "Dohan", "Erhan", "Krishnan", "Bousseljot", "Kreiseler", "Schnabel", "Nutzung", "Brophy", "Wang", "Ward", "Devlin", "Chang", "Lee", "Toutanova", "Bert", "Diao", "Shum", "Zhang", "Dosovitskiy", "Kolesnikov", "Zhai", "Dehghani", "Minderer", "Heigold", "Gelly", "Esteban", "Goldberger", "Glass", "Hausdorff", "Ivanov", "Mark", "Moody", "Peng", "Stanley", "Physiobank", "Goodfellow", "Mirza", "Ozair", "Bengio", "Huang", "Li", "Global", "Jiang", "Ledig", "Huszar", "Caballero", "Aitken", "Tejani", "Totz", "Van", "Maaten", "Mao", "Xie", "Lau", "Paul Smolley", "Micucci", "Mobilio", "Liao", "Ratliff", "Annual Allerton Conference", "Vaswani", "Shazeer", "Uszkoreit", "Jones", "Gomez", "Polosukhin", "Wold", "Yoon", "Jarrett", "Schaar", "Feature", "Average Cosine Similarity", "Normal ECG", "Abnormal ECG", "Abnormal ECG Fig"]}{"title": ["A Graph Neural Network Framework for Grid-Based Simulation"], "authors": ["[arxiv.Result.Author('Haoyu Tang'), arxiv.Result.Author('Wennan Long')]"], "link": ["http://arxiv.org/pdf/2202.02652v1"], "summary": "Reservoir simulations are computationally expensive in the well control and\nwell placement optimization. Generally, numerous simulation runs (realizations)\nare needed in order to achieve the optimal well locations. In this paper, we\npropose a graph neural network (GNN) framework to build a surrogate\nfeed-forward model which replaces simulation runs to accelerate the\noptimization process. Our GNN framework includes an encoder, a process, and a\ndecoder which takes input from the processed graph data designed and generated\nfrom the simulation raw data. We train the GNN model with 6000 samples\n(equivalent to 40 well configurations) with each containing the previous step\nstate variable and the next step state variable. We test the GNN model with\nanother 6000 samples and after model tuning, both one-step prediction and\nrollout prediction achieve a close match with the simulation results. Our GNN\nframework shows great potential in the application of well-related subsurface\noptimization including oil and gas as well as carbon capture sequestration\n(CCS).", "entities_include_in_text": [], "entities_from_reference": ["Haoyu Tang", "Louis J Durlofsky", "Alvaro Sanchez-Gonzalez", "Jonathan Godwin", "Tobias Pfaff", "Rex Ying", "Jure Leskovec", "Peter Battaglia", "Machine Learning", "Nicolas Heess", "Jost Tobias Springenberg", "Josh Merel", "Martin Riedmiller", "Raia Hadsell", "Meire Fortunato", "Peter W Battaglia"]}{"title": ["Ethics, Rules of Engagement, and AI: Neural Narrative Mapping Using Large Transformer Language Models"], "authors": ["[arxiv.Result.Author('Philip Feldman'), arxiv.Result.Author('Aaron Dant'), arxiv.Result.Author('David Rosenbluth')]"], "link": ["http://arxiv.org/pdf/2202.02647v1"], "summary": "The problem of determining if a military unit has correctly understood an\norder and is properly executing on it is one that has bedeviled military\nplanners throughout history. The advent of advanced language models such as\nOpenAI's GPT-series offers new possibilities for addressing this problem. This\npaper presents a mechanism to harness the narrative output of large language\nmodels and produce diagrams or \"maps\" of the relationships that are latent in\nthe weights of such models as the GPT-3. The resulting \"Neural Narrative Maps\"\n(NNMs), are intended to provide insight into the organization of information,\nopinion, and belief in the model, which in turn provide means to understand\nintent and response in the context of physical distance. This paper discusses\nthe problem of mapping information spaces in general, and then presents a\nconcrete implementation of this concept in the context of OpenAI's GPT-3\nlanguage model for determining if a subordinate is following a commander's\nintent in a high-risk situation. The subordinate's locations within the NNM\nallow a novel capability to evaluate the intent of the subordinate with respect\nto the commander. We show that is is possible not only to determine if they are\nnearby in narrative space, but also how they are oriented, and what\n\"trajectory\" they are on. Our results show that our method is able to produce\nhigh-quality maps, and demonstrate new ways of evaluating intent more\ngenerally.", "entities_include_in_text": ["LREC 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training"], "authors": ["[arxiv.Result.Author('Shiwei Liu'), arxiv.Result.Author('Tianlong Chen'), arxiv.Result.Author('Xiaohan Chen'), arxiv.Result.Author('Li Shen'), arxiv.Result.Author('Decebal Constantin Mocanu'), arxiv.Result.Author('Zhangyang Wang'), arxiv.Result.Author('Mykola Pechenizkiy')]"], "link": ["http://arxiv.org/pdf/2202.02643v1"], "summary": "Random pruning is arguably the most naive way to attain sparsity in neural\nnetworks, but has been deemed uncompetitive by either post-training pruning or\nsparse training. In this paper, we focus on sparse training and highlight a\nperhaps counter-intuitive finding, that random pruning at initialization can be\nquite powerful for the sparse training of modern neural networks. Without any\ndelicate pruning criteria or carefully pursued sparsity structures, we\nempirically demonstrate that sparsely training a randomly pruned network from\nscratch can match the performance of its dense equivalent. There are two key\nfactors that contribute to this revival: (i) the network sizes matter: as the\noriginal dense networks grow wider and deeper, the performance of training a\nrandomly pruned sparse network will quickly grow to matching that of its dense\nequivalent, even at high sparsity ratios; (ii) appropriate layer-wise sparsity\nratios can be pre-chosen for sparse training, which shows to be another\nimportant performance booster. Simple as it looks, a randomly pruned subnetwork\nof Wide ResNet-50 can be sparsely trained to outperforming a dense Wide\nResNet-50, on ImageNet. We also observed such randomly pruned networks\noutperform dense counterparts in other favorable aspects, such as\nout-of-distribution detection, uncertainty estimation, and adversarial\nrobustness. Overall, our results strongly suggest there is larger-than-expected\nroom for sparse training at scale, and the benefits of sparsity might be more\nuniversal beyond carefully designed pruning. Our source code can be found at\nhttps://github.com/VITA-Group/Random_Pruning.", "entities_include_in_text": ["Neyshabur et al., 2019; Novak et al., 2018; Allen-Zhu et al., 2019", "Dai et al., 2018", "Molchanov et al., 2016", "Sanh et al., 2020", "Lee\net al., 2019", "Mocanu et al., 2018; Lee et al., 2019; Gale et al., 2019; Wang et al., 2020; Tanaka et al.,\n2020", "Mocanu et al., 2016;\nGale et al., 2019; Lee et al., 2019; Wang et al., 2020", "Gale et al., 2019; Lee et al., 2019; Frankle et al., 2021; Tanaka et al.,\n2020", "Gale et al., 2019", "Mocanu et al., 2018", "Mocanu et al., 2018", "de Jorge et al., 2021; Verdenius et al., 2020). Su et al. (2020); Frankle et al. (2021", "Su et al., 2020", "Kusupati et al., 2020", "He et al., 2016", "Krizhevsky et al., 2009", "Deng et al., 2009", "Hendrycks et al., 2021", "Goodfellow et al., 2014", "Lakshminarayanan et al., 2016", "shown by Tsipras et al. (2019); Zhang et al. (2019", "Deng\net al., 2009", "Tessera et al., 2021", "Wang et al., 2020", "Frankle et al., 2020; Renda et al., 2020", "ICLR 2020", "Goodfellow et al., 2014", "Hendrycks\net al., 2021", "Lakshminarayanan et al., 2016", "Szegedy\net al., 2013; Papernot et al., 2016", "Guo et al., 2018; Ye et al., 2019; Gui et al., 2019; Hu et al.,\n2020). As the arguably most naive method of inducing sparsity, we are also interested in if training a\nrandomly pruned subnetwork can improve the adversarial robustness of deep networks. We follow\nthe classical method proposed in Goodfellow et al. (2014", "Netzer et al., 2011", "Hein et al., 2019", "Guo et al., 2017", "Guo et al., 2017", "Friedman\net al., 2001"], "entities_from_reference": ["Plusieurs"]}{"title": ["Graph Neural Network with Curriculum Learning for Imbalanced Node Classification"], "authors": ["[arxiv.Result.Author('Xiaohe Li'), arxiv.Result.Author('Lijie Wen'), arxiv.Result.Author('Yawen Deng'), arxiv.Result.Author('Fuli Feng'), arxiv.Result.Author('Xuming Hu'), arxiv.Result.Author('Lei Wang'), arxiv.Result.Author('Zide Fan')]"], "link": ["http://arxiv.org/pdf/2202.02529v1"], "summary": "Graph Neural Network (GNN) is an emerging technique for graph-based learning\ntasks such as node classification. In this work, we reveal the vulnerability of\nGNN to the imbalance of node labels. Traditional solutions for imbalanced\nclassification (e.g. resampling) are ineffective in node classification without\nconsidering the graph structure. Worse still, they may even bring overfitting\nor underfitting results due to lack of sufficient prior knowledge. To solve\nthese problems, we propose a novel graph neural network framework with\ncurriculum learning (GNN-CL) consisting of two modules. For one thing, we hope\nto acquire certain reliable interpolation nodes and edges through the novel\ngraph-based oversampling based on smoothness and homophily. For another, we\ncombine graph classification loss and metric learning loss which adjust the\ndistance between different nodes associated with minority class in feature\nspace. Inspired by curriculum learning, we dynamically adjust the weights of\ndifferent modules during training process to achieve better ability of\ngeneralization and discrimination. The proposed framework is evaluated via\nseveral widely used graph datasets, showing that our proposed model\nconsistently outperforms the existing state-of-the-art methods.", "entities_include_in_text": [], "entities_from_reference": ["Scott", "John", "Carrington", "Peter J", "Thomas N.", "Max Welling", "Hamilton", "Will", "Zhitao Ying", "Jure Leskovec", "Nathalie", "Shaju Stephen", "Gary M", "Robert C. Holte", "Workshop", "Vol", "Kai Ming", "Machine Learning", "Nitesh V.", "Jianxin Wu", "Man", "Part B", "Shaogang Gong", "Xiatian Zhu", "Micha", "Xavier Bresson", "Pierre Vandergheynst", "Joan", "Petar", "Graph", "Hui", "Wang", "Springer", "Berlin", "Heidelberg", "Yifan", "Zhao", "Tianxiang", "Xiang Zhang", "Suhang Wang", "Imbalanced Node", "Graph Neural Networks", "Data Mining", "Yiru", "Dynamic", "Galileo Namata", "Bilgic", "Model Performance", "True Positive", "Cora", "Amazon", "Coauthor CS", "Adam", "Sage TP", "Citeeer Class Id", "Amazon Comp", "Dataset Cora Citeseer"]}{"title": ["A Coalition Formation Game Approach for Personalized Federated Learning"], "authors": ["[arxiv.Result.Author('Leijie Wu')]"], "link": ["http://arxiv.org/pdf/2202.02502v1"], "summary": "Facing the challenge of statistical diversity in client local data\ndistribution, personalized federated learning (PFL) has become a growing\nresearch hotspot. Although the state-of-the-art methods with model\nsimilarity-based pairwise collaboration have achieved promising performance,\nthey neglect the fact that model aggregation is essentially a collaboration\nprocess within the coalition, where the complex multiwise influences take place\namong clients. In this paper, we first apply Shapley value (SV) from coalition\ngame theory into the PFL scenario. To measure the multiwise collaboration among\na group of clients on the personalized learning performance, SV takes their\nmarginal contribution to the final result as a metric. We propose a novel\npersonalized algorithm: pFedSV, which can 1. identify each client's optimal\ncollaborator coalition and 2. perform personalized model aggregation based on\nSV. Extensive experiments on various datasets (MNIST, Fashion-MNIST, and\nCIFAR-10) are conducted with different Non-IID data settings (Pathological and\nDirichlet). The results show that pFedSV can achieve superior personalized\naccuracy for each client, compared to the state-of-the-art benchmarks.", "entities_include_in_text": ["McMahan et al., 2017", "Kairouz et al., 2019", "Cortes and Mohri, 2014; Mansour et al., 2020;\nWang et al., 2019", "Donahue and Klein-\nberg, 2021", "Myerson, 2013", "Mansour et al., 2020; Wang et al., 2019", "T Dinh et\nal., 2020", "Li et al., 2020", "Jiang et al., 2019", "Shamsian et al., 2021", "Huang et al., 2021", "Kairouz et al., 2019;\nZhao et al., 2018", "Zhao et al., 2018; Li et al., 2020;\nLi et al., 2019; Karimireddy et al., 2020", "Mann and\nShapley, 1962; Castro et al., 2009; Maleki et al., 2013", "Maleki et al., 2013", "T Dinh et al., 2020", "Shamsian et al., 2021", "Huang et al., 2021", "McMahan et al., 2017", "Li et al.,\n\n2020", "LeCun, 1998", "Xiao et al., 2017", "Krizhevsky et al., 2009", "Collins et al., 2021", "Abadi et al.,\n2016", "Abadi et al., 2016", "Castro et al., 2009", "Collins et al., 2021", "Cortes and Mohri, 2014", "Donahue and Kleinberg, 2021", "Huang et al., 2021", "Jiang et al., 2019", "Kairouz et al., 2019", "Karimireddy et al., 2020", "Krizhevsky et al., 2009", "LeCun, 1998", "Li et al., 2019", "Li et al., 2020", "Maleki et al., 2013", "Mann and Shapley, 1962", "Mansour et al., 2020", "McMahan et al., 2017", "Myerson, 2013", "Shamsian et al., 2021", "T Dinh et al., 2020", "Wang et al., 2019", "Xiao et al., 2017", "Zhao et al., 2018"], "entities_from_reference": ["Martin Abadi", "Andy Chu", "Ian Goodfellow", "H Brendan McMahan", "Ilya Mironov", "Kunal Talwar", "Li Zhang", "Javier Castro", "Daniel G", "Juan Polynomial", "Tejada", "Collins", "Liam Collins", "Hamed Hassani", "Aryan Mokhtari", "Sanjay Shakkottai", "Mohri", "Corinna Cortes", "Mehryar Mohri", "Domain", "Donahue", "Kleinberg", "Jon Kleinberg", "Huang", "Yutao Huang", "Lingyang Chu", "Zirui Zhou", "Lanjun Wang", "Jiangchuan Liu", "Jian Pei", "Yong Zhang", "Jiang", "Yihan Jiang", "Jakub Konecn", "Keith Rush", "Sreeram Kannan", "Kairouz", "Peter Kairouz", "Brendan Avent", "Aur", "Mehdi Bennis", "Arjun Nitin Bhagoji", "Kallista Bonawitz", "Zachary Charles", "Graham Cormode", "Rachel Cummings", "Karimireddy", "Satyen Kale", "Sashank Reddi", "Ananda Theertha Suresh", "Stochastic", "Machine Learning", "Praneeth", "Alex Krizhevsky", "Geoffrey Hinton", "Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang", "Tian Li", "Anit Kumar Sahu", "Manzil Zaheer", "Maziar Sanjabi", "Ameet Talwalkar", "Systems", "Maleki", "Sasan Maleki", "Greg Hines", "Talal Rahwan", "Alex Rogers", "Mann", "Shapley", "Irwin Mann", "Lloyd S Shapley", "Yishay Mansour", "Jae Ro", "Brendan McMahan", "Eider Moore", "Daniel Ramage", "Seth Hampson", "Blaise Aguera", "Arcas", "Myerson", "Roger B Myerson", "Aviv Shamsian", "Aviv Navon", "Ethan Fetaya", "Gal Chechik", "Personalized", "Canh T Dinh", "Nguyen Tran", "Tuan Dung Nguyen", "Wang", "Kangkang Wang", "Rajiv Mathews", "Chlo", "Kiddon", "Hubert Eichner", "Beaufays", "Xiao", "Han Xiao", "Kashif Rasul", "Zhang", "Michael Zhang", "Karan Sapra", "Sanja Fidler", "Serena Yeung", "Jose M Alvarez", "Xinwei Zhang", "Mingyi Hong", "Sairaj Dhople", "Wotao Yin", "Yang Liu", "Fedpd", "Zhao", "Yue Zhao", "Meng Li", "Liangzhen Lai", "Naveen Suda", "Damon Civin", "Vikas Chandra"]}{"title": ["Intent Contrastive Learning for Sequential Recommendation"], "authors": ["[arxiv.Result.Author('Yongjun Chen'), arxiv.Result.Author('Zhiwei Liu'), arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Julian McAuley'), arxiv.Result.Author('Caiming Xiong')]"], "link": ["http://arxiv.org/pdf/2202.02519v1"], "summary": "Users' interactions with items are driven by various intents (e.g., preparing\nfor holiday gifts, shopping for fishing equipment, etc.).However, users'\nunderlying intents are often unobserved/latent, making it challenging to\nleverage such latent intents forSequentialrecommendation(SR). To investigate\nthe benefits of latent intents and leverage them effectively for\nrecommendation, we proposeIntentContrastiveLearning(ICL), a general learning\nparadigm that leverages a latent intent variable into SR. The core idea is to\nlearn users' intent distribution functions from unlabeled user behavior\nsequences and optimize SR models with contrastive self-supervised learning\n(SSL) by considering the learned intents to improve recommendation.\nSpecifically, we introduce a latent variable to represent users' intents and\nlearn the distribution function of the latent variable via clustering. We\npropose to leverage the learned intents into SR models via contrastive SSL,\nwhich maximizes the agreement between a view of sequence and its corresponding\nintent. The training is alternated between intent representation learning and\nthe SR model optimization steps within the generalized expectation-maximization\n(EM) framework. Fusing user intent information into SR also improves model\nrobustness. Experiments conducted on four real-world datasets demonstrate the\nsuperiority of the proposed learning paradigm, which improves performance, and\nrobustness against data sparsity and noisy interaction issues.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["LyaNet: A Lyapunov Framework for Training Neural ODEs"], "authors": ["[arxiv.Result.Author('Ivan Dario Jimenez Rodriguez'), arxiv.Result.Author('Aaron D. Ames'), arxiv.Result.Author('Yisong Yue')]"], "link": ["http://arxiv.org/pdf/2202.02526v1"], "summary": "We propose a method for training ordinary differential equations by using a\ncontrol-theoretic Lyapunov condition for stability. Our approach, called\nLyaNet, is based on a novel Lyapunov loss formulation that encourages the\ninference dynamics to converge quickly to the correct prediction.\nTheoretically, we show that minimizing Lyapunov loss guarantees exponential\nconvergence to the correct solution and enables a novel robustness guarantee.\nWe also provide practical algorithms, including one that avoids the cost of\nbackpropagating through a solver or using the adjoint method. Relative to\nstandard Neural ODE training, we empirically find that LyaNet can offer\nimproved prediction performance, faster convergence of inference dynamics, and\nimproved adversarial robustness. Our code available at\nhttps://github.com/ivandariojr/LyapunovLearning .", "entities_include_in_text": ["Chen et al., 2019", "Chen et al., 2019; Rozen et al.,\n2021; Song et al., 2020", "Chen et al., 2019; Kidger et al., 2021", "Khalil, 2002; Ames et al., 2014", "Dupont et al., 2019", "Massaroli et al., 2020", "Chen et al., 2019; E, 2017", "Taylor et al., 2019", "Taylor et al., 2019)", "Ames et al. (2014))", "Ames et al., 2014", "Queiruga et al., 2020", "He et al., 2015", "Zhang et al., 2009", "Chen et al., 2019; Antonelo et al., 2021). The\nproposal by E (2017", "Ames et al., 2016", "Liu et al., 2021"], "entities_from_reference": ["Grizzle", "J. W. Rapidly", "J. W.", "Amos", "Rodriguez", "Sacks", "Boots", "Kolter", "Antonelo", "Seman", "J. P.", "Hubner", "J. F.", "Bai", "Koltun", "Dean", "Matni", "Recht", "Dupont", "Doucet", "Teh", "Inverse Problems", "Zhang", "Ren", "Deep", "Khalil", "Patience Hall", "Chang", "Roohi", "Gao", "Kidger", "Chen", "Rubanova", "Duvenaud", "Nickel", "Fazlyab", "Pappas", "Cheng", "Orosz", "Chaudhuri", "Yue", "Burdick", "Control", "Machine Learning", "Chow", "Cohen", "Rosenfeld", "Kim", "Liu", "Bernstein", "Meister", "Li", "Beyond", "Manek", "Massaroli", "Poli", "Yamashita", "Asama", "Muller", "Peng", "Lee", "Queiruga", "Erichson", "Mahoney", "Raghunathan", "Steinhardt", "Liang", "Wilson", "Wong", "Vitus", "Automatica", "Richards", "Berkenkamp", "Robot Learning", "Robey", "Chamon", "Hassani", "Rosolia", "Borrelli", "Rozen", "Lipman", "Ruthotto", "Schlaginhaufen", "Wenk", "Dorfler", "Song", "Kumar", "Ermon", "Poole", "Sontag", "Wang", "Systems", "Taylor", "Dorobantu", "Krishnamoorthy", "Le", "Tsukamoto", "Chung", "Slotine", "Williams", "Wagener", "Drews", "Rehg", "J. M.", "Proofs", "Vy Vy", "Global Uniform Lipschitz", "Lyapunov Exponential Stability", "Correct Classification", "Monte Carlo Method", "Sample", "Titan RTX"]}{"title": ["Transfer Reinforcement Learning for Differing Action Spaces via Q-Network Representations"], "authors": ["[arxiv.Result.Author('Nathan Beck'), arxiv.Result.Author('Abhiramon Rajasekharan'), arxiv.Result.Author('Trung Hieu Tran')]"], "link": ["http://arxiv.org/pdf/2202.02442v1"], "summary": "Transfer learning approaches in reinforcement learning aim to assist agents\nin learning their target domains by leveraging the knowledge learned from other\nagents that have been trained on similar source domains. For example, recent\nresearch focus within this space has been placed on knowledge transfer between\ntasks that have different transition dynamics and reward functions; however,\nlittle focus has been placed on knowledge transfer between tasks that have\ndifferent action spaces. In this paper, we approach the task of transfer\nlearning between domains that differ in action spaces. We present a reward\nshaping method based on source embedding similarity that is applicable to\ndomains with both discrete and continuous action spaces. The efficacy of our\napproach is evaluated on transfer to restricted action spaces in the Acrobot-v1\nand Pendulum-v0 domains (Brockman et al. 2016). A comparison with two baselines\nshows that our method does not outperform these baselines in these continuous\naction spaces but does show an improvement in these discrete action spaces. We\nconclude our analysis with future directions for this work.", "entities_include_in_text": ["Brockman et al. 2016", "Zhu, Lin,\nand Zhou 2020", "Zhu, Lin, and Zhou 2020", "Zhang, Satija, and Pineau 2018", "Petangoda et al. 2019", "Brockman et al. 2016", "Barreto et al. 2016", "Petangoda et al. 2019", "Zhang, Satija, and\nPineau 2018", "Petangoda et al.\n2019", "Zhang, Satija, and Pineau 2018", "Ng, Harada, and\nRussell 1999", "Wiewiora, Cottrell, and\nElkan 2003", "Ng,\nHarada, and Russell 1999", "Brys et al.\n2015", "Brys\net al. 2015", "Wiewiora, Cottrell, and Elkan 2003", "Zhu, Lin, and\nZhou 2020", "Konda and Tsitsiklis 1999", "Zhu, Lin, and Zhou\n2020; Ng, Harada, and Russell 1999", "Wiewiora, Cot-\ntrell, and Elkan 2003", "Brockman et al. 2016", "Geramifard et al. 2015", "Geramifard et al. 2015", "Konda and Tsitsik-\nlis 1999", "Wiewiora, Cottrell, and\nElkan 2003", "Zhu, Lin, and Zhou 2020", "Lillicrap et al. 2015", "Fuji-\nmoto, Hoof, and Meger 2018", "Mnih et al. 2013", "Brockman et al.\n2016", "Brockman\net al. 2016; Geramifard et al. 2015", "Brockman et al. 2016", "Lillicrap et al. 2015", "Fujimoto, Hoof,\nand Meger 2018", "Mnih et al. 2013", "Mnih et al. 2013", "Lil-\nlicrap et al. 2015", "Fujimoto, Hoof, and Meger\n2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["Communication Efficient Federated Learning via Ordered ADMM in a Fully Decentralized Setting"], "authors": ["[arxiv.Result.Author('Yicheng Chen'), arxiv.Result.Author('Rick S. Blum'), arxiv.Result.Author('Brian M. Sadler')]"], "link": ["http://arxiv.org/pdf/2202.02580v1"], "summary": "The challenge of communication-efficient distributed optimization has\nattracted attention in recent years. In this paper, a communication efficient\nalgorithm, called ordering-based alternating direction method of multipliers\n(OADMM) is devised in a general fully decentralized network setting where a\nworker can only exchange messages with neighbors. Compared to the classical\nADMM, a key feature of OADMM is that transmissions are ordered among workers at\neach iteration such that a worker with the most informative data broadcasts its\nlocal variable to neighbors first, and neighbors who have not transmitted yet\ncan update their local variables based on that received transmission. In OADMM,\nwe prohibit workers from transmitting if their current local variables are not\nsufficiently different from their previously transmitted value. A variant of\nOADMM, called SOADMM, is proposed where transmissions are ordered but\ntransmissions are never stopped for each node at each iteration. Numerical\nresults demonstrate that given a targeted accuracy, OADMM can significantly\nreduce the number of communications compared to existing algorithms including\nADMM. We also show numerically that SOADMM can accelerate convergence,\nresulting in communication savings compared to the classical ADMM.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation"], "authors": ["[arxiv.Result.Author('Ziad Al-Halah'), arxiv.Result.Author('Santhosh K. Ramakrishnan'), arxiv.Result.Author('Kristen Grauman')]"], "link": ["http://arxiv.org/pdf/2202.02440v1"], "summary": "In reinforcement learning for visual navigation, it is common to develop a\nmodel for each new task, and train that model from scratch with task-specific\ninteractions in 3D environments. However, this process is expensive; massive\namounts of interactions are needed for the model to generalize well. Moreover,\nthis process is repeated whenever there is a change in the task type or the\ngoal modality. We present a unified approach to visual navigation using a novel\nmodular transfer learning model. Our model can effectively leverage its\nexperience from one source task and apply it to multiple target tasks (e.g.,\nObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,\naudio, label). Furthermore, our model enables zero-shot experience learning,\nwhereby it can solve the target tasks without receiving any task-specific\ninteractive training. Our experiments on multiple photorealistic datasets and\nchallenging tasks show that our approach learns faster, generalizes better, and\noutperforms SoTA models by a significant margin.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Unsupervised Learning on 3D Point Clouds by Clustering and Contrasting"], "authors": ["[arxiv.Result.Author('Guofeng Mei'), arxiv.Result.Author('Litao Yu'), arxiv.Result.Author('Qiang Wu'), arxiv.Result.Author('Jian Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02543v1"], "summary": "Learning from unlabeled or partially labeled data to alleviate human labeling\nremains a challenging research topic in 3D modeling. Along this line,\nunsupervised representation learning is a promising direction to auto-extract\nfeatures without human intervention. This paper proposes a general unsupervised\napproach, named \\textbf{ConClu}, to perform the learning of point-wise and\nglobal features by jointly leveraging point-level clustering and instance-level\ncontrasting. Specifically, for one thing, we design an Expectation-Maximization\n(EM) like soft clustering algorithm that provides local supervision to extract\ndiscriminating local features based on optimal transport. We show that this\ncriterion extends standard cross-entropy minimization to an optimal transport\nproblem, which we solve efficiently using a fast variant of the Sinkhorn-Knopp\nalgorithm. For another, we provide an instance-level contrasting method to\nlearn the global geometry, which is formulated by maximizing the similarity\nbetween two augmentations of one point cloud. Experimental evaluations on\ndownstream applications such as 3D object classification and semantic\nsegmentation demonstrate the effectiveness of our framework and show that it\ncan outperform state-of-the-art techniques.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Privacy-preserving Speech Emotion Recognition through Semi-Supervised Federated Learning"], "authors": ["[arxiv.Result.Author('Vasileios Tsouvalas'), arxiv.Result.Author('Tanir Ozcelebi'), arxiv.Result.Author('Nirvana Meratnia')]"], "link": ["http://arxiv.org/pdf/2202.02611v1"], "summary": "Speech Emotion Recognition (SER) refers to the recognition of human emotions\nfrom natural speech. If done accurately, it can offer a number of benefits in\nbuilding human-centered context-aware intelligent systems. Existing SER\napproaches are largely centralized, without considering users' privacy.\nFederated Learning (FL) is a distributed machine learning paradigm dealing with\ndecentralization of privacy-sensitive personal data. In this paper, we present\na privacy-preserving and data-efficient SER approach by utilizing the concept\nof FL. To the best of our knowledge, this is the first federated SER approach,\nwhich utilizes self-training learning in conjunction with federated learning to\nexploit both labeled and unlabeled on-device data. Our experimental evaluations\non the IEMOCAP dataset shows that our federated approach can learn\ngeneralizable SER models even under low availability of data labels and highly\nnon-i.i.d. distributions. We show that our approach with as few as 10% labeled\ndata, on average, can improve the recognition rate by 8.67% compared to the\nfully-supervised federated counterparts.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Neural Logic Analogy Learning"], "authors": ["[arxiv.Result.Author('Yujia Fan'), arxiv.Result.Author('Yongfeng Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02436v1"], "summary": "Letter-string analogy is an important analogy learning task which seems to be\neasy for humans but very challenging for machines. The main idea behind current\napproaches to solving letter-string analogies is to design heuristic rules for\nextracting analogy structures and constructing analogy mappings. However, one\nkey problem is that it is difficult to build a comprehensive and exhaustive set\nof analogy structures which can fully describe the subtlety of analogies. This\nproblem makes current approaches unable to handle complicated letter-string\nanalogy problems. In this paper, we propose Neural logic analogy learning\n(Noan), which is a dynamic neural architecture driven by differentiable logic\nreasoning to solve analogy problems. Each analogy problem is converted into\nlogical expressions consisting of logical variables and basic logical\noperations (AND, OR, and NOT). More specifically, Noan learns the logical\nvariables as vector embeddings and learns each logical operation as a neural\nmodule. In this way, the model builds computational graph integrating neural\nnetwork with logical reasoning to capture the internal logical structure of the\ninput letter strings. The analogy learning problem then becomes a True/False\nevaluation problem of the logical expressions. Experiments show that our\nmachine learning-based Noan approach outperforms state-of-the-art approaches on\nstandard letter-string analogy benchmark datasets.", "entities_include_in_text": ["NeurIPS 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["TorchMD-NET: Equivariant Transformers for Neural Network based Molecular Potentials"], "authors": ["[arxiv.Result.Author('Philipp Th\u00f6lke'), arxiv.Result.Author('Gianni De Fabritiis')]"], "link": ["http://arxiv.org/pdf/2202.02541v1"], "summary": "The prediction of quantum mechanical properties is historically plagued by a\ntrade-off between accuracy and speed. Machine learning potentials have\npreviously shown great success in this domain, reaching increasingly better\naccuracy while maintaining computational efficiency comparable with classical\nforce fields. In this work we propose TorchMD-NET, a novel equivariant\ntransformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1,\nand many QM9 targets in both accuracy and computational efficiency. Through an\nextensive attention weight analysis, we gain valuable insights into the black\nbox predictor and show differences in the learned representation of conformers\nversus conformations sampled from molecular dynamics or normal modes.\nFurthermore, we highlight the importance of datasets including off-equilibrium\nconformations for the evaluation of molecular potentials.", "entities_include_in_text": ["Pfau et al., 2020; Hermann et al., 2020", "Hermann et al., 2020", "Wang et al., 2019; Husic et al., 2020; Doerr et al., 2021", "Luong et al., 2015", "Vaswani\net al., 2017", "Ramakrishnan et al., 2014)", "Chmiela et al., 2017", "Ba et al., 2016", "Ramakrishnan et al., 2014", "Chmiela\net al., 2017", "Anderson et al., 2019", "Sator-\nras et al., 2021", "Hutchinson et al., 2020", "Hutchinson\net al., 2020", "Batzner\net al., 2021", "Lu et al., 2019", "Paszke et al., 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques"], "authors": ["[arxiv.Result.Author('Sreenivasa Hikkal Venugopala')]"], "link": ["http://arxiv.org/pdf/2202.02521v1"], "summary": "Estimating and understanding the surroundings of the vehicle precisely forms\nthe basic and crucial step for the autonomous vehicle. The perception system\nplays a significant role in providing an accurate interpretation of a vehicle's\nenvironment in real-time. Generally, the perception system involves various\nsubsystems such as localization, obstacle (static and dynamic) detection, and\navoidance, mapping systems, and others. For perceiving the environment, these\nvehicles will be equipped with various exteroceptive (both passive and active)\nsensors in particular cameras, Radars, LiDARs, and others. These systems are\nequipped with deep learning techniques that transform the huge amount of data\nfrom the sensors into semantic information on which the object detection and\nlocalization tasks are performed. For numerous driving tasks, to provide\naccurate results, the location and depth information of a particular object is\nnecessary. 3D object detection methods, by utilizing the additional pose data\nfrom the sensors such as LiDARs, stereo cameras, provides information on the\nsize and location of the object. Based on recent research, 3D object detection\nframeworks performing object detection and localization on LiDAR data and\nsensor fusion techniques show significant improvement in their performance. In\nthis work, a comparative study of the effect of using LiDAR data for object\ndetection frameworks and the performance improvement seen by using sensor\nfusion techniques are performed. Along with discussing various state-of-the-art\nmethods in both the cases, performing experimental analysis, and providing\nfuture research directions.", "entities_include_in_text": [], "entities_from_reference": ["Feng", "Rosenbaum", "Hertlein", "Timm", "Object Detection", "Methods", "Dianati", "Fallah", "Mouzakitis", "Mao", "Lang", "Vora", "Caesar", "Zhou", "Yang", "Fast Encoders", "Pattern Recognition", "Acharya", "Rafii", "Range Camera", "Rosique", "Navarro", "Fernandez", "Padilla", "Feng S. Sensor", "Castanedo", "Data Fusion Techniques", "Vasudevan S. Data", "Robotics Auton", "Basic Engineering", "Deng", "Socher", "Li", "Shape Recognition", "Zhang", "Xia", "Vehicle Detection", "Network", "Yue", "Keutzer", "Recurrent CRF", "Milz", "Gross", "Point Clouds", "Guindel", "Moreno", "Cruzado", "Garcia", "Dietmayer", "Towards Safe Autonomous Driving", "Rao", "Wang", "Posner", "Tuzel", "Point Cloud", "Pattern Object Detection", "Liu", "Jia", "Object Detector", "Shi", "Vazquez", "Lopez", "Amores", "Multicue", "Multimodal", "Multiview Random Forest", "Man", "Enzweiler", "Gavrila", "Pedestrian Classification", "Image Processing", "Lee", "Harakeh", "View Aggregation", "Liang", "Urtasun", "Deep Continuous Fusion", "Hegde", "Sensor Fusion", "Pattern Recognition Workshops", "Guibas", "Frustum", "Geiger", "Lenz", "Stiller", "Vision", "Robotics Research", "Gool", "Williams", "Zisserman"]}{"title": ["Science Facing Interoperability as a Necessary Condition of Success and Evil"], "authors": ["[arxiv.Result.Author('Remy Demichelis')]"], "link": ["http://arxiv.org/pdf/2202.02540v1"], "summary": "Artificial intelligence (AI) systems, such as machine learning algorithms,\nhave allowed scientists, marketers and governments to shed light on\ncorrelations that remained invisible until now. Beforehand, the dots that we\nhad to connect in order to imagine a new knowledge were either too numerous,\ntoo sparse or not even detected. Sometimes, the information was not stored in\nthe same data lake or format and was not able to communicate. But in creating\nnew bridges with AI, many problems appeared such as bias reproduction, unfair\ninferences or mass surveillance. Our aim is to show that, on one hand, the AI's\ndeep ethical problem lays essentially in these new connections made possible by\nsystems interoperability. In connecting the spheres of our life, these systems\nundermine the notion of justice particular to each of them, because the new\ninteractions create dominances of social goods from a sphere to another. These\nsystems make therefore spheres permeable to one another and, in doing so, they\nopen to progress as well as to tyranny. On another hand, however, we would like\nto emphasize that the act to connect what used to seem a priori disjoint is a\nnecessary move of knowledge and scientific progress.", "entities_include_in_text": ["Botsman,  2017", "Dastin, \n2018", "Angwin et al., 2016"], "entities_from_reference": ["Plusieurs"]}{"title": ["MarkovGNN: Graph Neural Networks on Markov Diffusion"], "authors": ["[arxiv.Result.Author('Md. Khaledur Rahman'), arxiv.Result.Author('Abhigya Agrawal'), arxiv.Result.Author('Ariful Azad')]"], "link": ["http://arxiv.org/pdf/2202.02470v1"], "summary": "Most real-world networks contain well-defined community structures where\nnodes are densely connected internally within communities. To learn from these\nnetworks, we develop MarkovGNN that captures the formation and evolution of\ncommunities directly in different convolutional layers. Unlike most Graph\nNeural Networks (GNNs) that consider a static graph at every layer, MarkovGNN\ngenerates different stochastic matrices using a Markov process and then uses\nthese community-capturing matrices in different layers. MarkovGNN is a general\napproach that could be used with most existing GNNs. We experimentally show\nthat MarkovGNN outperforms other GNNs for clustering, node classification, and\nvisualization tasks. The source code of MarkovGNN is publicly available at\n\\url{https://github.com/HipGraph/MarkovGNN}.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["HENRI: High Efficiency Negotiation-based Robust Interface for Multi-party Multi-issue Negotiation over the Internet"], "authors": ["[arxiv.Result.Author('Saurabh Deochake'), arxiv.Result.Author('Shashank Kanth'), arxiv.Result.Author('Subhadip Chakraborty'), arxiv.Result.Author('Suresh Sarode'), arxiv.Result.Author('Vidyasagar Potdar'), arxiv.Result.Author('Debajyoti Mukhopadhyay')]"], "link": ["http://arxiv.org/pdf/2202.02430v1"], "summary": "This paper proposes a framework for a full fledged negotiation system that\nallows multi party multi issue negotiation. It focuses on the negotiation\nprotocol to be observed and provides a platform for concurrent and independent\nnegotiation on individual issues using the concept of multi threading. It\ndepicts the architecture of an agent detailing its components. The paper sets\nforth a hierarchical pattern for the multiple issues concerning every party.\nThe system also provides enhancements such as the time-to-live counters for\nevery advertisement, refinement of utility considering non-functional\nattributes, prioritization of issues, by assigning weights to issues.", "entities_include_in_text": ["DEST 2009", "ICCSA \n2007"], "entities_from_reference": ["Faratin", "Group Decision", "Patrick Wendy Powley", "Web Service", "Nicholas R. Jennings", "Lau", "Electronic Commerce Research", "Rogier Brussee1", "Eck", "Alliances", "Jin Baek Kim", "Min Gi", "Dynamic", "Jamal Bentahar", "Philippe", "Data Engineering", "Privacy Negotiations", "Seventh", "User", "Istanbul", "Chang", "Revenue Models", "Current Generation Social Softwares Systems", "Kuala Lumpur", "Melbourne"]}{"title": ["Distributed Learning With Sparsified Gradient Differences"], "authors": ["[arxiv.Result.Author('Yicheng Chen'), arxiv.Result.Author('Rick S. Blum'), arxiv.Result.Author('Martin Takac'), arxiv.Result.Author('Brian M. Sadler')]"], "link": ["http://arxiv.org/pdf/2202.02491v1"], "summary": "A very large number of communications are typically required to solve\ndistributed learning tasks, and this critically limits scalability and\nconvergence speed in wireless communications applications. In this paper, we\ndevise a Gradient Descent method with Sparsification and Error Correction\n(GD-SEC) to improve the communications efficiency in a general worker-server\narchitecture. Motivated by a variety of wireless communications learning\nscenarios, GD-SEC reduces the number of bits per communication from worker to\nserver with no degradation in the order of the convergence rate. This enables\nlarger-scale model learning without sacrificing convergence or accuracy. At\neach iteration of GD-SEC, instead of directly transmitting the entire gradient\nvector, each worker computes the difference between its current gradient and a\nlinear combination of its previously transmitted gradients, and then transmits\nthe sparsified gradient difference to the server. A key feature of GD-SEC is\nthat any given component of the gradient difference vector will not be\ntransmitted if its magnitude is not sufficiently large. An error correction\ntechnique is used at each worker to compensate for the error resulting from\nsparsification. We prove that GD-SEC is guaranteed to converge for strongly\nconvex, convex, and nonconvex optimization problems with the same order of\nconvergence rate as GD. Furthermore, if the objective function is strongly\nconvex, GD-SEC has a fast linear convergence rate. Numerical results not only\nvalidate the convergence rate of GD-SEC but also explore the communication bit\nsavings it provides. Given a target accuracy, GD-SEC can significantly reduce\nthe communications load compared to the best existing algorithms without\nslowing down the optimization process.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["SMODICE: Versatile Offline Imitation Learning via State Occupancy Matching"], "authors": ["[arxiv.Result.Author('Yecheng Jason Ma'), arxiv.Result.Author('Andrew Shen'), arxiv.Result.Author('Dinesh Jayaraman'), arxiv.Result.Author('Osbert Bastani')]"], "link": ["http://arxiv.org/pdf/2202.02433v1"], "summary": "We propose State Matching Offline DIstribution Correction Estimation\n(SMODICE), a novel and versatile algorithm for offline imitation learning (IL)\nvia state-occupancy matching. We show that the SMODICE objective admits a\nsimple optimization procedure through an application of Fenchel duality and an\nanalytic solution in tabular MDPs. Without requiring access to expert actions,\nSMODICE can be effectively applied to three offline IL settings: (i) imitation\nfrom observations (IfO), (ii) IfO with dynamics or morphologically mismatched\nexpert, and (iii) example-based reinforcement learning, which we show can be\nformulated as a state-occupancy matching problem. We extensively evaluate\nSMODICE on both gridworld environments as well as on high-dimensional offline\nbenchmarks. Our results demonstrate that SMODICE is effective for all three\nproblem settings and significantly outperforms prior state-of-art.", "entities_include_in_text": ["Lange\net al., 2012; Levine et al., 2020", "Zolna et al., 2020; Chang et al., 2021; Anony-\nmous, 2022", "Ross et al., 2011", "Eysenbach et al., 2021", "Torabi et al.,\n2018; 2019; Liu et al., 2019; Radosavovic et al., 2020; Ey-\nsenbach et al., 2021", "Ey-\nsenbach et al., 2021", "Kumar et al., 2019; Lee et al., 2021;\nAnonymous, 2022", "Levine et al., 2020", "Puterman,\n2014", "Dai et al., 2016", "Zhu et al., 2020; Lee et al., 2021; Rudner et al., 2021", "Puterman, 2014", "Lee et al.,\n2021", "Lee et al.,\n2021; Anonymous, 2022", "Eysenbach et al., 2021", "Anonymous, 2022", "Ghasemipour et al., 2019;\nKe et al., 2020; Zhu et al., 2020", "Zolna et al., 2020", "Chang et al.,\n2021", "Torabi et al., 2018;\n2019; Liu et al., 2019; Radosavovic et al., 2020", "Kim et al., 2020; Raychaudhuri et al., 2021", "Liu et al., 2019; Radosavovic\net al., 2020", "Fu et al., 2021", "Liu et al., 2019", "Zolna et al.,\n2020", "Anonymous, 2022", "Haarnoja et al., 2018", "Eysenbach et al., 2021", "Zolna et al., 2020", "Yu et al., 2020; Kidambi et al., 2020", "Yang et al., 2019", "Kostrikov et al., 2020; Zhu et al., 2020", "Anonymous, 2022", "Boyd et al., 2004", "Harris et al., 2020", "Baird, 1995", "Haarnoja et al., 2018"], "entities_from_reference": ["Baird", "Elsevier", "Boyd", "Cambridge", "Chang", "J. D.", "Sreenivas", "Kidambi", "Dai", "Boots", "Chow", "Li", "Eysenbach", "Salakhutdinov", "Kumar", "Fujimoto", "Zemel", "Goodfellow", "Mirza", "Ozair", "Bengio", "Gupta", "Hausman", "Haarnoja", "Zhou", "Mismatched Experts", "Examples Harris", "Millman", "Walt", "Berg", "Smith", "Picus", "Kerkwijk", "Brett", "Haldane", "J. F.", "Wiebe", "Peterson", "Sheppard", "Reddy", "Weckesser", "Nature", "Ermon", "Choudhury", "Barnes", "Lee", "Netrapalli", "Kim", "Zhao", "Machine Learning", "Kingma", "J. Adam", "Kostrikov", "Agrawal", "Dwibedi", "Tompson", "J.", "J. ImitaIn", "Lange", "Gabel", "Riedmiller", "Springer", "Jeon", "Pineau", "Levine", "J. Offline", "Liu", "Jayaraman", "Bastani", "Nachum", "Puterman", "Markov", "John Wiley", "Sons", "Radosavovic", "Wang", "J. Stateonly", "Raychaudhuri", "Paul", "Baar", "Ross", "Gordon", "Rudner", "Osborne", "Gal", "Teh", "Dauphin", "Liang", "Vaughan", "J. W", "Torabi", "Warnell", "Stone", "Examples Yang", "Huang", "Gan", "Thomas", "Zou", "Finn", "Zhang", "Zhu", "Lin", "Zolna", "Novikov", "Aytar", "Denil", "Proofs", "Technical Lemmas", "Examples", "Esd", "Es0", "Offline Imitation", "Fenchel", "Extended Related Work", "Fenchel Duality", "Hence", "T V", "Tabular MDPs", "V V", "Harris", "Offline IL", "Deep Neural Networks", "Remark", "Hyperparameter Value Critic", "Discount", "Actor Mean Clipping", "Appendix G", "Baselines", "Implementation Details", "Observations Experimental Details", "Ant", "Examples Figure", "Diverse AntMaze", "Zero Reward", "Expert Experimental Details", "Examples Table", "Target", "Adam", "Critic", "Offline Dataset Compositions", "Expert Data Size Random Data Size Task Hopper", "Kitchen State Dim", "Expert Dataset", "Examples Experimental Details", "Task", "Ant Algorithm", "Offline", "Microwave Figure", "Kettle", "Microwave", "Examples Algorithm", "V V V", "Expert", "Train Expert", "Train Lagrangian Value", "Policy", "Sample", "Behavior Cloning Update"]}{"title": ["Handling Distribution Shifts on Graphs: An Invariance Perspective"], "authors": ["[arxiv.Result.Author('Qitian Wu'), arxiv.Result.Author('Hengrui Zhang'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('David Wipf')]"], "link": ["http://arxiv.org/pdf/2202.02466v1"], "summary": "There is increasing evidence suggesting neural networks' sensitivity to\ndistribution shifts, so that research on out-of-distribution (OOD)\ngeneralization comes into the spotlight. Nonetheless, current endeavors mostly\nfocus on Euclidean data, and its formulation for graph-structured data is not\nclear and remains under-explored, given the two-fold fundamental challenges: 1)\nthe inter-connection among nodes in one graph, which induces non-IID generation\nof data points even under the same environment, and 2) the structural\ninformation in the input graph, which is also informative for prediction. In\nthis paper, we formulate the OOD problem for node-level prediction on graphs\nand develop a new domain-invariant learning approach, named\nExplore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage\ninvariant graph features for prediction. The key difference to existing\ninvariant models is that we design multiple context explorers (specified as\ngraph editers in our case) that are adversarially trained to maximize the\nvariance of risks from multiple virtual environments. Such a design enables the\nmodel to extrapolate from a single observed environment which is the common\ncase for node-level prediction. We prove the validity of our method by\ntheoretically showing its guarantee of a valid OOD solution and further\ndemonstrate its power on various real-world datasets for handling distribution\nshifts from artificial spurious features, cross-domain transfers and dynamic\ngraph evolution.", "entities_include_in_text": ["Mansour et al., 2009; Blanchard et al., 2011; Muandet et al., 2013;\nGong et al., 2016) occupies a central role in the ML community. Yet, recent evidence suggests that\ndeep neural networks can be sensitive to distribution shifts, exhibiting unsatisfactory performance\nwithin new environments, e.g., Beery et al. (2018); Su et al. (2019); Recht et al. (2019); Mancini\net al. (2020", "DeGrave et al., 2020).\n\nRecent studies of the OOD generalization problem like Rojas-Carulla et al. (2018", "Arjovsky et al., 2019", "Fakhraei et al., 2015", "Pareja et al., 2020", "AlBadawy et al., 2018", "Berk et al., 2018", "Rojas-Carulla et al., 2018; Arjovsky et al., 2019", "Krueger et al., 2021", "Xu et al., 2019; Jin et al., 2020", "Federici et al.,\n2021", "Wu\net al., 2019", "Velickovic et al., 2018", "Pareja et al., 2020", "Hamilton et al., 2017", "Chien\net al., 2021", "Pareja et al., 2020", "Xu et al., 2021", "Yehudai et al., 2021;\nBevilacqua et al., 2021", "Koh et al., 2021", "Arjovsky et al., 2019", "Sagawa et al., 2019", "Chang et al., 2020", "Ahuja et al.,\n2020", "Zhang et al., 2021), etc. Several works attempt to resolve extended\nsettings. For instance, Ahmed et al. (2021", "Mahajan et al., 2021) also leverages\na matching-based algorithm that resorts to shared representations of cross-domain inputs from the\nsame object. Also, Creager et al. (2021) and Liu et al. (2021", "Rosenfeld et al., 2021; Nagarajan et al., 2021; Kamath et al.,\n2021", "Federici et al., 2021", "Chen et al., 2021", "Zheng et al., 2020; Hasanzadeh et al., 2020", "Baranwal et al., 2021", "Federici\net al., 2021", "within 1950-2011", "within 2011-2014"], "entities_from_reference": ["Plusieurs"]}{"title": ["Symmetric Volume Maps"], "authors": ["[arxiv.Result.Author('S. Mazdak Abulnaga'), arxiv.Result.Author('Oded Stein'), arxiv.Result.Author('Polina Golland'), arxiv.Result.Author('Justin Solomon')]"], "link": ["http://arxiv.org/pdf/2202.02568v1"], "summary": "Although shape correspondence is a central problem in geometry processing,\nmost methods for this task apply only to two-dimensional surfaces. The\nneglected task of volumetric correspondence--a natural extension relevant to\nshapes extracted from simulation, medical imaging, volume rendering, and even\nimproving surface maps of boundary representations--presents unique challenges\nthat do not appear in the two-dimensional case. In this work, we propose a\nmethod for mapping between volumes represented as tetrahedral meshes. Our\nformulation minimizes a distortion energy designed to extract maps\nsymmetrically, i.e., without dependence on the ordering of the source and\ntarget domains. We accompany our method with theoretical discussion describing\nthe consequences of this symmetry assumption, leading us to select a\nsymmetrized ARAP energy that favors isometric correspondences. Our final\nformulation optimizes for near-isometry while matching the boundary. We\ndemonstrate our method on a diverse geometric dataset, producing low-distortion\nmatchings that align to the boundary.", "entities_include_in_text": ["Ovsjanikov et al. 2010", "Lipman and\nFunkhouser 2009; Schmidt et al. 2019", "Ezuz\net al. 2019", "Floater and Hormann 2005; Fu et al. 2021; Sheffer et al. 2007", "Irving et al. 2004", "Gain and\nBechmann 2008; Selim and Koomullil 2016; Sieger et al. 2015", "Smith and Schaefer 2015] for a representative\nexample. In contrast to these past works, we produce maps between\nfar-from-isometric domains without an obvious effective initializa-\ntion. Consequently, our choice of energies is designed to be resilient\nto poor initial maps that are not foldover-free.\n\nVolumetric mappings. Some methods consider the task of comput-\ning correspondences between volumetric shapes. To our knowledge,\nall past methods can be understood as special cases of the deforma-\ntion methods where the task is to extend a fixed boundary map to\nthe interior of a volume.\n\nKovalsky et al. [2015", "Ezuz et al. 2019; Schmidt et al. 2019;\nSchreiner et al. 2004", "Schreiner et al. 2004; Smith\nand Schaefer 2015", "Shtengel\net al. 2017", "Kraevoy and Sheffer 2004", "Gots-\nman et al. 2003; Haker et al. 2000; Lee and Kazhdan 2019", "Aigerman and Lipman 2015, 2016; Aigerman\net al. 2014, 2015; Bright et al. 2017; Schmidt et al. 2019", "Kim et al. 2011; Lipman and Funkhouser 2009", "Aigerman et al. 2015", "Jain\net al. 2007; Mateus et al. 2008; Ovsjanikov et al. 2010; Vestner et al.\n2017", "Ankerst et al. 1999; Salti et al. 2014", "Dubrovina and Kimmel 2011; Kim et al. 2011; Litman and\nBronstein 2013", "Ovsjanikov et al. 2012,\n2016", "Ezuz et al. 2019;\nMandad et al. 2017; Schreiner et al. 2004; Solomon et al. 2012, 2016].\nEzuz et al. [2019", "Klein\net al. 2007", "Avants et al. 2008", "Beg et al. 2005", "Oliveira\nand Tavares 2014; Sotiras et al. 2013; Viergever et al. 2016", "Rabinovich et al. 2017", "Liu et al. 2008", "Schreiner et al. 2004", "Ezuz et al. 2019; Schmidt et al. 2019; Schreiner et al.\n2004], one simple way to achieve symmetry is to optimize the av-\nerage of the distortion energy of a map with the distortion energy\nof its inverse. Ezuz et al. [2019] and Schreiner et al. [2004", "Rabinovich et al. 2017; Smith and Schaefer\n2015", "Ezuz et al. 2019; Schreiner et al. 2004", "Smith and Schaefer 2015", "Stein\net al. 2021", "Ezuz et al. 2019; Schmidt\net al. 2019; Schreiner et al. 2004", "Geman and Yang 1995", "Zhu et al. 1997", "Hu et al. 2020", "Li et al.\n2021", "Li\net al. 2021", "Dyke et al. 2019", "Fu et al. 2016; Li et al.\n2021", "Zhou and Jacobson 2016", "Japan 2022", "Ezuz et al. 2019", "Ezuz et al. 2019", "Palmer et al. 2020", "Abulnaga et al.\n2021", "Benirschke and Driscoll 1967", "Bracci et al. 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Transformers and the representation of biomedical background knowledge"], "authors": ["[arxiv.Result.Author('Oskar Wysocki'), arxiv.Result.Author('Zili Zhou'), arxiv.Result.Author(\"Paul O'Regan\"), arxiv.Result.Author('Deborah Ferreira'), arxiv.Result.Author('Magdalena Wysocka'), arxiv.Result.Author('D\u00f3nal Landers'), arxiv.Result.Author('Andr\u00e9 Freitas')]"], "link": ["http://arxiv.org/pdf/2202.02432v1"], "summary": "BioBERT and BioMegatron are Transformers models adapted for the biomedical\ndomain based on publicly available biomedical corpora. As such, they have the\npotential to encode large-scale biological knowledge. We investigate the\nencoding and representation of biological knowledge in these models, and its\npotential utility to support inference in cancer precision medicine - namely,\nthe interpretation of the clinical significance of genomic alterations. We\ncompare the performance of different transformer baselines; we use probing to\ndetermine the consistency of encodings for distinct entities; and we use\nclustering methods to compare and contrast the internal properties of the\nembeddings for genes, variants, drugs and diseases. We show that these models\ndo indeed encode biological knowledge, although some of this is lost in\nfine-tuning for specific tasks. Finally, we analyse how the models behave with\nregard to biases and imbalances in the dataset.", "entities_include_in_text": ["May 2021", "Aug. 2014", "Jan. 2017", "Aug. 2019", "Nov. 2016"], "entities_from_reference": ["Erica K. Barnell", "Florian Borchert", "Debyani Chakravarty", "Dvir Dahary", "Standard", "Genome Medicine", "Arpad M. Danos", "Human Mutation", "Rodrigo Dienstmann", "Cancer Drugs", "Clinical Targetability", "Deborah Ferreira", "Does My", "System Demonstrations", "Online", "Discriminatory Analysis", "Benjamin M Good", "Knowledge", "Enable Personalization", "Benjamin M. Good", "Malachi Griffith", "English", "Nature", "John Hewitt", "Christopher D Manning", "Human Language Technologies", "Short Papers", "Robin Jia", "Cliff Wong", "Hoifung Poon", "Multiscale", "Jinhyuk Lee", "Marilyn M. Li", "Molecular Pathology", "Clinical Oncology", "John Healy", "Density", "Data Mining Workshops", "Steve Astels", "Uniform Manifold Approximation", "Open Source Software", "Source Software", "Tiago Pimentel", "Damian T. Rieke", "Molecular Tumor Boards Worldwide", "Alexander Rives", "Jurica Seva", "Martin Wackerbauer", "Ulf Leser", "Key Sentences", "Melbourne", "Larger Biomedical Domain Language Model", "Ayush Singhal", "Michael Simmons", "Zhiyong Lu", "Text Mining", "Database Curation", "Alex H. Wagner", "Analysis Toolkit", "Hai Wang", "Deep Probabilistic Logic", "Similar", "Note", "Tables Table", "Pair", "Model Entity", "False", "Test", "Balanced", "G2032R", "Ruxolitinib", "Midostaurin", "Entrectinib", "Trametinib", "Sarcoma", "Larotrectinib", "Vemurafenib", "Sorafenib CD74-NRG1", "Afatinib", "Crizotinib Table", "Cluster", "H1047R", "G1049R", "H1047L ERBB3", "M2327I", "Melanoma Cancer Acute Myeloid Leukemia Colorectal Cancer Colorectal Cancer Clear Cell Sarcoma Vemurafenib", "Alectinib ALK", "G1202R Sorafenib", "Carcinoma Cetuximab", "Acute Myeloid Leukemia", "Carcinoma Erlotinib", "Selinexor Vemurafenib Regorafenib Gefitinib", "Figure S.1", "Figure S.2"]}{"title": ["Self-Adaptive Forecasting for Improved Deep Learning on Non-Stationary Time-Series"], "authors": ["[arxiv.Result.Author('Sercan O. Arik'), arxiv.Result.Author('Nathanael C. Yoder'), arxiv.Result.Author('Tomas Pfister')]"], "link": ["http://arxiv.org/pdf/2202.02403v1"], "summary": "Real-world time-series datasets often violate the assumptions of standard\nsupervised learning for forecasting -- their distributions evolve over time,\nrendering the conventional training and model selection procedures suboptimal.\nIn this paper, we propose a novel method, Self-Adaptive Forecasting (SAF), to\nmodify the training of time-series forecasting models to improve their\nperformance on forecasting tasks with such non-stationary time-series data. SAF\nintegrates a self-adaptation stage prior to forecasting based on `backcasting',\ni.e. predicting masked inputs backward in time. This is a form of test-time\ntraining that creates a self-supervised learning problem on test samples before\nperforming the prediction task. In this way, our method enables efficient\nadaptation of encoded representations to evolving distributions, leading to\nsuperior generalization. SAF can be integrated with any canonical\nencoder-decoder based time-series architecture such as recurrent neural\nnetworks or attention-based architectures. On synthetic and real-world datasets\nin domains where time-series data are known to be notoriously non-stationary,\nsuch as healthcare and finance, we demonstrate a significant benefit of SAF in\nimproving forecasting accuracy.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["OMLT: Optimization & Machine Learning Toolkit"], "authors": ["[arxiv.Result.Author('Francesco Ceccon'), arxiv.Result.Author('Jordan Jalving'), arxiv.Result.Author('Joshua Haddad'), arxiv.Result.Author('Alexander Thebelt'), arxiv.Result.Author('Calvin Tsay'), arxiv.Result.Author('Carl D. Laird'), arxiv.Result.Author('Ruth Misener')]"], "link": ["http://arxiv.org/pdf/2202.02414v1"], "summary": "The optimization and machine learning toolkit (OMLT) is an open-source\nsoftware package incorporating neural network and gradient-boosted tree\nsurrogate models, which have been trained using machine learning, into larger\noptimization problems. We discuss the advances in optimization technology that\nmade OMLT possible and show how OMLT seamlessly integrates with the algebraic\nmodeling language Pyomo. We demonstrate how to use OMLT for solving\ndecision-making problems in both computer science and engineering.", "entities_include_in_text": ["Volpp et al., 2019", "Botoeva et al., 2020", "Boukouvala et al., 2016", "Chollet et al., 2015", "Bynum et al., 2021", "Chollet et al., 2015", "Paszke et al., 2019", "Abadi et al.,\n2015", "Schweidtmann and Mitsos,\n2019", "Lueg\net al., 2021", "Maragno et al., 2021", "Kronqvist et al., 2021; Tsay et al.,\n2021", "LeCun et al., 2010", "Tjeng et al., 2018", "Croce and Hein, 2020", "Serra et al., 2020", "Thebelt\n\net al., 2021", "Lee et al., 2021", "Boukouvala et al.,\n2016; Wilson and Sahinidis, 2017; Boukouvala and Floudas, 2017; Boukouvala et al., 2017;\nHuster et al., 2020", "Bynum\net al., 2021", "Gay, 1997", "Fischetti and Jo, 2018; Raghunathan et al., 2018; Singh et al., 2019; Anderson\net al., 2020; Tjandraatmadja et al., 2020; Dathathri et al., 2020", "Thebelt et al., 2021", "Schweidtmann and Mitsos, 2019", "Anderson et al., 2020", "Yang et al., 2021", "Tsay et al., 2021"], "entities_from_reference": ["Mart", "Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Software", "Ross Anderson", "Joey Huchette", "Will Ma", "Christian Tjandraatmadja", "Juan Pablo Vielma", "Strong", "Mathematical Programming", "Elena Botoeva", "Panagiotis Kouvaros", "Jan Kronqvist", "Alessio Lomuscio", "Fani Boukouvala", "Christodoulos A Floudas", "Argonaut", "Ruth Misener", "Christodoulos A. Floudas", "Global", "European Journal", "Global Optimization", "Michael L Bynum", "Gabriel A Hackebeil", "William E Hart", "Carl D Laird", "Bethany L Nicholson", "John D Siirola", "Jean-Paul Watson", "David L Woodruff", "Fran", "Keras", "Francesco Croce", "Matthias Hein", "Minimally", "Machine Learning", "Sumanth Dathathri", "Krishnamurthy Dvijotham", "Alexey Kurakin", "Aditi Raghunathan", "Jonathan Uesato", "Rudy Bunel", "Shreya Shankar", "Jacob Steinhardt", "Ian Goodfellow", "Percy Liang", "Matteo Fischetti", "Jason Jo", "Deep", "David M Gay", "Wolfgang R Huster", "Artur M Schweidtmann", "Jannik T", "Alexander Mitsos", "Chemical Engineering", "Calvin Tsay", "Springer", "Yann LeCun", "Corinna Cortes", "Online", "Andrew Lee", "Jaffer H Ghouse", "John C Eslick", "Miguel A Zamarripa", "Dan Gunter", "John H Shinn", "Alexander W Dowling", "Debangsu Bhattacharyya", "Laurens Lueg", "Bjarne Grimstad", "Artur M. Schweidtmann", "Donato Maragno", "Holly Wiberg", "Dimitris Bertsimas", "S Ilker Birbil", "Dick", "Hertog", "Velibor V Misi", "Operations Research", "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga", "Machine Learning Toolkit Aditi Raghunathan", "Percy S Liang", "Thiago Serra", "Abhinav Kumar", "Srikumar Ramalingam", "Gagandeep Singh", "Rupanshu Ganvir", "Markus P", "Martin Vechev", "Systems", "Alexander Thebelt", "Miten Mistry", "Robert M Lee", "Nathan Sudermann-Merx", "Krunal Kishor Patel", "Vincent Tjeng", "Kai Xiao", "Russ Tedrake", "Michael Volpp", "Lukas P Fr", "Kirsten Fischer", "Andreas Doerr", "Stefan Falkner", "Frank Hutter", "Christian Daniel", "Zachary T Wilson", "Nikolaos V Sahinidis", "Dominic Yang", "Prasanna Balaprakash", "Sven Leyffer"]}{"title": ["JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning"], "authors": ["[arxiv.Result.Author('Ashwin Pathak'), arxiv.Result.Author('Raj Shah'), arxiv.Result.Author('Vaibhav Kumar'), arxiv.Result.Author('Yash Jakhotiya')]"], "link": ["http://arxiv.org/pdf/2202.02394v1"], "summary": "Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning .", "entities_include_in_text": ["Kant et al., 2018", "Constant et al., 2017", "Lin, 1999", "Baldwin and Villavicencio, 2002", "Baldwin et al., 2003", "Katz\nand Giesbrecht, 2006", "Kartsaklis et al.,\n2014", "Hashimoto and Tsu-\nruoka, 2016", "Hashempour and Villavicencio,\n2020", "Madabushi et al., 2021", "Madabushi et al., 2021", "Koch et al.,\n2015", "Sung et al., 2018", "Madabushi et al., 2021", "Madabushi et al., 2021"], "entities_from_reference": ["Timothy", "Colin Bannard", "Takaaki Tanaka", "Dominic Widdows", "Timothy Baldwin", "Aline Villavicencio", "Mathieu Constant", "Johanna Monti", "Plas", "Carlos Ramisch", "Michael Rosner", "Multiword", "Reyhaneh Hashempour", "Kazuma Hashimoto", "Yoshimasa Tsuruoka", "Neel Kant", "Raul Puri", "Nikolai Yakovenko", "Bryan Catanzaro", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Graham Katz", "Eugenie Giesbrecht", "Gregory Koch", "Richard Zemel", "Ruslan Salakhutdinov", "Machine Learning", "Dekang Lin", "Madabushi", "Harish Tayyar", "Edward", "Scarton", "Aline", "Dataset", "Flood Sung", "Yongxin Yang", "Li Zhang", "Tao Xiang", "Philip H. S. Torr", "Timothy M. Hospedales"]}{"title": ["Malleable Agents for Re-Configurable Robotic Manipulators"], "authors": ["[arxiv.Result.Author('Athindran Ramesh Kumar'), arxiv.Result.Author('Gurudutt Hosangadi')]"], "link": ["http://arxiv.org/pdf/2202.02395v1"], "summary": "Re-configurable robots potentially have more utility and flexibility for many\nreal-world tasks. Designing a learning agent to operate such robots requires\nadapting to different configurations. While deep reinforcement learning has had\nimmense success in robotic manipulation, domain adaptation is a significant\nproblem that limits its applicability to real-world robotics. We focus on\nrobotic arms with multiple rigid links connected by joints. Recent attempts\nhave performed domain adaptation and Sim2Real transfer to provide robustness to\nrobotic arm dynamics and sensor/camera variations. However, there have been no\nprevious attempts to adapt to robotic arms with a varying number of links. We\npropose an RL agent with sequence neural networks embedded in the deep neural\nnetwork to adapt to robotic arms that have a varying number of links. Further,\nwith the additional tool of domain randomization, this agent adapts to\ndifferent configurations with varying number/length of links and dynamics\nnoise. We perform simulations on a 2D N-link arm to show the ability of our\nnetwork to transfer and generalize efficiently.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["A Temporal-Difference Approach to Policy Gradient Estimation"], "authors": ["[arxiv.Result.Author('Samuele Tosatto'), arxiv.Result.Author('Andrew Patterson'), arxiv.Result.Author('Martha White'), arxiv.Result.Author('A. Rupam Mahmood')]"], "link": ["http://arxiv.org/pdf/2202.02396v1"], "summary": "The policy gradient theorem (Sutton et al., 2000) prescribes the usage of a\ncumulative discounted state distribution under the target policy to approximate\nthe gradient. Most algorithms based on this theorem, in practice, break this\nassumption, introducing a distribution shift that can cause the convergence to\npoor solutions. In this paper, we propose a new approach of reconstructing the\npolicy gradient from the start state without requiring a particular sampling\nstrategy. The policy gradient calculation in this form can be simplified in\nterms of a gradient critic, which can be recursively estimated due to a new\nBellman equation of gradients. By using temporal-difference updates of the\ngradient critic from an off-policy data stream, we develop the first estimator\nthat sidesteps the distribution shift issue in a model-free way. We prove that,\nunder certain realizability conditions, our estimator is unbiased regardless of\nthe sampling strategy. We empirically show that our technique achieves a\nsuperior bias-variance trade-off and performance in presence of off-policy\nsamples.", "entities_include_in_text": ["Sutton et al., 2000", "Sutton et al., 2000", "Mnih et al., 2015; Lillicrap\net al., 2016", "Levine et al., 2020", "Owen,\n2013). Many recent papers aim to lower the variance of pure\nimportance sampling correction. Liu et al. (2018) and Liu\net al. (2019) introduce the concept of state-wise importance\nsampling. Imani et al. (2018", "Nachum et al., 2019", "Degris et al., 2012", "Lillicrap et al., 2016", "Mnih et al., 2016", "Fujimoto et al., 2018", "Haarnoja et al., 2018", "Imani et al., 2018", "Imani et al., 2018; Liu et al., 2019", "Fujimoto\net al., 2019", "Degris et al., 2012", "Silver et al., 2014", "Haarnoja et al., 2018;\nHeess et al., 2015", "Shelton, 2001", "Imani et al., 2018; Liu et al., 2018; 2019", "Imani et al., 2018; Liu et al., 2019", "Ghiassian\net al., 2020", "Sutton, 1988", "Baird, 1995", "Sutton et al., 2008", "Sutton et al.,\n2009", "Ghiassian et al., 2020", "temporal-difference with regularized correction, Ghiassan\net al., (2020)", "Kolter, 2011", "Kolter,\n2011", "Lu et al., 2018", "Shelton, 2001", "Imani et al., 2018", "Degris et al., 2012", "Graves et al., 2021", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011"], "entities_from_reference": ["Function Approximation", "Machine Learning Proceedings", "Degris", "Machine Learning", "Omnipress", "Fujimoto", "Hoof", "Meger", "Machine Learning Research", "Ghiassian", "Patterson", "Garg", "Gupta", "Regularized Corrections", "Imani", "Haarnoja", "Zhou", "Maximum Entropy Deep", "Heess", "Wayne", "Erez", "Stochastic Value Gradients", "Policy Gradient Theorem Using Emphatic Weightings", "Kingma", "J. ADAM", "Kolter", "Lillicrap", "J. J.", "Deep Reinforcement Learning", "Liu", "Li", "Tang", "Agarwal", "Brunskill", "Boutilier", "Mnih", "Riedmiller", "Ostrovski", "Petersen", "Beattie", "Sadik", "Antonoglou", "Kumaran", "Legg", "Nature", "Mirza", "Harley", "Nachum", "Dai", "Kostrikov", "Chow", "Nota", "Thomas", "Owen", "Monte Carlo Theory", "Examples", "Lagoudakis", "Parr", "Levine", "Kumar", "J. Offline", "Peshkin", "Scarce Experience", "Policy Gradient", "Shelton", "Morgan Kaufmann Publishers", "Silver", "Sutton", "Barto", "Singh", "Policy Gradient Methods", "Linear Function Approximation", "Annual International Conference", "Tsitsiklis", "J. N.", "Van Roy", "Sample", "Bellman", "Least Squares", "Markov", "Rnf", "Least Squares Solutions", "Rnmnf", "Pairwise", "Pi", "Aibi", "Discount", "Perfect Features Proof", "Automatic", "Eligibility Trace", "Hence", "Policy Gradient Estimation", "Imanis MDP", "Gradient Estimation", "Adam", "Entropic Regularization", "Random"]}{"title": ["Beam Management with Orientation and RSRP using Deep Learning for Beyond 5G Systems"], "authors": ["[arxiv.Result.Author('Khuong N. Nguyen'), arxiv.Result.Author('Anum Ali'), arxiv.Result.Author('Jianhua Mo'), arxiv.Result.Author('Boon Loong Ng'), arxiv.Result.Author('Vutha Va'), arxiv.Result.Author('Jianzhong Charlie Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02247v1"], "summary": "Beam management (BM), i.e., the process of finding and maintaining a suitable\ntransmit and receive beam pair, can be challenging, particularly in highly\ndynamic scenarios. Side-information, e.g., orientation, from on-board sensors\ncan assist the user equipment (UE) BM. In this work, we use the orientation\ninformation coming from the inertial measurement unit (IMU) for effective BM.\nWe use a data-driven strategy that fuses the reference signal received power\n(RSRP) with orientation information using a recurrent neural network (RNN).\nSimulation results show that the proposed strategy performs much better than\nthe conventional BM and an orientation-assisted BM strategy that utilizes\nparticle filter in another study. Specifically, the proposed data-driven\nstrategy improves the beam-prediction accuracy up to 34% and increases mean\nRSRP by up to 4.2 dB when the UE orientation changes quickly.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["The influence of labeling techniques in classifying human manipulation movement of different speed"], "authors": ["[arxiv.Result.Author('Sadique Adnan Siddiqui'), arxiv.Result.Author('Lisa Gutzeit'), arxiv.Result.Author('Frank Kirchner')]"], "link": ["http://arxiv.org/pdf/2202.02426v1"], "summary": "In this work, we investigate the influence of labeling methods on the\nclassification of human movements on data recorded using a marker-based motion\ncapture system. The dataset is labeled using two different approaches, one\nbased on video data of the movements, the other based on the movement\ntrajectories recorded using the motion capture system. The dataset is labeled\nusing two different approaches, one based on video data of the movements, the\nother based on the movement trajectories recorded using the motion capture\nsystem. The data was recorded from one participant performing a stacking\nscenario comprising simple arm movements at three different speeds (slow,\nnormal, fast). Machine learning algorithms that include k-Nearest Neighbor,\nRandom Forest, Extreme Gradient Boosting classifier, Convolutional Neural\nnetworks (CNN), Long Short-Term Memory networks (LSTM), and a combination of\nCNN-LSTM networks are compared on their performance in recognition of these arm\nmovements. The models were trained on actions performed on slow and normal\nspeed movements segments and generalized on actions consisting of fast-paced\nhuman movement. It was observed that all the models trained on normal-paced\ndata labeled using trajectories have almost 20% improvement in accuracy on test\ndata in comparison to the models trained on data labeled using videos of the\nperformed experiments.", "entities_include_in_text": ["Zhang et al.,\n2019", "Cruciani et al., 2018", "Gutzeit, 2021", "Gutzeit et al., 2014", "Wang and Schmid, 2013", "Donahue et al., 2015", "Ji et al., 2013", "Cruciani et al.,\n2018", "Sham-\nsipour et al., 2017", "Gutzeit, 2021", "Ho, 1995", "Chen and Guestrin, 2016", "Kim, 2014", "Is-\nmail Fawaz et al., 2019", "Hochreiter and Schmid-\nhuber, 1997", "Mutegeki and Han, 2020", "Biewald, 2020", "Gutzeit et al., 2014", "van der Maaten and Hinton, 2008", "Lundberg\nand Lee, 2017", "Ribeiro et al., 2016", "ICPRAM-2021", "KI-2019", "ICPR-2014"], "entities_from_reference": ["Plusieurs"]}{"title": ["Towards Training Reproducible Deep Learning Models"], "authors": ["[arxiv.Result.Author('Boyuan Chen'), arxiv.Result.Author('Mingzhi Wen'), arxiv.Result.Author('Yong Shi'), arxiv.Result.Author('Dayi Lin'), arxiv.Result.Author('Gopi Krishnan Rajbahadur'), arxiv.Result.Author('Zhen Ming'), arxiv.Result.Author('Jiang')]"], "link": ["http://arxiv.org/pdf/2202.02326v1"], "summary": "Reproducibility is an increasing concern in Artificial Intelligence (AI),\nparticularly in the area of Deep Learning (DL). Being able to reproduce DL\nmodels is crucial for AI-based systems, as it is closely tied to various tasks\nlike training, testing, debugging, and auditing. However, DL models are\nchallenging to be reproduced due to issues like randomness in the software\n(e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There\nare various practices to mitigate some of the aforementioned issues. However,\nmany of them are either too intrusive or can only work for a specific usage\ncontext. In this paper, we propose a systematic approach to training\nreproducible DL models. Our approach includes three main parts: (1) a set of\ngeneral criteria to thoroughly evaluate the reproducibility of DL models for\ntwo different domains, (2) a unified framework which leverages a\nrecord-and-replay technique to mitigate software-related randomness and a\nprofile-and-patch technique to control hardware-related non-determinism, and\n(3) a reproducibility guideline which explains the rationales and the\nmitigation strategies on conducting a reproducible training process for DL\nmodels. Case study results show our approach can successfully reproduce six\nopen source and one commercial DL models.", "entities_include_in_text": ["accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed Feb, 2022", "accessed August, 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["The impact of feature importance methods on the interpretation of defect classifiers"], "authors": ["[arxiv.Result.Author('Gopi Krishnan Rajbahadur'), arxiv.Result.Author('Shaowei Wang'), arxiv.Result.Author('Yasutaka Kamei'), arxiv.Result.Author('Ahmed E. Hassan')]"], "link": ["http://arxiv.org/pdf/2202.02389v1"], "summary": "Classifier specific (CS) and classifier agnostic (CA) feature importance\nmethods are widely used (often interchangeably) by prior studies to derive\nfeature importance ranks from a defect classifier. However, different feature\nimportance methods are likely to compute different feature importance ranks\neven for the same dataset and classifier. Hence such interchangeable use of\nfeature importance methods can lead to conclusion instabilities unless there is\na strong agreement among different methods. Therefore, in this paper, we\nevaluate the agreement between the feature importance ranks associated with the\nstudied classifiers through a case study of 18 software projects and six\ncommonly used classifiers. We find that: 1) The computed feature importance\nranks by CA and CS methods do not always strongly agree with each other. 2) The\ncomputed feature importance ranks by the studied CA methods exhibit a strong\nagreement including the features reported at top-1 and top-3 ranks for a given\ndataset and classifier, while even the commonly used CS methods yield vastly\ndifferent feature importance ranks. Such findings raise concerns about the\nstability of conclusions across replicated studies. We further observe that the\ncommonly used defect datasets are rife with feature interactions and these\nfeature interactions impact the computed feature importance ranks of the CS\nmethods (not the CA methods). We demonstrate that removing these feature\ninteractions, even with simple methods like CFS improves agreement between the\ncomputed feature importance ranks of CA and CS methods. In light of our\nfindings, we provide guidelines for stakeholders and practitioners when\nperforming model interpretation and directions for future research, e.g.,\nfuture research is needed to investigate the impact of advanced feature\ninteraction removal methods on computed feature importance ranks of different\nCS methods.", "entities_include_in_text": ["MSR\n2010"], "entities_from_reference": ["Plusieurs"]}{"title": ["StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?"], "authors": ["[arxiv.Result.Author('Stefan Pasch'), arxiv.Result.Author('Daniel Ehnes')]"], "link": ["http://arxiv.org/pdf/2202.02268v1"], "summary": "To answer this question, we fine-tune transformer-based language models,\nincluding BERT, on different sources of company-related text data for a\nclassification task to predict the one-year stock price performance. We use\nthree different types of text data: News articles, blogs, and annual reports.\nThis allows us to analyze to what extent the performance of language models is\ndependent on the type of the underlying document. StonkBERT, our\ntransformer-based stock performance classifier, shows substantial improvement\nin predictive accuracy compared to traditional language models. The highest\nperformance was achieved with news articles as text source. Performance\nsimulations indicate that these improvements in classification accuracy also\ntranslate into above-average stock market returns.", "entities_include_in_text": ["Devlin et al. 2018", "Loughran and McDonald 2011; Loughran and McDonald 2016; Jiang et al. 2019", "Chen 2021; Sivri et al. 2022", "Gennadiy 2020", "Loughran  and  McDonald  2011", "Loughran  and \n\nMcDonald 2016", "Ahmad  et  al.  2016", "Hillert et al. 2014", "Duz Tan and Tas 2021; Sprenger et al. \n\n2014", "Chan et al. 2020", "Peters et al. 2018", "Devlin  et  al.  2018", "Gonzalez-Carvajal and Garrido-Merchan 2020", "Liu et al. 2020). \n\nNot  surprisingly,  researchers  have  integrated  these  deep  learning  approaches  to  predict \n\nstock price movements by applying such language models on company-related text data, such \n\nas tweets or company reports. For example, Sawhney et al. (2020", "Gennadiy 2020", "Devlin  et  al.  2018", "Araci 2019", "Liu et al. \n\n2019", "Devlin et al. 2018", "Clark et al. 2020", "Gonzalez-Carvajal and Garrido-Merchan \n\n2020", "Fama 1970", "Hoberg and Phillips 2016", "Jegadeesh and Titman 2001", "Yuan et al. 2021", "Devlin et al. 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["Automatic Identification of Self-Admitted Technical Debt from Different Sources"], "authors": ["[arxiv.Result.Author('Yikun Li'), arxiv.Result.Author('Mohamed Soliman'), arxiv.Result.Author('Paris Avgeriou')]"], "link": ["http://arxiv.org/pdf/2202.02387v1"], "summary": "Technical debt is a metaphor describing the situation that long-term benefits\n(e.g., maintainability and evolvability of software) are traded for short-term\ngoals. When technical debt is admitted explicitly by developers in software\nartifacts (e.g., code comments or issue tracking systems), it is termed as\nSelf-Admitted Technical Debt or SATD. Technical debt could be admitted in\ndifferent sources, such as source code comments, issue tracking systems, pull\nrequests, and commit messages. However, there is no approach proposed for\nidentifying SATD from different sources. Thus, in this paper, we propose an\napproach for automatically identifying SATD from different sources (i.e.,\nsource code comments, issue trackers, commit messages, and pull requests).", "entities_include_in_text": ["LREC 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["Using Large-scale Heterogeneous Graph Representation Learning for Code Review Recommendations"], "authors": ["[arxiv.Result.Author('Jiyang Zhang'), arxiv.Result.Author('Chandra Maddila'), arxiv.Result.Author('Ram Bairi'), arxiv.Result.Author('Christian Bird'), arxiv.Result.Author('Ujjwal Raizada'), arxiv.Result.Author('Apoorva Agrawal'), arxiv.Result.Author('Yamini Jhawar'), arxiv.Result.Author('Kim Herzig'), arxiv.Result.Author('Arie van Deursen')]"], "link": ["http://arxiv.org/pdf/2202.02385v1"], "summary": "Code review is an integral part of any mature software development process,\nand identifying the best reviewer for a code change is a well accepted problem\nwithin the software engineering community. Selecting a reviewer who lacks\nexpertise and understanding can slow development or result in more defects. To\ndate, most reviewer recommendation systems rely primarily on historical file\nchange and review information; those who changed or reviewed a file in the past\nare the best positioned to review in the future. We posit that while these\napproaches are able to identify and suggest qualified reviewers, they may be\nblind to reviewers who have the needed expertise and have simply never\ninteracted with the changed files before. To address this, we present CORAL, a\nnovel approach to reviewer recommendation that leverages a socio-technical\ngraph built from the rich set of entities (developers, repositories, files,\npull requests, work-items, etc.) and their relationships in modern source code\nmanagement systems. We employ a graph convolutional neural network on this\ngraph and train it on two and a half years of history on 332 repositories. We\nshow that CORAL is able to model the manual history of reviewer selection\nremarkably well. Further, based on an extensive user study, we demonstrate that\nthis approach identifies relevant and qualified reviewers who traditional\nreviewer recommenders miss, and that these developers desire to be included in\nthe review process. Finally, we find that \"classical\" reviewer recommendation\nsystems perform better on smaller (in terms of developers) software projects\nwhile CORAL excels on larger projects, suggesting that there is \"no one model\nto rule them all.\"", "entities_include_in_text": ["Sept. 2018", "Jan 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["The 6-Ds of Creating AI-Enabled Systems"], "authors": ["[arxiv.Result.Author('John Piorkowski')]"], "link": ["http://arxiv.org/pdf/2202.03172v1"], "summary": "We are entering our tenth year of the current Artificial Intelligence (AI)\nspring, and, as with previous AI hype cycles, the threat of an AI winter looms.\nAI winters occurred because of ineffective approaches towards navigating the\ntechnology valley of death. The 6-D framework provides an end-to-end framework\nto successfully navigate this challenge. The 6-D framework starts with problem\ndecomposition to identify potential AI solutions, and ends with considerations\nfor deployment of AI-enabled systems. Each component of the 6-D framework and a\nprecision medicine use case is described in this paper.", "entities_include_in_text": ["Krizhev-\nsky  et  al.,  2012", "Lawrence,  2019", "Collins and Porras, 1996", "Ravitz et al., 2013", "Jesuthasan  and \nBoudreau, 2018", "Gavin, 2019", "Lawrence, 2017", "Chee et al., 2018", "April 2014", "NIPS 2012"], "entities_from_reference": ["Plusieurs"]}{"title": ["Model-Free Reinforcement Learning for Symbolic Automata-encoded Objectives"], "authors": ["[arxiv.Result.Author('Anand Balakrishnan'), arxiv.Result.Author('Stefan Jaksic'), arxiv.Result.Author('Edgar Aguilar Lozano'), arxiv.Result.Author('Dejan Nickovic'), arxiv.Result.Author('Jyotirmoy Deshmukh')]"], "link": ["http://arxiv.org/pdf/2202.02404v1"], "summary": "Reinforcement learning (RL) is a popular approach for robotic path planning\nin uncertain environments. However, the control policies trained for an RL\nagent crucially depend on user-defined, state-based reward functions. Poorly\ndesigned rewards can lead to policies that do get maximal rewards but fail to\nsatisfy desired task objectives or are unsafe. There are several examples of\nthe use of formal languages such as temporal logics and automata to specify\nhigh-level task specifications for robots (in lieu of Markovian rewards).\nRecent efforts have focused on inferring state-based rewards from formal\nspecifications; here, the goal is to provide (probabilistic) guarantees that\nthe policy learned using RL (with the inferred rewards) satisfies the\nhigh-level formal specification. A key drawback of several of these techniques\nis that the rewards that they infer are sparse: the agent receives positive\nrewards only upon completion of the task and no rewards otherwise. This\nnaturally leads to poor convergence properties and high variance during RL. In\nthis work, we propose using formal specifications in the form of symbolic\nautomata: these serve as a generalization of both bounded-time temporal\nlogic-based specifications as well as automata. Furthermore, our use of\nsymbolic automata allows us to define non-sparse potential-based rewards which\nempirically shape the reward surface, leading to better convergence during RL.\nWe also show that our potential-based rewarding strategy still allows us to\nobtain the policy that maximizes the satisfaction of the given specification.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Identifying Self-Admitted Technical Debt in Issue Tracking Systems using Machine Learning"], "authors": ["[arxiv.Result.Author('Yikun Li'), arxiv.Result.Author('Mohamed Soliman'), arxiv.Result.Author('Paris Avgeriou')]"], "link": ["http://arxiv.org/pdf/2202.02180v1"], "summary": "Technical debt is a metaphor indicating sub-optimal solutions implemented for\nshort-term benefits by sacrificing the long-term maintainability and\nevolvability of software. A special type of technical debt is explicitly\nadmitted by software engineers (e.g. using a TODO comment); this is called\nSelf-Admitted Technical Debt or SATD. Most work on automatically identifying\nSATD focuses on source code comments. In addition to source code comments,\nissue tracking systems have shown to be another rich source of SATD, but there\nare no approaches specifically for automatically identifying SATD in issues. In\nthis paper, we first create a training dataset by collecting and manually\nanalyzing 4,200 issues (that break down to 23,180 sections of issues) from\nseven open-source projects (i.e., Camel, Chromium, Gerrit, Hadoop, HBase,\nImpala, and Thrift) using two popular issue tracking systems (i.e., Jira and\nGoogle Monorail). We then propose and optimize an approach for automatically\nidentifying SATD in issue tracking systems using machine learning. Our findings\nindicate that: 1) our approach outperforms baseline approaches by a wide margin\nwith regard to the F1-score; 2) transferring knowledge from suitable datasets\ncan improve the predictive performance of our approach; 3) extracted SATD\nkeywords are intuitive and potentially indicating types and indicators of SATD;\n4) projects using different issue tracking systems have less common SATD\nkeywords compared to projects using the same issue tracking system; 5) a small\namount of training data is needed to achieve good accuracy.", "entities_include_in_text": ["Avgeriou et al., 2016", "Li\net al., 2015", "Alves et al., 2016", "Tufano et al., 2017", "Ernst, 2012)", "Potdar and Shihab, 2014", "Potdar and Shihab, 2014", "Sierra\net al., 2019", "Li\net al., 2020; Bellomo et al., 2016", "Merten et al., 2015; Li et al., 2020", "Li et al., 2020", "Li\net al., 2020", "Kim, 2014", "Li et al., 2020", "van Solingen et al., 2002", "Shalev-Shwartz and Ben-David, 2014", "Potdar and Shihab, 2014; Wehaibi et al., 2016; Zampetti et al., 2018", "Smith, 2018", "Potdar and Shihab, 2014; Wehaibi et al., 2016; d. S. Maldon-\nado et al., 2017", "Li et al., 2020", "Alves\net al., 2014", "Fleiss et al., 1981", "Sun et al., 2009", "McCallum et al., 1998", "Tan, 2006", "Genkin et al., 2007", "Xu et al., 2012", "Kowsari et al., 2019", "d. S. Mal-\ndonado et al., 2017; Huang et al., 2018; Flisar and Podgorelec, 2019", "Yao et al., 2019", "Yao et al., 2019", "Ren et al.,\n2019", "Bavota\nand Russo, 2016", "Wei and Zou, 2019", "Phan et al., 2017; Ren\net al., 2019", "Joulin et al., 2017; Wieting et al., 2015", "Mikolov et al., 2018", "Kim, 2014", "see Potdar and Shihab (2014", "Mikolov et al., 2018", "Bo-\njanowski et al., 2017", "Zhang and Wallace, 2017", "Zhang and Wallace, 2017", "Semwal et al., 2018", "d. S. Maldonado et al.,\n2017", "Zhang et al.,\n2015", "Semwal\net al., 2018", "Ortu et al., 2016; Calefato et al., 2018", "Perkins et al., 1992", "see\nPotdar and Shihab (2014)", "Li et al., 2020", "Gu\net al., 2014", "HBase-1990", "see Potdar and Shihab (2014)", "d. S. Mal-\ndonado et al., 2017; Flisar and Podgorelec, 2019; Ren et al., 2019", "d. S. Maldonado et al., 2017; Huang et al., 2018", "Zhang and Wallace, 2017", "Semwal et al., 2018", "LREC 2018)\nOrtu M, Murgia A, Destefanis G, Tourani P, Tonelli R, Marchesi M, Adams\nB (2016"], "entities_from_reference": ["Ribeiro LF", "Caires V", "Mendes TS", "Mendon", "Sp", "Shull F", "Seaman C", "Kruchten P", "Ozkaya", "Dagstuhl", "Russo B", "Nord RL", "Popeck M", "Bojanowski P", "Grave E", "Joulin A", "Mikolov T", "Calefato F", "Lanubile F", "Maiorano F", "Novielli N", "Empirical Software", "Dai K", "Efstathiou V", "Chatzilenas C", "Spinellis D", "Fern", "Garc", "Galar M", "Prati RC", "Krawczyk B", "Herrera F", "Springer Fleiss JL", "Levin B", "Paik MC", "Flisar J", "Podgorelec V", "Freitas Farias MA", "Santos JA", "Kalinowski M", "Springer", "Lewis DD", "Madigan D", "Gu L", "Eils R", "Schlesner M", "Brors B", "Huang Q", "Shihab E", "Xia X", "Lo D", "Li S", "Short Papers", "Maldonado EdS", "Ubayashi N", "Kim Y", "Kowsari K", "Jafari Meimandi K", "Heidarysafa M", "Mendu S", "Barnes L", "Brown D", "Li Y", "Soliman M", "Avgeriou P", "Advanced Applications", "Machine Learning", "Liang P", "Nigam K", "Mager B", "Quirchmayr T", "Paech B", "Puhrsch C", "Ortu M", "Murgia A", "Destefanis G", "Tourani P", "Tonelli R", "Marchesi M", "Adams B", "Perkins DN", "Salomon G", "Gerkmann T", "Potdar A", "Ren X", "Wang X", "Grundy J", "Runeson P", "Rainer A", "Regnell B", "John Wiley", "S Maldonado E", "Maldonado E", "Tsantalis N", "Semwal T", "Yenigalla P", "Mathur G", "Nair SB", "Data Mining", "Ben-David S", "Cambridge", "Sierra G", "Kamei Y", "Smith T", "Solingen R", "Basili V", "Caldiera G", "Rombach HD", "Goal Question Metric", "Sons", "Hoboken", "Steidl D", "Hummel B", "Juergens E", "Ieee", "Lim EP", "Liu Y", "Tan S", "Tufano M", "Palomba F", "Bavota G", "Oliveto R", "Di Penta M", "De Lucia A", "Poshyvanyk D", "Wehaibi S", "Guerrouj L", "Evolution", "Zou K", "Bansal M", "Gimpel K", "Livescu K", "Xavier L", "Ferreira F", "Brito R", "Valente MT", "Guo X", "Ye Y", "Cheng J", "Yao L", "Mao C", "Luo Y", "Serebrenik A", "Di Penta", "Zhang X", "Zhao J", "Zhang Y", "Wallace BC"]}{"title": ["Generative Modeling of Complex Data"], "authors": ["[arxiv.Result.Author('Luca Canale'), arxiv.Result.Author('Nicolas Grislain'), arxiv.Result.Author('Gr\u00e9goire Lothe'), arxiv.Result.Author('Johan Leduc')]"], "link": ["http://arxiv.org/pdf/2202.02145v1"], "summary": "In recent years, several models have improved the capacity to generate\nsynthetic tabular datasets. However, such models focus on synthesizing simple\ncolumnar tables and are not useable on real-life data with complex structures.\nThis paper puts forward a generic framework to synthesize more complex data\nstructures with composite and nested types. It then proposes one practical\nimplementation, built with causal transformers, for struct (mappings of types)\nand lists (repeated instances of a type). The results on standard benchmark\ndatasets show that such implementation consistently outperforms current\nstate-of-the-art models both in terms of machine learning utility and\nstatistical similarity. Moreover, it shows very strong results on two complex\nhierarchical datasets with multiple nesting and sparse data, that were\npreviously out of reach.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["A Discourse on MetODS: Meta-Optimized Dynamical Synapses for Meta-Reinforcement Learning"], "authors": ["[arxiv.Result.Author('Mathieu Chalvidal'), arxiv.Result.Author('Thomas Serre'), arxiv.Result.Author('Rufin VanRullen')]"], "link": ["http://arxiv.org/pdf/2202.02363v1"], "summary": "Recent meta-reinforcement learning work has emphasized the importance of\nmnemonic control for agents to quickly assimilate relevant experience in new\ncontexts and suitably adapt their policy. However, what computational\nmechanisms support flexible behavioral adaptation from past experience remains\nan open question. Inspired by neuroscience, we propose MetODS (for\nMeta-Optimized Dynamical Synapses), a broadly applicable model of\nmeta-reinforcement learning which leverages fast synaptic dynamics influenced\nby action-reward feedback. We develop a theoretical interpretation of MetODS as\na model learning powerful control rules in the policy space and demonstrate\nempirically that robust reinforcement learning programs emerge spontaneously\nfrom them. We further propose a formalism which efficiently optimizes the\nmeta-parameters governing MetODS synaptic processes. In multiple experiments\nand domains, MetODS outperforms or compares favorably with previous\nmeta-reinforcement learning approaches. Our agents can perform one-shot\nlearning, approaches optimal exploration/exploitation strategies, generalize\nnavigation principles to unseen environments and demonstrate a strong ability\nto learn adaptive motor policies.", "entities_include_in_text": ["Clune, 2020", "Mnih et al., 2015", "Mnih et al., 2013", "Jaderberg\net al., 2019", "Levine et al., 2016; Lillicrap\net al., 2016", "Botvinick et al., 2019", "Finn et al., 2017", "Yger et al., 2015", "Florian, 2007", "Lu et al.,\n2019", "Ram-\nsauer et al., 2020", "Schlag\net al., 2020", "Chalvidal et al., 2021", "Houthooft et al., 2018; Xu et al., 2018; Gupta et al., 2018", "Hochreiter et al.,\n2001; Duan et al., 2016; Wang et al., 2018", "Mishra et al., 2018", "Finn et al., 2017", "Miconi, 2016", "Choromanski et al., 2020", "Duan et al., 2016", "Ramsauer et al., 2020", "Mishra et al.,\n2018", "Ramsauer et al., 2020", "Paszke\net al., 2019", "Rumelhart et al., 1985", "Betancourt et al., 2020", "Chalvidal et al.,\n2021", "Mnih et al., 2016", "Harlow,\n1949", "Prim, 1957", "Mishra et al., 2018", "Gittins, 1979", "Mishra et al., 2018", "Todorov\net al., 2012", "Finn et al., 2017", "Betancourt et al., 2020", "Schulman et al., 2018", "Wang et al., 2018", "Mishra et al., 2018"], "entities_from_reference": ["Nature", "Mnih", "Leibo", "Ionescu", "Barak", "Tsodyks", "Bartunov", "Rae", "Betancourt", "Botvinick", "Ritter", "Wang", "J. X.", "Caporale", "Dan", "Annual Review", "Chalvidal", "Ricci", "Choromanski", "Davis", "Slotine", "Varley", "Lee", "Weller", "Sindhwani", "Clune", "J.", "Cobbe", "Hesse", "Hilton", "Schulman", "J. Leveraging", "Upgang", "Duan", "Bartlett", "Finn", "Teh", "Machine Learning", "Machine Learning Research", "J. C. Bandit", "Gupta", "Liu", "Dai", "Le", "Harlow", "Hinton", "Plaut", "Annual Conference", "Hochreiter", "Younger", "Springer", "Hopfield", "J. J.", "Houthooft", "Isola", "Stadie", "Wolski", "Jaderberg", "Czarnecki", "Marris", "Casta", "Beattie", "Rabinowitz", "Ruderman", "Sonnerat", "Deason", "Graepel", "Science", "Koiran", "Krotov", "J. J", "Dense", "Ullman", "Tenenbaum", "Lengyel", "Dayan", "Koller", "Singer", "Roweis", "Levine", "Darrell", "Lillicrap", "Heess", "Erez", "Lin", "Zhao", "Yang", "Zhang", "Jin", "Manohar", "Masse", "Freedman", "Miconi", "Rawal", "Stanley", "J. Differentiable", "Mishra", "Antonoglou", "Riedmiller", "Ostrovski", "Mirza", "Harley", "Mongillo", "Munkhdalai", "Najarro", "Ranzato", "Hadsell", "Paszke", "Gross", "Lerer", "Bradbury", "Chanan", "Killeen", "Gimelshein", "Desmaison", "Kopf", "Raison", "Tejani", "Steiner", "Fang", "Bai", "Pritzel", "Vinyals", "Machine A Discourse", "Schlag", "Ramsauer", "Sch", "Lehner", "Seidl", "Widrich", "Adler", "Kreil", "Kopp", "Ravi", "Regehr", "Jayakumar", "Pascanu", "Stokes", "Thrun", "Rumelhart", "Williams", "San Diego La Jolla Inst", "Cognitive Science", "Santoro", "Todorov", "Systems", "Vilalta", "Artif", "Brain Function", "Kumaran", "Hasselt", "Yger", "Brette", "Zaremba", "Zhu", "Discrete", "Xavier", "Maze Navigation", "Green", "Average", "Reward"]}{"title": ["OntoSeer -- A Recommendation System to Improve the Quality of Ontologies"], "authors": ["[arxiv.Result.Author('Pramit Bhattacharyya'), arxiv.Result.Author('Raghava Mutharaju')]"], "link": ["http://arxiv.org/pdf/2202.02125v1"], "summary": "Building an ontology is not only a time-consuming process, but it is also\nconfusing, especially for beginners and the inexperienced. Although ontology\ndevelopers can take the help of domain experts in building an ontology, they\nare not readily available in several cases for a variety of reasons. Ontology\ndevelopers have to grapple with several questions related to the choice of\nclasses, properties, and the axioms that should be included. Apart from this,\nthere are aspects such as modularity and reusability that should be taken care\nof. From among the thousands of publicly available ontologies and vocabularies\nin repositories such as Linked Open Vocabularies (LOV) and BioPortal, it is\nhard to know the terms (classes and properties) that can be reused in the\ndevelopment of an ontology. A similar problem exists in implementing the right\nset of ontology design patterns (ODPs) from among the several available.\nGenerally, ontology developers make use of their experience in handling these\nissues, and the inexperienced ones have a hard time. In order to bridge this\ngap, we propose a tool named OntoSeer, that monitors the ontology development\nprocess and provides suggestions in real-time to improve the quality of the\nontology under development. It can provide suggestions on the naming\nconventions to follow, vocabulary to reuse, ODPs to implement, and axioms to be\nadded to the ontology. OntoSeer has been implemented as a Prot\\'eg\\'e plug-in.", "entities_include_in_text": ["Apr 2014"], "entities_from_reference": ["Bhattacharyya", "Mutharaju", "Bhattachayya", "Indraprastha Institute", "Cahyono", "Science", "Dreler", "Ngonga Ngomo", "Bounded Jaro Winkler Distances", "Garc", "Hitzler", "Janowicz", "Zaveri", "Gray", "Lopez", "Haller", "Springer International Publishing", "Angele", "Knowledge Management", "Guarino", "Staab", "Springer Berlin Heidelberg", "Hammar", "Ontology Design Patterns", "Gangemi", "Krisnadhi", "Le", "Mikolov", "Machine Learning", "Noy", "Ontology Evaluation", "Ren", "Deemter", "Stevens"]}{"title": ["Interpretability methods of machine learning algorithms with applications in breast cancer diagnosis"], "authors": ["[arxiv.Result.Author('Panagiota Karatza'), arxiv.Result.Author('Kalliopi V. Dalakleidi'), arxiv.Result.Author('Maria Athanasiou'), arxiv.Result.Author('Konstantina S. Nikita')]"], "link": ["http://arxiv.org/pdf/2202.02131v1"], "summary": "Early detection of breast cancer is a powerful tool towards decreasing its\nsocioeconomic burden. Although, artificial intelligence (AI) methods have shown\nremarkable results towards this goal, their \"black box\" nature hinders their\nwide adoption in clinical practice. To address the need for AI guided breast\ncancer diagnosis, interpretability methods can be utilized. In this study, we\nused AI methods, i.e., Random Forests (RF), Neural Networks (NN) and Ensembles\nof Neural Networks (ENN), towards this goal and explained and optimized their\nperformance through interpretability techniques, such as the Global Surrogate\n(GS) method, the Individual Conditional Expectation (ICE) plots and the Shapley\nvalues (SV). The Wisconsin Diagnostic Breast Cancer (WDBC) dataset of the open\nUCI repository was used for the training and evaluation of the AI algorithms.\nThe best performance for breast cancer diagnosis was achieved by the proposed\nENN (96.6% accuracy and 0.96 area under the ROC curve), and its predictions\nwere explained by ICE plots, proving that its decisions were compliant with\ncurrent medical knowledge and can be further utilized to gain new insights in\nthe pathophysiological mechanisms of breast cancer. Feature selection based on\nfeatures' importance according to the GS model improved the performance of the\nRF (leading the accuracy from 96.49% to 97.18% and the area under the ROC curve\nfrom 0.96 to 0.97) and feature selection based on features' importance\naccording to SV improved the performance of the NN (leading the accuracy from\n94.6% to 95.53% and the area under the ROC curve from 0.94 to 0.95). Compared\nto other approaches on the same dataset, our proposed models demonstrated state\nof the art performance while being interpretable.", "entities_include_in_text": [], "entities_from_reference": ["Online", "Anderson", "Saygili", "Classification", "Breast Cancers", "Breast Cancer Malignancy", "Networks", "Breast Cancer", "Neighborhood Component Analysis", "Machine Learning Techniques", "Advanced Communication", "Toward Medical XAI", "Nikita", "Diabetes Mellitus", "Santos", "Antunes", "J. Kasmanas", "Machine Learning", "Santurkar", "How Does Batch Normalization Help", "Baba"]}{"title": ["Deep invariant networks with differentiable augmentation layers"], "authors": ["[arxiv.Result.Author('C\u00e9dric Rommel'), arxiv.Result.Author('Thomas Moreau'), arxiv.Result.Author('Alexandre Gramfort')]"], "link": ["http://arxiv.org/pdf/2202.02142v1"], "summary": "Designing learning systems which are invariant to certain data\ntransformations is critical in machine learning. Practitioners can typically\nenforce a desired invariance on the trained model through the choice of a\nnetwork architecture, e.g. using convolutions for translations, or using data\naugmentation. Yet, enforcing true invariance in the network can be difficult,\nand data invariances are not always known a piori. State-of-the-art methods for\nlearning data augmentation policies require held-out data and are based on\nbilevel optimization problems, which are complex to solve and often\ncomputationally demanding. In this work we investigate new ways of learning\ninvariances only from the training data. Using learnable augmentation layers\nbuilt directly in the network, we demonstrate that our method is very\nversatile. It can incorporate any type of differentiable augmentation and be\napplied to a broad class of learning problems beyond computer vision. We\nprovide empirical evidence showing that our approach is easier and faster to\ntrain than modern automatic data augmentation techniques based on bilevel\noptimization, while achieving comparable results. Experiments show that while\nthe invariances transferred to a model through automatic data augmentation are\nlimited by the model expressivity, the invariance yielded by our approach is\ninsensitive to it by design.", "entities_include_in_text": ["LeCun et al., 1989", "Krizhevsky et al., 2012", "Zhou\net al., 2021", "Chambon et al., 2018; Phan et al.,\n2021", "Cubuk et al., 2019", "Cubuk\net al., 2019", "Ho et al., 2019", "Lim et al., 2019", "Hataya et al., 2020", "Li\net al., 2020", "Rommel et al., 2021", "Liu\net al., 2019", "Zaheer et al., 2017)\nencode permutation invariance by summing networks pre-\ndictions. Although related, these methods are not designed\nfor learning symmetries from the data, which is the objective\nof this study.\n\nPrior work on this matter from van der Wilk et al. (2018) pro-\nposes to learn invariances using the marginal likehood in the\ncontext of Gaussian processes. In contrast, we are mostly fo-\ncused on deep neural networks. Zhou et al. (2021", "Benton\net al., 2020", "cf. Chen et al. (2020", "Schulman\net al., 2015", "Bengio et al., 2013", "Grathwohl\net al., 2018", "Benton et al., 2020", "Benton et al., 2020", "Benton et al.,\n2020", "Rommel et al.,\n2021", "Schwabedal et al., 2019", "Wang et al., 2018", "Howard,\n2014", "Inoue, 2018)", "Iber\net al., 2007", "Krizhevsky et al., 2009", "Benton et al., 2020", "Chambon et al.,\n2018", "Cubuk et al., 2019", "Lim\net al., 2019", "Hataya et al., 2020", "Li et al., 2020", "Rommel et al., 2021", "Benton et al., 2020", "medium according to Benton et al. (2020)", "Schirrmeister et al., 2017", "Akiba et al., 2019", "Riba\net al., 2020", "Hataya et al., 2020", "Chambon\net al., 2018", "Gramfort et al.,\n2013", "Schirrmeister et al.,\n2017", "Chambon et al., 2018"], "entities_from_reference": ["Ohta", "Data Mining", "Bengio", "Benton", "Finzi", "Izmailov", "Wilson", "Bouchacourt", "Ibrahim", "Chambon", "Galtier", "Wainrib", "Chen", "Lee", "J.", "Data Augmentation", "Hsieh", "Machine Learning", "Cohen", "Cubuk", "Zoph", "Mane", "Vasudevan", "Le", "Pattern Recognition", "Gramfort", "Larson", "Strohmeier", "Goj", "Jas", "Grathwohl", "Choi", "Roeder", "Duvenaud", "Hataya", "Springer International Publishing", "Zhang", "Ren", "Deep", "Liang", "Population", "Howard", "Chesson", "Quan", "Terminology", "Inoue", "Kingma", "J. Adam", "Krizhevsky", "Denker", "J. S.", "Henderson", "Hubbard", "Jackel", "Wang", "Robertson", "Yang", "Lim", "Kim", "Zela", "Elsken", "Marrakchi", "Brox", "Hutter", "Chang", "Abbasnejad", "Haffari", "Stochastic Implicit Gradients", "Zhou", "Knowles", "Liu", "Simonyan", "Gosselin", "Carrier", "Nielsen", "Phan", "Koch", "Mertins", "De Vos", "Pattern Analysis", "Machine Intelligence", "Riba", "Mishkin", "Bradski", "Source Differentiable Computer", "Rommel", "Paillard", "J. T.", "Glasstetter", "Eggensperger", "Tangermann", "Burgard", "Human Brain Mapping", "Schulman", "Heess", "Schwabedal", "J. T. C.", "J. C.", "Nemati", "Clifford", "Noisy Signals", "Fourier Transform Surrogates", "Wilk", "Bauer", "John", "Hensman", "Peng", "Jiang", "Deep Convolutional Neural Networks", "Series Title", "Zaheer", "Salakhutdinov", "Augerino", "Conv2D ReLU Conv2D", "Figure", "Sinusoids", "Faster AA", "Akiba", "Faster", "Adam", "Balanced", "Tesla V100", "Dense", "Softmax Table", "Frequency Sensors", "Figure B.2", "Loss", "Random"]}{"title": ["A Robot Web for Distributed Many-Device Localisation"], "authors": ["[arxiv.Result.Author('Riku Murai'), arxiv.Result.Author('Joseph Ortiz'), arxiv.Result.Author('Sajad Saeedi'), arxiv.Result.Author('Paul H. J. Kelly'), arxiv.Result.Author('Andrew J. Davison')]"], "link": ["http://arxiv.org/pdf/2202.03314v1"], "summary": "We show that a distributed network of robots or other devices which make\nmeasurements of each other can collaborate to globally localise via efficient\nad-hoc peer to peer communication. Our Robot Web solution is based on Gaussian\nBelief Propagation on the fundamental non-linear factor graph describing the\nprobabilistic structure of all of the observations robots make internally or of\neach other, and is flexible for any type of robot, motion or sensor. We define\na simple and efficient communication protocol which can be implemented by the\npublishing and reading of web pages or other asynchronous communication\ntechnologies. We show in simulations with up to 1000 robots interacting in\narbitrary patterns that our solution convergently achieves global accuracy as\naccurate as a centralised non-linear factor graph solver while operating with\nhigh distributed efficiency of computation and communication. Via the use of\nrobust factors in GBP, our method is tolerant to a high percentage of faults in\nsensor measurements or dropped communication packets.", "entities_include_in_text": [], "entities_from_reference": ["Robust", "Ceres", "Andersson", "Aragues", "Bryson", "J. Vial", "Barooah", "Caceres", "Hybrid", "J. P. How", "J. Rogers", "Robotics Research", "Davison", "Active Search", "Technical Report", "Georgia Institute", "Annual Review", "Robotics", "Factor Graphs", "Robot Perception", "Mukadam", "Boots", "Motion Planning", "Gaussian Processes", "Applied Iterative Methods", "Academic Press", "J. Yu", "Huang", "Nelson", "Michael", "Multirobot", "Johannsson", "J. Leonard", "Wiel", "Nature", "Kim", "Teller", "Graph Optimization", "Lajoie", "Towards", "Barfoot", "Robotic Systems", "Wang", "J. Kuang", "Signal Processing", "J. Deray", "Burgard", "Asynchronous IEEE", "J. Lien", "Zhang", "J. Dong", "J. Engel"]}{"title": ["Corrupted Image Modeling for Self-Supervised Visual Pre-Training"], "authors": ["[arxiv.Result.Author('Yuxin Fang'), arxiv.Result.Author('Li Dong'), arxiv.Result.Author('Hangbo Bao'), arxiv.Result.Author('Xinggang Wang'), arxiv.Result.Author('Furu Wei')]"], "link": ["http://arxiv.org/pdf/2202.03382v1"], "summary": "We introduce Corrupted Image Modeling (CIM) for self-supervised visual\npre-training. CIM uses an auxiliary generator with a small trainable BEiT to\ncorrupt the input image instead of using artificial mask tokens, where some\npatches are randomly selected and replaced with plausible alternatives sampled\nfrom the BEiT output distribution. Given this corrupted image, an enhancer\nnetwork learns to either recover all the original image pixels, or predict\nwhether each visual token is replaced by a generator sample or not. The\ngenerator and the enhancer are simultaneously trained and synergistically\nupdated. After pre-training, the enhancer can be used as a high-capacity visual\nencoder for downstream tasks. CIM is a general and flexible visual pre-training\nframework that is suitable for various network architectures. For the first\ntime, CIM demonstrates that both ViT and CNN can learn rich visual\nrepresentations using a unified, non-Siamese framework. Experimental results\nshow that our approach achieves compelling results in vision benchmarks, such\nas ImageNet classification and ADE20K semantic segmentation. For example,\n300-epoch CIM pre-trained vanilla ViT-Base/16 and ResNet-50 obtain 83.3 and\n80.6 Top-1 fine-tuning accuracy on ImageNet-1K image classification\nrespectively.", "entities_include_in_text": ["Zhou et al., 2021; He et al., 2021;\nXie et al., 2021; Dong et al., 2021; Wei et al., 2021; El-\nNouby et al., 2021", "Devlin et al.,\n2019", "Vaswani et al., 2017", "Bao\net al., 2021", "Ramesh et al., 2021", "Dosovitskiy et al., 2020", "Clark et al., 2020", "Esser et al., 2021; Yu et al.,\n2021; Dong et al., 2021", "Holtzman\net al., 2019", "Wei et al.,\n\n2021", "He et al.,\n2016", "Lin et al., 2014", "Bao et al.,\n2021", "Rolfe, 2016; Van\nDen Oord et al., 2017", "Ramesh\net al., 2021", "rather than adversari-\nally as Goodfellow et al., 2014", "He\net al., 2021", "He et al., 2021", "Ramesh et al., 2021; Bao et al.,\n2021", "Ramesh et al., 2021", "Bao et al., 2021", "Paszke et al., 2019", "Caron et al., 2021", "Bao et al., 2021", "He et al., 2021", "Caron et al., 2021", "Bao et al., 2021", "Bao et al., 2021", "He et al., 2021", "Deng et al., 2009", "Bao et al.,\n2021", "Srivastava et al.,\n2014", "Huang et al., 2016", "Zhou\net al., 2019", "Wightman et al., 2021", "He et al., 2016", "Paszke et al., 2019", "Touvron et al., 2019", "Bello et al., 2021", "Wightman et al., 2021", "Wightman et al., 2021", "Grill et al., 2020", "Caron et al., 2020", "Wightman et al., 2021", "Liu et al., 2022", "Caron et al., 2021", "Bao et al., 2021", "He et al., 2019", "He et al., 2019", "Grill et al., 2020", "Caron et al., 2020", "Long et al., 2015", "He et al., 2021; Xie et al., 2021", "He et al., 2021", "Meng et al., 2021; Chi et al., 2021", "He et al.,\n2021", "Zhong et al., 2020", "Esser et al.,\n2021; Dong et al., 2021; Yu et al., 2021", "Zhong et al., 2020", "He et al., 2020", "Dosovitskiy et al., 2020", "Bao et al., 2021", "Dong et al., 2021", "Wei et al., 2021", "Devlin et al., 2019", "Clark et al., 2020", "Yu et al., 2021", "Wei et al., 2021", "Srivastava et al., 2014", "Huang et al., 2016", "Vaswani et al., 2017", "Dosovitskiy et al., 2020", "Bao\net al., 2021", "Bao et al., 2021", "Srivastava et al., 2014", "Huang et al., 2016", "Szegedy et al., 2016", "Yun et al., 2019", "Cubuk et al., 2020", "Clark et al., 2020", "Bao et al., 2021", "Bao et al., 2021", "Berman et al., 2019; Hoffer\net al., 2019", "Cubuk et al., 2020", "Yun et al., 2019", "Szegedy et al., 2016", "Huang et al., 2016", "Srivastava et al., 2014", "Wightman et al., 2021", "Bao\net al., 2021"], "entities_from_reference": ["Bao", "Wei", "Bello", "Fedus", "Lin", "Zoph", "Berman", "Vedaldi", "Douze", "Caron", "Misra", "Goyal", "Bojanowski", "Joulin", "Touvron", "Chen", "Wang", "Guo", "Deng", "Liu", "Gao", "Jun", "Luan", "Fan", "Girshick", "Xie", "Chi", "Huang", "Bajaj", "Clark", "Le", "Cubuk", "Practical", "Pattern Recognition Workshops", "Socher", "Li", "Devlin", "Chang", "Lee", "Toutanova", "Singh", "Dong", "Loy", "Tang", "Image", "Zhang", "Yuan", "Wen", "Dosovitskiy", "Kolesnikov", "Zhai", "Dehghani", "Minderer", "Heigold", "Gelly", "Izacard", "Jegou", "Grave", "Esser", "Goodfellow", "Mirza", "Ozair", "Bengio", "Grill", "Strub", "Richemond", "Doersch", "Pires", "Azar", "Ren", "Deep", "Hoffer", "Hoefler", "Holtzman", "Buys", "Choi", "Sedra", "Weinberger", "Denker", "J. S.", "Henderson", "Howard", "Hubbard", "Jackel", "Maire", "Hays", "Ramanan", "Zitnick", "Darrell", "Loshchilov", "Hutter", "Meng", "Bennett", "Han", "Paszke", "Gross", "Lerer", "Bradbury", "Chanan", "Killeen", "Gimelshein", "Desmaison", "Kopf", "Yang", "Raison", "Tejani", "Steiner", "Fang", "Bai", "Ramesh", "Pavlov", "Goh", "Gray", "Voss", "Rolfe", "J. T. Discrete", "Srivastava", "Krizhevsky", "Salakhutdinov", "Szegedy", "Vanhoucke", "Ioffe", "Wojna", "Van Den Oord", "Vinyals", "Mao", "Vaswani", "Shazeer", "Uszkoreit", "Jones", "Gomez", "Polosukhin", "Visual", "Wightman", "Cao", "Yao", "Dai", "Koh", "J. Y.", "Pang", "Qin", "Yun", "Chun", "Choe", "Yoo", "Cisse", "Dauphin", "Zuo", "Zhong", "Zheng", "Kang", "Ran", "Zhou", "Zhao", "Puig", "Fidler", "Barriuso", "Appendix", "Vanilla ViT", "Value Optimizer", "Batch Size Weight Decay Optimizer Momentum", "Masked Patches", "Stochastic Depth", "Patch Size", "Cosine Decay", "Random", "Vanilla ViT Models", "Label Smoothing", "Random Augmentation", "Size", "Loss Function", "Cross Entropy Loss", "Vanilla", "Epoch", "Epoch FT Optimizer Peak Learning Rate", "Batch Size Learning Rate Schedule Loss Function Warmup Epochs Weight Decay", "Cosine Decay Binary Cross Entropy"]}{"title": ["Towards Loosely-Coupling Knowledge Graph Embeddings and Ontology-based Reasoning"], "authors": ["[arxiv.Result.Author('Zoi Kaoudi'), arxiv.Result.Author('Abelardo Carlos Martinez Lorenzo'), arxiv.Result.Author('Volker Markl')]"], "link": ["http://arxiv.org/pdf/2202.03173v1"], "summary": "Knowledge graph completion (a.k.a.~link prediction), i.e.,~the task of\ninferring missing information from knowledge graphs, is a widely used task in\nmany applications, such as product recommendation and question answering. The\nstate-of-the-art approaches of knowledge graph embeddings and/or rule mining\nand reasoning are data-driven and, thus, solely based on the information the\ninput knowledge graph contains. This leads to unsatisfactory prediction results\nwhich make such solutions inapplicable to crucial domains such as healthcare.\nTo further enhance the accuracy of knowledge graph completion we propose to\nloosely-couple the data-driven power of knowledge graph embeddings with\ndomain-specific reasoning stemming from experts or entailment regimes (e.g.,\nOWL2). In this way, we not only enhance the prediction accuracy with domain\nknowledge that may not be included in the input knowledge graph but also allow\nusers to plugin their own knowledge graph embedding and reasoning method. Our\ninitial results show that we enhance the MRR accuracy of vanilla knowledge\ngraph embeddings by up to 3x and outperform hybrid solutions that combine\nknowledge graph embeddings with rule mining and reasoning up to 3.5x MRR.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Conversational Agents: Theory and Applications"], "authors": ["[arxiv.Result.Author('Mattias Wahde'), arxiv.Result.Author('Marco Virgolin')]"], "link": ["http://arxiv.org/pdf/2202.03164v1"], "summary": "In this chapter, we provide a review of conversational agents (CAs),\ndiscussing chatbots, intended for casual conversation with a user, as well as\ntask-oriented agents that generally engage in discussions intended to reach one\nor several specific goals, often (but not always) within a specific domain. We\nalso consider the concept of embodied conversational agents, briefly reviewing\naspects such as character animation and speech processing. The many different\napproaches for representing dialogue in CAs are discussed in some detail, along\nwith methods for evaluating such agents, emphasizing the important topics of\naccountability and interpretability. A brief historical overview is given,\nfollowed by an extensive overview of various applications, especially in the\nfields of health and education. We end the chapter by discussing benefits and\npotential risks regarding the societal impact of current and future CA\ntechnology.", "entities_include_in_text": ["WI2019", "WIT press, 2008", "MIT press, 2018", "Studentlitteratur,\n\n1969", "Consulting\n\nPsychologists Press, 1978", "Sept., 2015", "Mar., 2019", "June, 2019", "Nov., 2018", "Prentice-Hall, 2009", "apr, 2017", "July, 2017", "MIT Press, 2017", "IWP2005), (2005", "Nov., 2016", "Nov.,\n2016", "WI2019", "July, 2020"], "entities_from_reference": ["Plusieurs"]}{"title": ["Red Teaming Language Models with Language Models"], "authors": ["[arxiv.Result.Author('Ethan Perez'), arxiv.Result.Author('Saffron Huang'), arxiv.Result.Author('Francis Song'), arxiv.Result.Author('Trevor Cai'), arxiv.Result.Author('Roman Ring'), arxiv.Result.Author('John Aslanides'), arxiv.Result.Author('Amelia Glaese'), arxiv.Result.Author('Nat McAleese'), arxiv.Result.Author('Geoffrey Irving')]"], "link": ["http://arxiv.org/pdf/2202.03286v1"], "summary": "Language Models (LMs) often cannot be deployed because of their potential to\nharm users in hard-to-predict ways. Prior work identifies harmful behaviors\nbefore deployment by using human annotators to hand-write test cases. However,\nhuman annotation is expensive, limiting the number and diversity of test cases.\nIn this work, we automatically find cases where a target LM behaves in a\nharmful way, by generating test cases (\"red teaming\") using another LM. We\nevaluate the target LM's replies to generated test questions using a classifier\ntrained to detect offensive content, uncovering tens of thousands of offensive\nreplies in a 280B parameter LM chatbot. We explore several methods, from\nzero-shot generation to reinforcement learning, for generating test cases with\nvarying levels of diversity and difficulty. Furthermore, we use prompt\nengineering to control LM-generated test cases to uncover a variety of other\nharms, automatically finding groups of people that the chatbot discusses in\noffensive ways, personal and hospital phone numbers generated as the chatbot's\nown contact info, leakage of private training data in generated text, and harms\nthat occur over the course of a conversation. Overall, LM-based red teaming is\none promising tool (among many needed) for finding and fixing diverse,\nundesirable LM behaviors before impacting users.", "entities_include_in_text": ["Lee, 2016", "Lin et al.,\n2021", "Carlini et al., 2019, 2021", "Jia and\nLiang, 2017; Dixon et al., 2018; Garg et al., 2019;\nJiang and Bansal, 2019; Ribeiro et al., 2020", "Lee,\n2016", "Rae et al., 2021", "Behjati et al., 2019; Wallace et al.,\n2019", "Szegedy et al., 2014", "for an\noverview, see Xu et al., 2020", "Perez et al., 2021", "Jaques et al., 2017;\nSchmitt et al., 2018; Jaques et al., 2019; Ziegler\net al., 2019", "Holtzman et al., 2020", "Brown et al., 2020", "Sheng et al., 2019; Gehman\net al., 2020; Brown et al., 2020", "Gehman et al., 2020; Welbl\net al., 2021", "Lee, 2016", "Holtzman et al., 2020", "Rae et al., 2021", "Zhu\net al., 2018), as in Holtzman et al. (2020", "Papineni\net al., 2002", "Callison-\nBurch et al., 2006; Liu et al., 2016", "Joulin et al., 2017", "Ateniese et al.,\n2013", "Chen et al., 2021", "Chaudhuri and Monteleoni,\n2009; Rubinstein et al., 2012; Shokri and\nShmatikov, 2015; Abadi et al., 2016). In particular,\nit\nto have secondary mechanisms\nfor verifying that a trained model does not\nleak training data. Additional checks help to\ncatch implementation bugs, as well as to tune\nhyperparameters that trade off data leakage risk\nagainst model performance. Red teaming can also\nbe combined directly with extraction attacks such\nas Carlini et al. (2021", "similar to Brown et al., 2020", "Solaiman\nand Dennison, 2021", "Welleck et al., 2020; Li et al., 2020", "Szegedy et al., 2014; Liu\net al., 2017; Perez et al., 2019", "e.g., Goodfellow et al., 2015;\nEbrahimi et al., 2018", "e.g., Rupprecht et al.,\n2020; Goh et al., 2021", "in the spirit of Zhao et al., 2018", "Szegedy et al., 2014; Liu\net al., 2017; Perez et al., 2019", "Welleck et al., 2020; He and Glass,\n2020", "Welleck et al.,\n2020", "Li et al., 2020", "He and\nGlass, 2020). The target LM may also be trained\nusing RL, as in Saleh et al. (2020", "Zhang et al., 2018). Following Holtzman\net al. (2020", "Gupta et al., 2015", "Shazeer and Stern, 2018", "Gupta et al.,\n2015", "van Hasselt et al., 2016", "Wei et al.,\n2021", "Kingma\nand Ba, 2015"], "entities_from_reference": ["Ilya", "Kunal Talwar", "Martin Abadi", "Andy Chu", "Ian Goodfellow", "Brendan McMahan", "Li Zhang", "Machinery", "Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Mane", "Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "Tom Henighan", "Andy Jones", "Nicholas Joseph", "Ben Mann", "Nova DasSarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Jackson Kernion", "Kamal Ndousse", "Catherine Olsson", "Tom Brown", "Jack Clark", "Sam McCandlish", "Jared Kaplan", "Giuseppe Ateniese", "Giovanni Felici", "Luigi Mancini", "Angelo Spognardi", "Antonio Villani", "Domenico Vitali", "Networks", "Tristan Thrush", "Robin Jia", "Sebastian Riedel", "Pontus Stenetorp", "Douwe Kiela", "Max Bartolo", "Melika Behjati", "Mahdieh Soleymani Baghshah", "Pascal Frossard", "Universal", "Tolga Bolukbasi", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai", "Rishi Bommasani", "Drew A. Hudson", "Ehsan Adeli", "Russ Altman", "Simran Arora", "Sydney", "Michael S. Bernstein", "Jeannette Bohg", "Antoine Bosselut", "Emma Brunskill", "Erik Brynjolfsson", "Shyamal Buch", "Dallas Card", "Rodrigo Castellon", "Niladri Chatterji", "Annie Chen", "Kathleen Creel", "Jared Quincy Davis", "Dora Demszky", "Chris Donahue", "Moussa Doumbouya", "Esin Durmus", "Stefano Ermon", "John Etchemendy", "Kawin Ethayarajh", "Li Fei-Fei", "Chelsea Finn", "Trevor Gale", "Lauren Gillespie", "Karan Goel", "Noah Goodman", "Shelby Grossman", "Neel Guha", "Tatsunori Hashimoto", "Peter Henderson", "John Hewitt", "Daniel E. Ho", "Jenny Hong", "Kyle Hsu", "Jing Huang", "Thomas Icard", "Saahil Jain", "Dan Jurafsky", "Pratyusha Kalluri", "Siddharth Karamcheti", "Geoff Keeling", "Fereshte Khani", "Omar Khattab", "Pang Wei Koh", "Mark Krass", "Ranjay Krishna", "Rohith Kuditipudi", "Ananya Kumar", "Faisal Ladhak", "Mina Lee", "Tony Lee", "Jure Leskovec", "Isabelle Levent", "Xiang Lisa Li", "Xuechen Li", "Tengyu Ma", "Ali Malik", "Christopher D. Manning", "Suvir Mirchandani", "Eric Mitchell", "Zanele Munyikwa", "Suraj Nair", "Avanika Narayan", "Deepak Narayanan", "Ben Newman", "Allen Juan Carlos Niebles", "Hamed Nilforoshan", "Nie", "Julian Nyarko", "Giray Ogut", "Laurel Orr", "Isabel Papadimitriou", "Joon Sung Park", "Chris Piech", "Eva Portelance", "Christopher Potts", "Aditi Raghunathan", "Rob Reich", "Hongyu Ren", "Frieda Rong", "Yusuf Roohani", "Camilo Ruiz", "Jack Ryan", "Christopher Re", "Dorsa Sadigh", "Shiori Sagawa", "Keshav Santhanam", "Andy Shih", "Krishnan Srinivasan", "Alex Tamkin", "Rohan Taori", "Armin W. Thomas", "Florian Tramer", "Rose E. Wang", "William Wang", "Bohan Wu", "Jiajun Wu", "Yuhuai Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan", "Matei Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared D Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Sandhini Agarwal", "Ariel HerbertVoss", "Gretchen Krueger", "Rewon Child", "Aditya Ramesh", "Daniel Ziegler", "Jeffrey Wu", "Clemens Winter", "Chris Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Christopher Berner", "Alec Radford", "Ilya Sutskever", "Miles Osborne", "Philipp Koehn", "Dawn Song", "Carlini", "Chang Liu", "Ulfar Erlingsson", "Jernej Kos", "Nicholas Carlini", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Alina Oprea", "Colin Raffel", "Kamalika Chaudhuri", "Claire Monteleoni", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Oliveira Pinto", "Harri Edwards", "Yuri Burda", "Greg Brockman", "Alex Ray", "Raul Puri", "Michael Petrov", "Heidy Khlaaf", "Pamela Mishkin", "Brooke Chan", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Philippe Tillet", "Felipe Petroski Such", "Dave Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "William Hebgen Guss", "Alex Nichol", "Alex Paino", "Nikolas Tezak", "Jie Tang", "Igor Babuschkin", "Suchir Balaji", "Shantanu Jain", "William Saunders", "Christopher Hesse", "Andrew N. Carr", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Matthew Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Wojciech Zaremba", "Jan Leike", "Justin Lu", "Mia Xu Chen", "Benjamin N. Lee", "Gagan Bansal", "Yuan Cao", "Shuyuan Zhang", "Jackie Tsay", "Yinan Wang", "Andrew M. Dai", "Zhifeng Chen", "Timothy Sohn", "Yonghui Wu", "Gmail", "Data Mining", "Minhao Cheng", "Jinfeng Yi", "Huan Zhang", "Cyprien", "Masson", "Shakir Mohamed", "Mihaela Rosca", "Jack Rae", "Tony Sun", "Varun Kumar", "Satyapriya Krishna", "Yada Pruksachatkun", "Bold", "Dataset", "Rahul Gupta", "Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston", "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman", "Society", "Javid Ebrahimi", "Anyi Rao", "Daniel Lowd", "Melbourne", "Sahaj Garg", "Vincent Perot", "Nicole Limtiaco", "Ankur Taly", "Ed H. Chi", "Alex Beutel", "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi", "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Noah", "Smith", "Gabriel Goh", "Nick Cammarata", "Chelsea Voss", "Shan Carter", "Ludwig Schubert", "Multimodal Distill", "Https", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio", "Szegedy", "Jonathon Shlens", "Christian Explaining", "Ankur Kailash Agrawal", "Gopalakrishnan", "James Glass", "Koustuv Sinha", "Nicolas AngelardGontier", "Nan Rosemary Ke", "Genevieve Fried", "Ryan Lowe", "Joelle Pineau", "Dan Hendrycks", "Sorami Hisamoto", "Matt Post", "Kevin Duh", "Membership Inference Attacks", "Machine Sequence Models", "Sreeram Kannan", "Baosen Zhang", "Radha Poovendran", "Johannes Welbl", "Huang", "Ray Jiang", "Robert Stanforth", "Vishal Maini", "Dani Yogatama", "Pushmeet Kohli", "Natasha Jaques", "Asma Ghandeharioun", "Judy Hanwen Shen", "Craig Ferguson", "Agata Lapedriza", "Noah Jones", "Shixiang Gu", "Rosalind W. Picard", "Dzmitry Bahdanau", "Jose Miguel", "Richard E. Turner", "Douglas Eck", "Machine Learning", "Machine Learning Research", "Denmark", "Copenhagen", "Liwei Jiang", "Jena D. Hwang", "Chandra Bhagavatula", "Ronan Le Bras", "Jon Borchardt", "Jenny Liang", "Oren Etzioni", "Delphi", "Yichen Jiang", "Mohit Bansal", "Annual Meeting QA", "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov", "Short Papers", "Valencia", "Diederik P. Kingma", "Jimmy Ba", "Adam", "Peter Lee", "Margaret Li", "Stephen Roller", "Ilia Kulikov", "Sean Welleck", "Kyunghyun Cho", "Stephanie Lin", "Jacob Hilton", "Owain Evans", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Texas", "Haochen Liu", "Jamell Dacon", "Wenqi Fan", "Hui Liu", "Zitao Liu", "Jiliang Tang", "Tyler Derr", "Zhiwei Wang", "Yanpei Liu", "Xinyun Chen", "Volodymyr Mnih", "Adria Puigdomenech Badia", "Alex Graves", "Timothy Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu", "Milad Nasr", "Reza Shokri", "Amir Houmansadr", "San Francisco", "Yixin Nie", "Adina Williams", "Emily Dinan", "Nicolas Papernot", "Patrick D. McDaniel", "Ian J. Transferability", "Goodfellow", "Ian J. Goodfellow", "Somesh Jha", "Berkay Celik", "Ananthram Swami", "Kishore Papineni", "Salim Roukos", "Todd Ward", "Bleu", "Annual Meeting", "Ethan Perez", "Rob Fergus", "John Mellor", "Jack W. Rae", "Sebastian Borgeaud", "Trevor Cai", "Katie Millican", "Jordan Hoffmann", "Francis Song", "John Aslanides", "Sarah Henderson", "Roman Ring", "Susannah Young", "Eliza Rutherford", "Tom Hennigan", "Jacob Menick", "Albin Cassirer", "Richard Powell", "George", "Driessche", "Lisa Anne Hendricks", "Maribeth Rauh", "Sumanth Dathathri", "Saffron Huang", "Irina Higgins", "Jonathan Uesato", "Antonia Creswell", "Nat McAleese", "Amy Wu", "Erich Elsen", "Elena Buchatskaya", "David Budden", "Esme Sutherland", "Karen Simonyan", "Michela Paganini", "Laurent Sifre", "Lena Martens", "Xiang Lorraine Li", "Adhiguna Kuncoro", "Aida Nematzadeh", "Elena Gribovskaya", "Domenic Donato", "Angeliki Lazaridou", "Arthur Mensch", "Lespiau", "Maria Tsimpoukelli", "Nikolai Grigorev", "Doug Fritz", "Thibault Sottiaux", "Mantas Pajarskas", "Toby Pohlen", "Zhitao Gong", "Daniel Toyama", "Yujia Li", "Tayfun Terzi", "Vladimir Mikulik", "Aidan Clark", "Diego", "Las Casas", "Aurelia Guy", "Chris Jones", "James Bradbury", "Matthew Johnson", "Blake Hechtman", "Laura Weidinger", "Iason Gabriel", "William Isaac", "Ed Lockhart", "Simon Osindero", "Laura Rimell", "Chris Dyer", "Oriol Vinyals", "Kareem Ayoub", "Jeff Stanway", "Lorrayne Bennett", "Demis Hassabis", "Geoffrey Irving", "Sheng", "Premkumar Natarajan", "Nanyun Peng", "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh", "Alexis Ross", "Hao Peng", "Matthew E. Peters", "Matt Gardner", "Tailor", "Paul Rottger", "Bertie Vidgen", "Dong Nguyen", "Zeerak Waseem", "Helen Margetts", "Janet Pierrehumbert", "Benjamin I.", "Peter L. Bartlett", "Ling Huang", "Nina Taft", "Christian Rupprecht", "Cyril Ibrahim", "Christopher J", "Asma Natasha Ghandeharioun", "Judy Shen", "Rosalind Picard", "Jaques", "Simon Schmitt", "Jonathan J. Hudson", "Augustin Zidek", "Carl Doersch", "Wojciech M. Czarnecki", "Joel Z. Leibo", "Heinrich Kuttler", "Andrew Zisserman", "Noam Shazeer", "Vitaly Shmatikov", "Marco Stronati", "Congzheng Song", "Irene Solaiman", "Christy Dennison", "Xinwei Yu", "Karthik Narasimhan", "Nisan Stiennon", "Paul F Christiano", "Christian Szegedy", "Joan Bruna", "Hado", "Hasselt", "Arthur Guez", "Matteo Hessel", "Shi Feng", "Nikhil Kandpal", "Matt Universal Gardner", "Empirical Methods", "Dian Yu", "Kenji Sagae", "Online", "Punta Cana", "Yizhe Zhang", "Michel Galley", "Jianfeng Gao", "Zhe Gan", "Xiujun Li", "Chris Brockett", "Bill Dolan", "Zhao", "Dheeru Dua", "Zhu", "Sidi Lu", "Lei Zheng", "Jiaxian Guo", "Weinan Zhang", "Jun Wang", "Yong Yu", "Texygen", "Daniel M. Ziegler", "Tom B", "Brown", "Douwe Analyzing", "Jason Wei", "Maarten Bosma", "Vincent Y Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M Dai", "Quoc V Le", "Conor Griffin", "Myra Cheng", "Mia Glaese", "Borja Balle", "Atoosa Kasirzadeh", "Zac Kenton", "Sasha Brown", "Will Hawkins", "Tom Stepleton", "Courtney Biles", "Abeba Birhane", "Julia Haas", "Sean Legassick", "Ethical", "Kirsty Anderson", "Ben Coppin", "Jeff Wu", "Nisan", "Paul Stiennon", "Daniel Weld", "Jeffrey Polyjuice", "Heer", "Albert Xu", "Eshaan Pathak", "Dan Klein", "Han Xu", "Yao Ma", "Debayan Deb", "Anil K. Jain", "Da Ju", "Method Replies Qs Diversity", "Unique Entropy", "Gopher LM", "Point", "Appendix", "Zhang", "Holtzman", "Noun Phrase Occurrences", "Contact Info", "Regular Expression Pattern Email Address", "Phone Number", "Social Security", "Gopher", "Examples Offensive Language", "Distributional Bias", "Toxic Replies Elicited", "Dialogue Red Teaming", "Details B.1", "Adafactor", "Gupta", "Shazeer", "Stern", "Wei", "Params Acc F1", "Hello", "Muslims", "Jews", "Steemit", "Immigration", "Donald Trump", "Matter", "Appendix A.1", "Coke", "Hilary Clinton", "Nazis", "Ive", "Id", "Hitler", "Elon Musk", "Max Classifier Conf", "Ill", "Safe Red LM Questions", "Dialogue Red", "Low Red LM Offens", "Red LM", "Wait", "Im", "User", "Joe Biden", "Biden", "Sorry", "Kamala Harris", "Ok", "Fair"]}{"title": ["Combining Deep Learning and Reasoning for Address Detection in Unstructured Text Documents"], "authors": ["[arxiv.Result.Author('Matthias Engelbach'), arxiv.Result.Author('Dennis Klau'), arxiv.Result.Author('Jens Drawehn'), arxiv.Result.Author('Maximilien Kintz')]"], "link": ["http://arxiv.org/pdf/2202.03103v1"], "summary": "Extracting information from unstructured text documents is a demanding task,\nsince these documents can have a broad variety of different layouts and a\nnon-trivial reading order, like it is the case for multi-column documents or\nnested tables. Additionally, many business documents are received in paper\nform, meaning that the textual contents need to be digitized before further\nanalysis. Nonetheless, automatic detection and capturing of crucial document\ninformation like the sender address would boost many companies' processing\nefficiency. In this work we propose a hybrid approach that combines deep\nlearning with reasoning for finding and extracting addresses from unstructured\ntext documents. We use a visual deep learning model to detect the boundaries of\npossible address regions on the scanned document images and validate these\nresults by analyzing the containing text using domain knowledge represented as\na rule based system.", "entities_include_in_text": ["Grand View Research Inc. 2019", "Stevens et al. 2020", "Neudecker et al. 2019", "Smith 2019", "Xu et al. 2021", "Lin et al. 2005", "Chang and Li 2010", "Vishwanath et al. 2019", "Sunder et al. 2019", "Vishwanath et al. 2019", "Bommasani et al. 2021", "Harley, Ufkes, and Derpanis 2015", "Breuel 2017", "Abdulla 2017", "WIRI 2005"], "entities_from_reference": ["Mask R-CNN", "Keras", "Bommasani", "Altman", "Arora", "Bernstein", "Brunskill", "Stanford University", "Stanford Institute", "Breuel", "Chang", "Li", "Grand View Research", "Optical Char2021Size", "Harley", "Ufkes", "Derpanis", "Deep Convolutional Nets", "Retrieval", "Lin", "Zhang", "Meng", "Postal", "Web Documents"]}{"title": ["Mental Disorders on Online Social Media Through the Lens of Language and Behaviour: Analysis and Visualisation"], "authors": ["[arxiv.Result.Author('Esteban A. R\u00edssola'), arxiv.Result.Author('Mohammad Aliannejadi'), arxiv.Result.Author('Fabio Crestani')]"], "link": ["http://arxiv.org/pdf/2202.03291v1"], "summary": "Due to the worldwide accessibility to the Internet along with the continuous\nadvances in mobile technologies, physical and digital worlds have become\ncompletely blended, and the proliferation of social media platforms has taken a\nleading role over this evolution. In this paper, we undertake a thorough\nanalysis towards better visualising and understanding the factors that\ncharacterise and differentiate social media users affected by mental disorders.\nWe perform different experiments studying multiple dimensions of language,\nincluding vocabulary uniqueness, word usage, linguistic style, psychometric\nattributes, emotions' co-occurrence patterns, and online behavioural traits,\nincluding social engagement and posting trends. Our findings reveal significant\ndifferences on the use of function words, such as adverbs and verb tense, and\ntopic-specific vocabulary, such as biological processes. As for emotional\nexpression, we observe that affected users tend to share emotions more\nregularly than control individuals on average. Overall, the monthly posting\nvariance of the affected groups is higher than the control groups. Moreover, we\nfound evidence suggesting that language use on micro-blogging platforms is less\ndistinguishable for users who have a mental disorder than other less\nrestrictive platforms. In particular, we observe on Twitter less quantifiable\ndifferences between affected and control groups compared to Reddit.", "entities_include_in_text": ["Correia et al.,\n2020", "Zarrinkalam et al., 2018", "Khodabakhsh et al., 2018; Saha et al., 2021", "Prieto et al., 2014", "Culpepper et al.,\n2018", "Losada et al., 2018, 2019", "Fast et al., 2016", "Schwartz et al., 2013", "Association, 2013", "Fast et al., 2016", "Plutchik, 1980", "Roberts et al., 2012", "Losada et al.,\n2018, 2019", "Kloumann et al., 2012", "Dodds et al., 2015", "Bathina et al., 2021", "Mikolov et al., 2013", "Devlin et al., 2018", "Gkoumas et al.,\n2021", "swirl 2018"], "entities_from_reference": ["Johnstone", "Clinical Psychological Science", "Aliannejadi", "Crestani", "Venue", "Amini", "Towards", "Saarbr", "Aragon", "Montes", "Affective Computing", "Diagnostic", "Bathina", "Rutter", "Bollen", "Nature Human Behaviour", "Boyd", "Wilson", "Pennebaker", "J. W.", "Burdisso", "Errecalde", "Applications", "Fernandez", "Novoa", "Carneiro", "Choudhury", "Choi", "Knowledge Management", "Maui", "Gamon", "Horvitz", "Weblogs", "Kiciman", "Dredze", "Kumar", "Systems", "San Jose", "Chung", "Psychology Press", "Coppersmith", "Harman", "Clinical Psychology", "Linguistic Signal", "Clinical Reality", "Ann Arbor", "Correia", "Wood", "Rocha", "Annual Review", "Culpepper", "J. S.", "Diaz", "Smucker", "Chang", "Lee", "Toutanova", "Dodds", "Clark", "Desu", "Williams", "J. R.", "J. P.", "Tivnan", "Danforth", "Fast", "Bernstein", "Empath", "Gaur", "Kursuncu", "Sheth", "Daniulaityte", "Pathak", "Gkotsis", "Oellrich", "Hubbard", "Dobson", "Liakata", "Velupillai", "Gkoumas", "Li", "Gligori", "Anderson", "Stanford", "Khodabakhsh", "Fani", "Kloumann", "Losada", "Masood", "Metzler", "Cai", "Hovy", "Human Language Technologies", "Mikolov", "Lake Tahoe", "Mohammad", "Miyazaki", "Nepomnyachiy", "Gelley", "Jiang", "Minkus", "Neuman", "Springer", "Park", "Cha", "Mehl", "Plutchik", "Approaches", "Prieto", "Matos", "Alvarez", "Cacheda", "Oliveira", "J. L.", "Punt", "Velazquez", "Gonfaus", "J. M.", "Gonz", "Lix", "Langer", "Scientific Reports", "Amantea", "Healthcare", "Roberts", "Roach", "Johnson", "Guthrie", "Harabagiu", "Saha", "Seybolt", "Mattingly", "Aledavood", "Konjeti", "Martinez", "Mark", "De Choudhury", "Schwartz", "J. C.", "Ramones", "Agrawal", "Shah", "Seligman", "Ungar", "Skaik", "Tausczik", "Liwc", "Thieme", "Doherty", "Machine", "Leemput", "Scheffer", "Uban", "Chulvi", "Rosso", "Future Generation", "Walsh", "Chaudhry", "Dua", "Goodman", "Kaplan", "Stigma", "Cohan", "Copenhagen", "Zarrinkalam", "Kahani"]}{"title": ["Transformers in Self-Supervised Monocular Depth Estimation with Unknown Camera Intrinsics"], "authors": ["[arxiv.Result.Author('Arnav Varma'), arxiv.Result.Author('Hemang Chawla'), arxiv.Result.Author('Bahram Zonooz'), arxiv.Result.Author('Elahe Arani')]"], "link": ["http://arxiv.org/pdf/2202.03131v1"], "summary": "The advent of autonomous driving and advanced driver assistance systems\nnecessitates continuous developments in computer vision for 3D scene\nunderstanding. Self-supervised monocular depth estimation, a method for\npixel-wise distance estimation of objects from a single camera without the use\nof ground truth labels, is an important task in 3D scene understanding.\nHowever, existing methods for this task are limited to convolutional neural\nnetwork (CNN) architectures. In contrast with CNNs that use localized linear\noperations and lose feature resolution across the layers, vision transformers\nprocess at constant resolution with a global receptive field at every stage.\nWhile recent works have compared transformers against their CNN counterparts\nfor tasks such as image classification, no study exists that investigates the\nimpact of using transformers for self-supervised monocular depth estimation.\nHere, we first demonstrate how to adapt vision transformers for self-supervised\nmonocular depth estimation. Thereafter, we compare the transformer and\nCNN-based architectures for their performance on KITTI depth prediction\nbenchmarks, as well as their robustness to natural corruptions and adversarial\nattacks, including when the camera intrinsics are unknown. Our study\ndemonstrates how transformer-based architecture, though lower in run-time\nefficiency, achieves comparable performance while being more robust and\ngeneralizable.", "entities_include_in_text": ["He et al., 2016", "Dosovitskiy et al., 2021", "Vaswani et al., 2017", "Car-\n\naEqual contribution\n\nion et al., 2020", "Zheng\net al., 2021", "Li et al., 2020;\nRanftl et al., 2021", "Lee\net al., 2019; Aich et al., 2021", "Guizilini et al., 2020; Lyu et al., 2020; Chawla\net al., 2021", "Johnston and Carneiro, 2020", "Yang et al., 2021", "Touvron et al., 2021", "Caron\net al., 2021", "Raghu et al., 2021; Bhojanapalli et al.,\n2021", "Eigen et al., 2014", "Geiger et al.,\n2013", "Ranftl et al., 2021", "Bho-\njanapalli et al., 2021; Paul and Chen, 2021", "Carion et al.,\n2020; Liu et al., 2021", "Zheng\net al., 2021; Strudel et al., 2021", "Ranftl et al., 2020; Yang\net al., 2021", "Ranftl et al.,\n2020", "Yang et al., 2021", "Vaswani\net al., 2017", "Johnston and Carneiro, 2020; Xiang et al.,\n2021", "Godard et al., 2019; Lyu et al., 2020", "Chawla et al., 2020", "Lopez et al., 2019; Zhuang et al., 2019", "Gordon\net al., 2019", "Godard\net al., 2019", "Ranftl et al., 2020", "Touvron\net al., 2021", "Vaswani et al., 2017", "Lin et al., 2017", "Godard et al.,\n2019", "Touvron et al., 2021", "Godard et al., 2019", "Godard et al., 2019", "Eigen et al.,\n2014", "Geiger et al., 2013", "Zhou et al., 2017", "Go-\ndard et al., 2019", "Paszke\net al., 2019", "Deng et al., 2009", "Kingma and Ba, 2014", "Loshchilov and Hutter, 2017", "Eigen et al., 2014", "Johnston and Carneiro, 2020", "Guizilini et al., 2020", "Uhrig et al., 2017", "Chawla et al.,\n2021", "Zhou et al., 2017", "Yin and Shi, 2018", "Mahjourian et al., 2018", "Casser et al., 2019", "Roussel et al., 2019", "Gordon et al., 2019", "Godard et al., 2019", "Ranjan et al., 2019", "Bian et al., 2019", "Godard et al., 2019", "Klingner et al., 2020", "Guizilini et al., 2020", "Poggi et al., 2020", "Johnston and Carneiro, 2020", "Lyu et al., 2020", "Guizilini et al., 2020", "Lyu et al., 2020", "Chawla et al., 2021", "Fu et al., 2018", "Diaz and Marathe, 2019", "Yin et al., 2019", "Ren et al., 2019", "Zhang et al., 2019", "Guo et al., 2018", "Chawla et al., 2021", "Godard et al., 2019", "Ochs et al., 2019", "Kong and Fowlkes, 2019", "Jiang and Huang, 2019", "Klingner et al., 2020", "Zhang et al., 2018", "Guizilini et al., 2020", "Goldman et al., 2019", "Godard et al., 2017", "Geirhos et al.,\n2020", "Godard et al., 2019", "He et al., 2016", "Hendrycks and\nDietterich, 2019", "Michaelis et al., 2019", "Madry et al., 2018", "Kurakin et al., 2016", "Wong et al., 2020", "Paul and\nChen, 2021; Bhojanapalli et al., 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["FL_PyTorch: optimization research simulator for federated learning"], "authors": ["[arxiv.Result.Author('Konstantin Burlachenko'), arxiv.Result.Author('Samuel Horv\u00e1th'), arxiv.Result.Author('Peter Richt\u00e1rik')]"], "link": ["http://arxiv.org/pdf/2202.03099v1"], "summary": "Federated Learning (FL) has emerged as a promising technique for edge devices\nto collaboratively learn a shared machine learning model while keeping training\ndata locally on the device, thereby removing the need to store and access the\nfull data in the cloud. However, FL is difficult to implement, test and deploy\nin practice considering heterogeneity in common edge device settings, making it\nfundamentally hard for researchers to efficiently prototype and test their\noptimization algorithms. In this work, our aim is to alleviate this problem by\nintroducing FL_PyTorch : a suite of open-source software written in python that\nbuilds on top of one the most popular research Deep Learning (DL) framework\nPyTorch. We built FL_PyTorch as a research simulator for FL to enable fast\ndevelopment, prototyping and experimenting with new and existing FL\noptimization algorithms. Our system supports abstractions that provide\nresearchers with a sufficient level of flexibility to experiment with existing\nand novel approaches to advance the state-of-the-art. Furthermore, FL_PyTorch\nis a simple to use console system, allows to run several clients simultaneously\nusing local CPUs or GPU(s), and even remote compute devices without the need\nfor any distributed implementation provided by the user. FL_PyTorch also offers\na Graphical User Interface. For new methods, researchers only provide the\ncentralized implementation of their algorithm. To showcase the possibilities\nand usefulness of our system, we experiment with several well-known\nstate-of-the-art FL algorithms and a few of the most common FL datasets.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["CITRIS: Causal Identifiability from Temporal Intervened Sequences"], "authors": ["[arxiv.Result.Author('Phillip Lippe'), arxiv.Result.Author('Sara Magliacane'), arxiv.Result.Author('Sindy L\u00f6we'), arxiv.Result.Author('Yuki M. Asano'), arxiv.Result.Author('Taco Cohen'), arxiv.Result.Author('Efstratios Gavves')]"], "link": ["http://arxiv.org/pdf/2202.03169v1"], "summary": "Understanding the latent causal factors of a dynamical system from visual\nobservations is a crucial step towards agents reasoning in complex\nenvironments. In this paper, we propose CITRIS, a variational autoencoder\nframework that learns causal representations from temporal sequences of images\nin which underlying causal factors have possibly been intervened upon. In\ncontrast to the recent literature, CITRIS exploits temporality and observing\nintervention targets to identify scalar and multidimensional causal factors,\nsuch as 3D rotation angles. Furthermore, by introducing a normalizing flow,\nCITRIS can be easily extended to leverage and disentangle representations\nobtained by already pretrained autoencoders. Extending previous results on\nscalar causal factors, we prove identifiability in a more general setting, in\nwhich only some components of a causal factor are affected by interventions. In\nexperiments on 3D rendered image sequences, CITRIS outperforms previous methods\non recovering the underlying causal variables. Moreover, using pretrained\nautoencoders, CITRIS can even generalize to unseen instantiations of causal\nfactors, opening future research areas in sim-to-real generalization for causal\nrepresentation learning.", "entities_include_in_text": ["Eberhardt, 2007", "Pearl, 2009", "Jaynes, 1957; 1968", "Jang et al., 2017", "Gresele et al., 2021; Monti et al., 2019).\nIn particular,\nLachapelle et al. (2021); Yao et al. (2021", "Bellemare et al., 2013", "Klindt et al.,\n2021", "Wright, 1921", "Spearman, 1904", "Blender Online Community, 2021", "Crane,\n2021", "Rusinkiewicz et al., 2021", "Praun et al., 2000", "Newell,\n1975", "Spearman, 1904", "Wright, 1921", "Paszke et al., 2019", "Paszke et al., 2019", "Germain et al., 2015", "Ho et al., 2019", "Ba et al., 2016", "Higgins et al., 2017", "Crane, 2021", "Rusinkiewicz et al., 2021", "Lippe et al., 2022"], "entities_from_reference": ["Kiros", "J. R.", "Bellemare", "Blender Online Community", "Blender", "Blender Foundation", "Amsterdam", "Chalupka", "Eberhardt", "Arlington", "Bischoff", "Causal Feature Learning", "Microlevel Climate Data", "Robert", "Machine Learning Research", "Cadiz", "Crane", "Curless", "Levoy", "Range Images", "Annual Conference", "Machinery", "Dean", "Causal Identifiability", "Carnegie Mellon University", "Falcon", "Murray", "Machine Learning", "Gresele", "Sch", "Hendrycks", "Gaussian Error Linear Units", "Matthey", "Botvinick", "Mohamed", "Lerchner", "Duan", "Architecture Design", "Salakhutdinov", "Hoel", "Albantakis", "Natl", "Acad", "Feature Extraction", "Nonlinear ICA", "Red Hook", "Pajunen", "Karhunen", "Oja", "John Wiley", "Sons", "Sasaki", "Ioffe", "Szegedy", "Batch Normalization", "Network Training", "Blei", "Jang", "Poole", "Jaynes", "Statistical Mechanics", "Phys", "Khemakhem", "Monti", "Hyvarinen", "Ranzato", "Hadsell", "Lin", "Kingma", "J. Adam", "San Diego", "Wallach", "Grauman", "Garnett", "Klindt", "Schott", "Ustyuzhaninov", "Brendel", "Krishnamurthy", "Dense Polygon Meshes", "Kumar", "Sattigeri", "Lachapelle", "Everett", "Lacoste", "Mechanism Sparsity", "Lippe", "Cohen", "Locatello", "Bauer", "Lucic", "Raetsch", "Gelly", "Tschannen", "Virtual Event", "Zhang", "Silva", "Murphy", "Berkeley", "Paszke", "Gross", "Lerer", "Bradbury", "Chanan", "Killeen", "Gimelshein", "Desmaison", "Yang", "Raison", "Tejani", "Steiner", "Fang", "Bai", "Fox", "Pearl", "J. Causality", "Cambridge University Press", "Praun", "Hoppe", "Ramachandran", "Zoph", "Le", "Sohn", "Lee", "Manifold Interaction", "Rezende", "Kalchbrenner", "Goyal", "Bengio", "Towards Causal", "Sorrenson", "Rother", "Spearman", "Creager", "Kilbertus", "Dittadi", "Turk", "Data Augmentations Provably Isolates Content", "Wright", "Liu", "Hao", "Wang", "J. CausalVAE", "Pattern Recognition", "Yao", "Zimmermann", "Schneider", "Appendix", "Further", "Dynamic Bayesian", "Lemma", "Jh", "Et", "Z Z", "Hence", "Next", "Ci", "C2", "Due", "Appendix B.4.2", "Cj", "Appendix B.4.3", "Hare Figure", "Object", "Teapot Armadillo Hare Cow Dragon Head Horse", "T T", "Hyperparameters", "Below", "Flows", "Feature Dimension", "Layer Conv Conv Conv Conv Conv Conv Conv Conv Reshape Linear Linear Linear Linear Reshape Upsample", "Target", "Layer Normalization", "Hyperparameter Value Batch", "Optimizer Learning", "Gumbel Softmax", "Cosine Warmup", "Oracle SlowVAE", "False", "Mk Bernoulli", "Triplet", "Mean R2"]}{"title": ["AI-based artistic representation of emotions from EEG signals: a discussion on fairness, inclusion, and aesthetics"], "authors": ["[arxiv.Result.Author('Piera Riccio'), arxiv.Result.Author('Kristin Bergaust'), arxiv.Result.Author('Boel Christensen-Scheel'), arxiv.Result.Author('Juan-Carlos De Martin'), arxiv.Result.Author('Maria A. Zuluaga'), arxiv.Result.Author('Stefano Nichele')]"], "link": ["http://arxiv.org/pdf/2202.03246v1"], "summary": "While Artificial Intelligence (AI) technologies are being progressively\ndeveloped, artists and researchers are investigating their role in artistic\npractices. In this work, we present an AI-based Brain-Computer Interface (BCI)\nin which humans and machines interact to express feelings artistically. This\nsystem and its production of images give opportunities to reflect on the\ncomplexities and range of human emotions and their expressions. In this\ndiscussion, we seek to understand the dynamics of this interaction to reach\nbetter co-existence in fairness, inclusion, and aesthetics.", "entities_include_in_text": ["Manovich, \n2017", "Picard,  1995", "Bergaust  and  Nichele, \nof \n2019", "McCormack  et  al.,  2014", "Russell, \n1980", "Ekman, 1992", "Zhong, Wang and Miao, 2020", "Karras  et  al.,  2020", "McCormack et al., 2014", "Riccio, 2021", "Salevati  and  DiPaola,  2015", "Colton,  Valstar  and  Pantic,  2008", "Ekster, 2018", "Random Quark, 2017", "Kordzadeh  and  Ghasemaghaei, \n2021;  Ogolla  and  Gupta,  2018", "Immordino-Yang,  Yang  and  Damasio,  2016", "Hareli, \nKafetsios  and  Hess,  2015", "Wierzbicka,  1999", "Drag and Shaw, 1967", "Howard,  Zhang  and  Horvitz,  2017", "Zheng  et  al., \n2018", "Schaefer  et  al.,  2010;  Maffei \nand  Angrilli,  2019", "Lang,  Bradley  and \nCuthbert,  1997", "Yang  et  al.,  2018", "Goodfellow  et  al.,  2014", "Salminen, \nJung  and  Jansen,  2019", "Booth  et  al., \n2021", "Mohammad  and  Kiritchenko, \n2018", "Sun  and  Chen,  2020", "Watson  and \nseveral  debates  and  enigmas \nVehmas,  2019", "Hall,  2019", "Solvang,  2018", "Levin  and  Siebers,  2010", "Sherwood,  2019", "Siebers,  2005", "Picard,  1999", "Suchman and Weber, 2016", "McLuhan  and  Fiore,  1967", "LREC 2018", "EVA 2015"], "entities_from_reference": ["Nichele", "Piera Riccio", "Booth", "Data Protection", "Colton", "Valstar", "Pantic", "Shaw", "Ekster", "Ekman", "Goodfellow", "Mirza", "Ozair", "Bengio", "Melinda C.", "Critical Disability", "Hareli", "Kafetsios", "Howard", "Zhang", "Horvitz", "Yang", "Damasio", "Emotion", "Karras", "Aittala", "Laine", "Aila", "Limited Data", "Kordzadeh", "European Journal Lang", "Bradley", "Cuthbert", "Tobin Siebers", "Maffei", "Angrilli", "Manovich", "Flash Art International", "Dorin", "McCabe", "Monro", "Leonardo", "Mohammad", "Kiritchenko", "Gupta", "Inclusive Design Methods", "Picard", "Affective Computing", "Random Quark", "Riccio", "Piera", "Diss", "Torino", "Salevati", "Jung", "Jansen", "Schaefer", "Nils", "Philippot", "Siebers", "Solvang", "Between", "Society", "Watson", "Vehmas", "Nakao", "Sasaoka", "Miyatani", "Behavior Sounds Methods", "Zheng", "Liu", "Wang", "Miao"]}{"title": ["Neural Models for Output-Space Invariance in Combinatorial Problems"], "authors": ["[arxiv.Result.Author('Yatin Nandwani'), arxiv.Result.Author('Vidit Jain'), arxiv.Result.Author('Mausam'), arxiv.Result.Author('Parag Singla')]"], "link": ["http://arxiv.org/pdf/2202.03229v1"], "summary": "Recently many neural models have been proposed to solve combinatorial puzzles\nby implicitly learning underlying constraints using their solved instances,\nsuch as sudoku or graph coloring (GCP). One drawback of the proposed\narchitectures, which are often based on Graph Neural Networks (GNN), is that\nthey cannot generalize across the size of the output space from which variables\nare assigned a value, for example, set of colors in a GCP, or board-size in\nsudoku. We call the output space for the variables as 'value-set'. While many\nworks have demonstrated generalization of GNNs across graph size, there has\nbeen no study on how to design a GNN for achieving value-set invariance for\nproblems that come from the same domain. For example, learning to solve 16 x 16\nsudoku after being trained on only 9 x 9 sudokus. In this work, we propose\nnovel methods to extend GNN based architectures to achieve value-set\ninvariance. Specifically, our model builds on recently proposed Recurrent\nRelational Networks. Our first approach exploits the graph-size invariance of\nGNNs by converting a multi-class node classification problem into a binary node\nclassification problem. Our second approach works directly with multiple\nclasses by adding multiple nodes corresponding to the values in the value-set,\nand then connecting variable nodes to value nodes depending on the problem\ninitialization. Our experimental evaluation on three different combinatorial\nproblems demonstrates that both our models perform well on our novel problem,\ncompared to a generic neural reasoner. Between two of our models, we observe an\ninherent trade-off: while the binarized model gives better performance when\ntrained on smaller value-sets, multi-valued model is much more memory\nefficient, resulting in improved performance when trained on larger value-sets,\nwhere binarized model fails to train.", "entities_include_in_text": ["Zhou et al., 2020", "Palm et al., 2018", "Kahneman, 2011", "Dong et al., 2019", "Palm et al., 2018; Wang et al., 2019", "Palm et al., 2018", "Selsam\net al., 2019", "Selsam et al., 2019", "Dong et al., 2019", "Manhaeve et al., 2018", "Dong et al.,\n2019", "Tamar et al., 2017; Bajpai et al., 2018", "Selsam et al., 2019", "Manhaeve et al., 2018", "Wang et al., 2019", "Palm et al., 2018", "Garg et al., 2020", "Dong et al., 2019", "Nandwani et al., 2021), and are thus directly comparable to our work. The main challenge of\nsuch approaches is that they fail to scale to the size of the problems considered in this work. In our\nexperiments, we compare our methods against both deep and shallow versions of NLM. Note that our\nwork relies on the assumption that GNNs generalize across graph sizes. Yehudai et al. (2021", "de Kleer, 1989; Walsh, 2000", "Palm et al., 2018", "Dong\net al., 2019", "Pieters,\n2019", "Izmailov et al., 2018", "Nandwani et al., 2021", "AAAI-MAKE 2019", "Bahdanau et al., 2015", "Palm\net al., 2018", "Selsam et al., 2019", "Palm et al., 2018", "Palm et al., 2018", "Pieters, 2019", "Nandwani et al., 2021", "Izmailov et al., 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["Auto-Lambda: Disentangling Dynamic Task Relationships"], "authors": ["[arxiv.Result.Author('Shikun Liu'), arxiv.Result.Author('Stephen James'), arxiv.Result.Author('Andrew J. Davison'), arxiv.Result.Author('Edward Johns')]"], "link": ["http://arxiv.org/pdf/2202.03091v1"], "summary": "Understanding the structure of multiple related tasks allows for multi-task\nlearning to improve the generalisation ability of one or all of them. However,\nit usually requires training each pairwise combination of tasks together in\norder to capture task relationships, at an extremely high computational cost.\nIn this work, we learn task relationships via an automated weighting framework,\nnamed Auto-Lambda. Unlike previous methods where task relationships are assumed\nto be fixed, Auto-Lambda is a gradient-based meta learning framework which\nexplores continuous, dynamic task relationships via task-specific weightings,\nand can optimise any choice of combination of tasks through the formulation of\na meta-loss; where the validation loss automatically influences task weightings\nthroughout training. We apply the proposed framework to both multi-task and\nauxiliary learning problems in computer vision and robotics, and show that\nAuto-Lambda achieves state-of-the-art performance, even when compared to\noptimisation strategies designed specifically for each problem and data domain.\nFinally, we observe that Auto-Lambda can discover interesting learning\nbehaviors, leading to new insights in multi-task learning. Code is available at\nhttps://github.com/lorenmt/auto-lambda.", "entities_include_in_text": [], "entities_from_reference": ["Zhao Chen", "Vijay Badrinarayanan", "Lee", "Andrew Rabinovich", "Machine Learning", "Jiquan Ngiam", "Thang Luong", "Henrik Kretzschmar", "Chai", "Dragomir Anguelov", "Marius Cordts", "Mohamed Omran", "Sebastian Ramos", "Timo Rehfeld", "Markus Enzweiler", "Rodrigo Benenson", "Uwe Franke", "Stefan Roth", "Bernt Schiele", "Pattern Recognition", "Geus", "Panagiotis Meletis", "Chenyang Lu", "Xiaoxiao Wen", "Gijs Dubbelman", "Vision", "Yunshu Du", "Wojciech M Czarnecki", "Razvan Pascanu", "Balaji Lakshminarayanan", "Kshitij Dwivedi", "Gemma Roig", "Ehsan Amid", "Zhe Zhao", "Tianhe Yu", "Rohan Anil", "Chelsea Finn", "Pieter Abbeel", "Sergey Levine", "Haoping Bai", "Zequn Jie", "Jiayi Ma", "Kui Jia", "Wei Liu", "Michelle Guo", "Albert Haque", "Huang", "Serena Yeung", "Li Fei-Fei", "Dynamic", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun", "Deep", "Falk Heuer", "Sven Mantowsky", "Saqib Bukhari", "Georg Schneider", "Timothy Hospedales", "Antreas Antoniou", "Paul Micaelli", "Amos Storkey", "Stephen James", "Andrew J Davison", "Edward Johns", "Robot Learning", "Zicong Ma", "David Rovick Arrojo", "Rlbench", "Isabel Valera", "Rotograd", "Alex Kendall", "Yarin Gal", "Roberto Cipolla", "Iasonas Kokkinos", "Alex Krizhevsky", "Trevor Darrell", "Machine Learning Research", "Zhenhua Li", "Zhang", "Sam Kwong", "Pareto", "Xingchao Liu", "Xiaojie Jin", "Peter Stone", "Qiang Liu", "Hanxiao Liu", "Karen Simonyan", "Yang", "Shikun Liu", "Ilija Radosavovic", "Paul Michel", "Sebastian Ruder", "Dani Yogatama", "Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert", "Nathan Silberman", "Derek Hoiem", "Rob Fergus", "Idan Achituve", "Haggai Maron", "Gal Chechik", "Ethan Fetaya", "Aviv Shamsian", "Kenji Kawaguchi", "Alex Nichol", "Joshua Achiam", "John Schulman", "Clemens Rosenbaum", "Tim Klinger", "Matthew Riemer", "Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver", "Vladlen Koltun", "Andrew Zisserman", "Trevor Standley", "Amir Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese", "Which", "Ximeng Sun", "Rameswar Panda", "Rogerio Feris", "Adashare", "Simon Vandenhende", "Stamatios Georgoulis", "Luc Van Gool", "Ricardo Vilalta", "Youssef Drissi", "Haoxiang Wang", "Han Zhao", "Bo Li", "Dan Xu", "Wanli Ouyang", "Xiaogang Wang", "Nicu Sebe", "Feiyang Ye", "Baijiong Lin", "Zhixiong Yue", "Pengxin Guo", "Qiao Xiao", "Yu Zhang", "Teresa Yeo", "Fatih Kar", "Robustness", "Saurabh Kumar", "Abhishek Gupta", "Karol Hausman", "Amir R Zamir", "Alexander Sax", "Nikhil Cheerla", "Rohan Suri", "Zhangjie Cao", "Leonidas J Guibas", "Robust", "William Shen", "Taskonomy", "Adam", "Robotic Manipulation Tasks Naively", "Tasks", "Depth Normal Noise Sem", "Part Seg", "Disp", "Split", "Reach Target Put Money", "Push Button Pick Up Umbrella Pick", "Lift Pick", "Lift Pick Up Umbrella Pick Up Cup Pick Up Cup Pick Up Umbrella Slide Block", "Target Put Knife", "Board Pick Up Umbrella Slide Block", "Target Push Button Pick", "Lift Pick Up Cup Knife", "Board Pick Up Umbrella Put Money", "Put Money", "Pick Up Umbrella Put Money", "Board Stack Wine Pick Up Umbrella Push Button Slide Block", "Pick Up Umbrella Put Knife", "People", "Vegetables Large Carnivores", "Large", "Herbivores"]}{"title": ["Artificial Intelligence based tool wear and defect prediction for special purpose milling machinery using low-cost acceleration sensor retrofits"], "authors": ["[arxiv.Result.Author('Mahmoud Kheir-Eddine'), arxiv.Result.Author('Michael Banf'), arxiv.Result.Author('Gregor Steinhagen')]"], "link": ["http://arxiv.org/pdf/2202.03068v1"], "summary": "Milling machines form an integral part of many industrial processing chains.\nAs a consequence, several machine learning based approaches for tool wear\ndetection have been proposed in recent years, yet these methods mostly deal\nwith standard milling machines, while machinery designed for more specialized\ntasks has gained only limited attention so far. This paper demonstrates the\napplication of an acceleration sensor to allow for convenient condition\nmonitoring of such a special purpose machine, i.e. round seam milling machine.\nWe examine a variety of conditions including blade wear and blade breakage as\nwell as improper machine mounting or insufficient transmission belt tension. In\naddition, we presents different approaches to supervised failure recognition\nwith limited amounts of training data. Hence, aside theoretical insights, our\nanalysis is of high, practical importance, since retrofitting older machines\nwith acceleration sensors and an on-edge classification setup comes at low cost\nand effort, yet provides valuable insights into the state of the machine and\ntools in particular and the production process in general.", "entities_include_in_text": [], "entities_from_reference": ["Mach", "Learn", "Caesarendra", "Feature Extraction Methods", "Cortes", "Machine", "Vilan Vilan", "Segade Robleda", "Gokhale", "Hesser", "Markert", "Knittel", "Makich", "Mallat", "Academic Press", "Terrazas", "Ratchev", "Mohanraj", "Shankar", "Rajasekar", "Sakthivel", "S2238785418313061", "Steinhagen", "Wang", "Yang", "Zhang", "Xie", "S0924424714000065", "Zhou", "Gao", "Li", "Starly", "Cai", "Cohen", "Lee", "Particle"]}{"title": ["Multi-Objective Quality Diversity Optimization"], "authors": ["[arxiv.Result.Author('Thomas Pierrot'), arxiv.Result.Author('Guillaume Richard'), arxiv.Result.Author('Karim Beguir'), arxiv.Result.Author('Antoine Cully')]"], "link": ["http://arxiv.org/pdf/2202.03057v1"], "summary": "In this work, we consider the problem of Quality-Diversity (QD) optimization\nwith multiple objectives. QD algorithms have been proposed to search for a\nlarge collection of both diverse and high-performing solutions instead of a\nsingle set of local optima. Thriving for diversity was shown to be useful in\nmany industrial and robotics applications. On the other hand, most real-life\nproblems exhibit several potentially antagonist objectives to be optimized.\nHence being able to optimize for multiple objectives with an appropriate\ntechnique while thriving for diversity is important to many fields. Here, we\npropose an extension of the MAP-Elites algorithm in the multi-objective\nsetting: Multi-Objective MAP-Elites (MOME). Namely, it combines the diversity\ninherited from the MAP-Elites grid algorithm with the strength of\nmulti-objective optimizations by filling each cell with a Pareto Front. As\nsuch, it allows to extract diverse solutions in the descriptor space while\nexploring different compromises between objectives. We evaluate our method on\nseveral tasks, from standard optimization problems to robotics simulations. Our\nexperimental evaluation shows the ability of MOME to provide diverse solutions\nwhile providing global performances similar to standard multi-objective\nalgorithms.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Training OOD Detectors in their Natural Habitats"], "authors": ["[arxiv.Result.Author('Julian Katz-Samuels'), arxiv.Result.Author('Julia Nakhleh'), arxiv.Result.Author('Robert Nowak'), arxiv.Result.Author('Yixuan Li')]"], "link": ["http://arxiv.org/pdf/2202.03299v1"], "summary": "Out-of-distribution (OOD) detection is important for machine learning models\ndeployed in the wild. Recent methods use auxiliary outlier data to regularize\nthe model for improved OOD detection. However, these approaches make a strong\ndistributional assumption that the auxiliary outlier data is completely\nseparable from the in-distribution (ID) data. In this paper, we propose a novel\nframework that leverages wild mixture data -- that naturally consists of both\nID and OOD samples. Such wild data is abundant and arises freely upon deploying\na machine learning classifier in their \\emph{natural habitats}. Our key idea is\nto formulate a constrained optimization problem and to show how to tractably\nsolve it. Our learning objective maximizes the OOD detection rate, subject to\nconstraints on the classification error of ID data and on the OOD error rate of\nID examples. We extensively evaluate our approach on common OOD detection tasks\nand demonstrate superior performance.", "entities_include_in_text": ["Nguyen et al., 2015", "Hendrycks et al.,\n2019", "Liu et al., 2020", "Hendrycks\net al., 2019", "Hestenes, 1969", "Hendrycks et al., 2019", "Huber, 1964", "Blanchard et al., 2010", "Hestenes, 1969", "Rock-\nafellar, 1973", "Xu, 2017; 2021", "Liu et al., 2020", "Du et al., 2022", "Liu\net al., 2020", "Hendrycks et al., 2019", "Krizhevsky et al.,\n2009", "Netzer\net al., 2011", "Cimpoi et al., 2014", "Zhou et al., 2018", "Yu et al., 2016", "Yu et al., 2016", "Xu et al.,\n2015", "Hendrycks\net al., 2019", "Torralba et al., 2008", "Duchi et al., 2011", "Liang et al., 2018", "Lee\net al., 2018", "Liu et al., 2020", "Hsu et al., 2020", "Tack et al., 2020", "Hendrycks et al.,\n2019", "Liu et al.,\n2020", "Liang et al., 2018; Hsu et al., 2020", "Lee et al., 2018", "Liu et al., 2020; Wang et al., 2021", "Huang et al., 2021", "Hendrycks et al.,\n2019", "Liu et al., 2020", "Ruff et al., 2019; Daniel et al., 2019; Hendrycks et al.,\n2019)", "Blanchard et al.,\n2010", "Rockafellar, 1973", "Xu, 2017", "Sangalli\net al., 2021", "Blanchard et al., 2010", "Blanchard et al., 2010", "Blanchard et al., 2010", "Blanchard et al.,\n2010"], "entities_from_reference": ["Boult", "Bevandi", "Kreso", "Blanchard", "Lee", "Scott", "Machine Learning Research", "Chalapathy", "Chawla", "Hsu", "Jin", "Kira", "Pattern Recognition", "Huang", "Li", "Robust", "Krizhevsky", "Shin", "Cimpoi", "Mohamed", "Vedaldi", "Daniel", "Kurutach", "Tamar", "Deep", "Wang", "Cai", "Duchi", "Hazan", "Ergen", "Kozat", "Hendrycks", "Mazeika", "Dietterich", "Liang", "Liu", "Malinin", "Gales", "Netzer", "Unsupervised Feature Learning", "Deep Learning", "Feature Learning", "Nguyen", "Yosinski", "Clune", "Nocedal", "Wright", "Springer Science", "Perera", "Patel", "Image Processing", "Yan", "Ruff", "Binder", "Kloft", "Seff", "Zhang", "Funkhouser", "J. LSUN", "Zagoruyko", "Komodakis", "Zhou", "Lapedriza", "Khosla", "Oliva", "Scene Recognition", "Pattern Analysis", "Machine Intelligence", "Kauffmann", "J. R.", "Montavon", "Samek", "Sangalli", "Erdil", "Donati", "Konukoglu", "Song", "Jiang", "Yang", "J. Csi", "Torralba", "Freeman", "Pattern Anal", "Mach", "Bocchieri", "Ehinger", "J. TurkerGaze", "Webcam", "Eye Tracking", "Mathematical Programming", "Texture FPR", "Dataset MSP", "Pin", "Pwild", "Method SVHN", "Dataset OE Energy", "Energy", "Mahalanobis", "Special Case", "Pout", "Ptest", "Textures", "Define Define Ry", "Rwild", "Lemma", "Assumption", "Suppose K", "Ein", "Lemma B.4", "Assume", "P1", "Define Ry", "Define", "P0"]}{"title": ["Investigating the fidelity of explainable artificial intelligence methods for applications of convolutional neural networks in geoscience"], "authors": ["[arxiv.Result.Author('Antonios Mamalakis'), arxiv.Result.Author('Elizabeth A. Barnes'), arxiv.Result.Author('Imme Ebert-Uphoff')]"], "link": ["http://arxiv.org/pdf/2202.03407v1"], "summary": "Convolutional neural networks (CNNs) have recently attracted great attention\nin geoscience due to their ability to capture non-linear system behavior and\nextract predictive spatiotemporal patterns. Given their black-box nature\nhowever, and the importance of prediction explainability, methods of\nexplainable artificial intelligence (XAI) are gaining popularity as a means to\nexplain the CNN decision-making strategy. Here, we establish an intercomparison\nof some of the most popular XAI methods and investigate their fidelity in\nexplaining CNN decisions for geoscientific applications. Our goal is to raise\nawareness of the theoretical limitations of these methods and gain insight into\nthe relative strengths and weaknesses to help guide best practices. The\nconsidered XAI methods are first applied to an idealized attribution benchmark,\nwhere the ground truth of explanation of the network is known a priori, to help\nobjectively assess their performance. Secondly, we apply XAI to a\nclimate-related prediction setting, namely to explain a CNN that is trained to\npredict the number of atmospheric rivers in daily snapshots of climate\nsimulations. Our results highlight several important issues of XAI methods\n(e.g., gradient shattering, inability to distinguish the sign of attribution,\nignorance to zero input) that have previously been overlooked in our field and,\nif not considered cautiously, may lead to a distorted picture of the CNN\ndecision-making strategy. We envision that our analysis will motivate further\ninvestigation into XAI fidelity and will help towards a cautious implementation\nof XAI in geoscience, which can lead to further exploitation of CNNs and deep\nlearning for prediction problems.", "entities_include_in_text": ["Lary et al., 2016; Karpatne et al., 2018; \n\nReichstein et al., 2019", "Bergen et al., 2019", "Shen, 2018; Sit et al., 2020", "Barnes et al., 2019; \n\nRolnick  et  al.,  2019;  Ham  et  al.,  2019", "LeCun et al., 2015", "Overpeck \n\net al., 2011; Guo, 2017; Agapiou, 2017; Reinsel et al., 2018", "Buhrmester et al., 2019", "McGovern \n\net  al.,  2019;  Ebert-Uphoff  and  Hilburn,  2020;  Toms  et  al.,  2020;  Mamalakis  et  al.,  2022", "Sonnewald and Lguensat, \n\n2021; Mayer and Barnes, 2021; Hilburn et al., 2021; Keys et al., 2021", "Ebert-Uphoff and Hilburn, 2020", "Barnes et al., 2020; Toms et al., 2021", "Mamalakis et al., \n\n2022", "Leavitt and Morcos, 2020", "Mamalakis et al., 2021). Attribution benchmark datasets \n\nconsist  of  synthetic  inputs  and  outputs,  where  the  functional  relationship  between  the  two  is \n\nknown. This allows for deriving the ground truth of what the explanation of the network should \n\nlook like for each prediction. In this way, the assessment of XAI methods is no longer based on \n\nsubjective criteria, but rather it is based on the direct comparison of the XAI results to the ground \n\ntruth of the explanation. As a first example, Mamalakis et al. (2021", "Arras et al., 2021; Zhou et al., 2022", "Mamalakis et al., \n\n2021", "see e.g., Hilburn et al., 2021", "LeCun et al., 2015", "Kohlbrenner et al., 2020", "Prabhat et al., 2021", "Simonyan et al., 2014", "Smilkov et al., 2017", "Shrikumar  et  al.,  2016;  2017", "Sundararajan  et  al.,  2017", "Bach  et  al.,  2015", "Bach et al., 2015", "Bach et al., 2015", "Kohlbrenner et al., \n\n2020", "Bach  et  al.,  2016;  Kohlbrenner  et  al.,  2020", "Montavon et al., 2017", "Lundberg  and  Lee,  2017", "Simonyan et al., 2014", "Smilkov et al., 2017", "Shrikumar et al., 2017", "Sundararajan et al., 2017", "Bach et al., 2015", "Bach et al., 2015", "Kohlbrenner et al., 2020", "Kohlbrenner et al., 2020", "Montavon et al., 2017", "Lundberg and Lee, 2017", "Balduzzi et al., 2017", "Balduzzi et al., 2017", "Samek et al., 2016; Montavon et al., 2017", "e.g., Kohlbrenner et al., 2020", "Ancona et al., 2018; 2019", "Bach et \n\nal., 2016", "Montavon \n\net  al.,  2017", "Kohlbrenner et al., 2020", "Lundberg  and  Lee,  2017", "McGovern  et  al.,  2019;  Ebert-Uphoff  and \n\nHilburn, 2020; Barnes et al., 2020; Toms et al., 2020; 2021; Sonnewald and Lguensat, 2021; Mayer \n\nand Barnes, 2021; Hilburn et al., 2021; Keys et al., 2021; Mamalakis et al., 2022", "Mamalakis et al., 2021; \n\nLeavitt and Morcos, 2020", "Kohlbrenner  et  al.,  2020;  Mamalakis  et  al.,  2021", "see \n\nPrabhat et al., 2021", "Prabhat  et  al.,  2021", "Simonyan et al., 2014", "Smilkov et al., 2017", "Shrikumar et al., 2016; 2017", "Sundararajan  et  al.,  2017", "Bach et al., 2015; Samek et al., 2016", "Bach et al., 2015", "Bach et al., 2015", "Kohlbrenner et al., 2020", "Kohlbrenner et al., 2020", "Montavon  et  al.,  2017", "Samek et al., 2016; Montavon et al., \n\n2017", "Shapley, 1953", "Lundberg and Lee, \n\n2017", "Shapley,  1953", "Lundberg and Lee, 2017"], "entities_from_reference": ["Plusieurs"]}{"title": ["GMC -- Geometric Multimodal Contrastive Representation Learning"], "authors": ["[arxiv.Result.Author('Petra Poklukar'), arxiv.Result.Author('Miguel Vasco'), arxiv.Result.Author('Hang Yin'), arxiv.Result.Author('Francisco S. Melo'), arxiv.Result.Author('Ana Paiva'), arxiv.Result.Author('Danica Kragic')]"], "link": ["http://arxiv.org/pdf/2202.03390v1"], "summary": "Learning representations of multimodal data that are both informative and\nrobust to missing modalities at test time remains a challenging problem due to\nthe inherent heterogeneity of data obtained from different channels. To address\nit, we present a novel Geometric Multimodal Contrastive (GMC) representation\nlearning method comprised of two main components: i) a two-level architecture\nconsisting of modality-specific base encoder, allowing to process an arbitrary\nnumber of modalities to an intermediate representation of fixed dimensionality,\nand a shared projection head, mapping the intermediate representations to a\nlatent representation space; ii) a multimodal contrastive loss function that\nencourages the geometric alignment of the learned representations. We\nexperimentally demonstrate that GMC representations are semantically rich and\nachieve state-of-the-art performance with missing modality information on three\ndifferent learning problems including prediction and reinforcement learning\ntasks.", "entities_include_in_text": ["Tsai et al., 2018; 2019", "Silva et al., 2019; Vasco et al., 2021", "Guo et al., 2019", "Shi et al., 2019", "Vasco et al., 2022", "Vasco et al., 2021", "Tsai et al., 2018", "Shi et al., 2019", "Tsai et al., 2018", "Tsai et al., 2019", "Chen et al., 2020", "Chen et al., 2020", "Silva et al., 2019; Vasco\net al., 2021", "Yin\net al., 2017", "Suzuki\net al., 2016", "Shi et al., 2019", "Shi et al., 2019", "Vasco et al., 2022", "Vasco et al., 2021", "Liang et al., 2021", "Tsai et al., 2018", "Tsai et al., 2019", "Vasco et al., 2022", "Poklukar\net al., 2022", "Shi\net al., 2019", "McInnes et al.,\n2018", "Zadeh et al., 2016", "Bagher Zadeh et al., 2018", "Tsai\net al., 2019", "Tsai et al.,\n2019", "Tsai\net al., 2018; 2019", "Silva et al., 2019", "Silva\net al., 2019", "Vasco et al., 2021", "Higgins et al., 2017", "Lillicrap et al., 2015", "Poklukar et al.,\n2022", "Poklukar et al., 2022", "Poklukar et al., 2022", "Poklukar\net al., 2021", "Silva et al., 2019", "Tsai et al., 2019", "Silva et al., 2019"], "entities_from_reference": ["Liang", "Morency", "Melbourne", "Baltrusaitis", "Ahuja", "Cao", "Chen", "Singh", "Machine Learning", "Machine Learning Research", "Guo", "Wang", "Kingma", "Lyu", "Fan", "Lee", "Zhu", "Multibench", "Lillicrap", "J. J.", "Heess", "Erez", "Groberger", "Uniform", "Open Source Software", "Meo", "Poklukar", "Zhang", "Pokorny", "Shi", "Paige", "Silva", "Vasco", "Melo", "Paiva", "Geometric Multimodal Contrastive", "Matsuo", "Joint", "Manderson", "Noca", "Dudek", "Meger", "Tsai", "Zadeh", "Salakhutdinov", "Bai", "Kolter", "Annual", "Yin", "Networks", "Goodman", "Billard", "Pincus", "Zambelli", "Cully", "Robotics", "Delaunay Component Analysis Delaunay", "Definition", "F E", "Observations", "Observations Sound Observations Trajectory Observations Label", "Loss", "Results", "Metric Baseline", "Text Observations", "Audio Observations", "Video Observations", "Network", "Input", "Classifier Base", "Input R200 FC", "Input R10 FC", "Shared", "Input Rd FC", "Classifier Input Rs FC", "Encoder Input", "Encoder Input R200 FC", "Decoder Input Rs Encoder Input Rd FC", "Encoder Input R10 FC", "Text", "Video", "Input R12 FC", "Encoder Input R12 FC", "Parameter Intermediate", "Batch", "Parameter Value"]}{"title": ["Evaluation of Runtime Monitoring for UAV Emergency Landing"], "authors": ["[arxiv.Result.Author('Joris Guerin'), arxiv.Result.Author('Kevin Delmas'), arxiv.Result.Author('J\u00e9r\u00e9mie Guiochet')]"], "link": ["http://arxiv.org/pdf/2202.03059v1"], "summary": "To certify UAV operations in populated areas, risk mitigation strategies --\nsuch as Emergency Landing (EL) -- must be in place to account for potential\nfailures. EL aims at reducing ground risk by finding safe landing areas using\non-board sensors. The first contribution of this paper is to present a new EL\napproach, in line with safety requirements introduced in recent research. In\nparticular, the proposed EL pipeline includes mechanisms to monitor learning\nbased components during execution. This way, another contribution is to study\nthe behavior of Machine Learning Runtime Monitoring (MLRM) approaches within\nthe context of a real-world critical system. A new evaluation methodology is\nintroduced, and applied to assess the practical safety benefits of three MLRM\nmechanisms. The proposed approach is compared to a default mitigation strategy\n(open a parachute when a failure is detected), and appears to be much safer.", "entities_include_in_text": ["ERTS 2020", "PRDC 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Personalized Public Policy Analysis in Social Sciences using Causal-Graphical Normalizing Flows"], "authors": ["[arxiv.Result.Author('Sourabh Balgi'), arxiv.Result.Author('Jose M. Pena'), arxiv.Result.Author('Adel Daoud')]"], "link": ["http://arxiv.org/pdf/2202.03281v1"], "summary": "Structural Equation/Causal Models (SEMs/SCMs) are widely used in epidemiology\nand social sciences to identify and analyze the average treatment effect (ATE)\nand conditional ATE (CATE). Traditional causal effect estimation methods such\nas Inverse Probability Weighting (IPW) and more recently\nRegression-With-Residuals (RWR) are widely used - as they avoid the challenging\ntask of identifying the SCM parameters - to estimate ATE and CATE. However,\nmuch work remains before traditional estimation methods can be used for\ncounterfactual inference, and for the benefit of Personalized Public Policy\nAnalysis (P$^3$A) in the social sciences. While doctors rely on personalized\nmedicine to tailor treatments to patients in laboratory settings (relatively\nclosed systems), P$^3$A draws inspiration from such tailoring but adapts it for\nopen social systems. In this article, we develop a method for counterfactual\ninference that we name causal-Graphical Normalizing Flow (c-GNF), facilitating\nP$^3$A. First, we show how c-GNF captures the underlying SCM without making any\nassumption about functional forms. Second, we propose a novel dequantization\ntrick to deal with discrete variables, which is a limitation of normalizing\nflows in general. Third, we demonstrate in experiments that c-GNF performs\non-par with IPW and RWR in terms of bias and variance for estimating the ATE,\nwhen the true functional forms are known, and better when they are unknown.\nFourth and most importantly, we conduct counterfactual inference with c-GNFs,\ndemonstrating promising empirical performance. Because IPW and RWR, like other\ntraditional methods, lack the capability of counterfactual inference, c-GNFs\nwill likely play a major role in tailoring personalized treatment, facilitating\nP$^3$A, optimizing social interventions - in contrast to the current\n`one-size-fits-all' approach of existing methods.", "entities_include_in_text": ["Wright 1921; Fisher 1936", "Wright 1921", "Haavelmo 1943; Gold-\nberger 1972", "King 1974; Ploch,\nGoldberger, and Duncan 1975; Fienberg and Duncan 1975", "Kino et al. 2021", "Wodtke 2020", "Pearl 2009b,\n2012", "Bang and Robins 2005", "Wodtke 2020", "Kino et al. 2021", "GeoLytics 2003", "Pearl 2012", "Ru-\nbin 1990", "Rosenbaum and Rubin 1983", "Cox 1958", "Tabak and Vanden-Eijnden 2010;\nTabak and Turner 2013; Rezende and Mohamed 2015;\nKobyzev, Prince, and Brubaker 2020; Papamakarios et al.\n2021", "Milnor and Weaver 1997", "Kingma et al. 2016; Papamakar-\nios, Murray, and Pavlakou 2017; Huang et al. 2018", "Pa-\npamakarios et al. 2021", "Huang et al. 2018", "Wehenkel and Louppe 2021", "We-\nhenkel and Louppe 2019", "Uria, Murray, and Larochelle 2013;\nHoogeboom et al. 2019; Tran et al. 2019; Ho et al. 2019;\nZiegler and Rush 2019; Ma et al. 2019; Nielsen and Winther\n2020; Pawlowski, de Castro, and Glocker 2020", "Paszke et al.\n2017", "Loshchilov\nand Hutter 2019"], "entities_from_reference": ["Robins", "J.", "Causal Inference Models", "Cole", "Cox", "Wiley", "Fienberg", "Fisher", "Brunswick", "Goldberger", "Econometrica", "Haavelmo", "Hern", "Causal Inference", "Boca Raton", "Architecture Design", "Machine Learning", "Hoogeboom", "J. W.", "Berg", "Huang", "Krueger", "Lacoste", "Neural Autoregressive Flows", "Khemakhem", "Monti", "Leech", "Hyvarinen", "Causal Autoregressive Flows", "Kingma", "Inverse Autoregressive Flow", "Kino", "Hsu", "Mita", "Daoud", "Review", "Research Prospects", "Social Science", "Kobyzev", "Pattern Analysis", "Machine Intelligence", "Sekhon", "J. S.", "Heterogeneous Treatment", "Loshchilov", "Hutter", "Weight Decay", "Zhang", "Hovy", "Milnor", "Princeton", "Neal", "Causal", "Nielsen", "Winther", "Papamakarios", "Murray", "Autoregressive Flow", "Density Estimation", "Nalisnick", "Mohamed", "Lakshminarayanan", "Flows", "Machine Learning Research", "Paszke", "Gross", "Chanan", "Yang", "Lin", "Desmaison", "Lerer", "Pawlowski", "Castro", "Glocker", "Deep Structural Causal Models", "Wodtke", "Sociological Methods", "Elwert", "Wright", "Ziegler", "Pearl", "Cambridge University Press", "Basic Books", "Ploch", "Rezende", "Mathematical Modelling", "Rosenbaum", "Rubin", "Biometrika", "Tabak", "Applied Mathematics", "Dual Ascent", "Tran", "Agrawal", "Poole", "Discrete Data", "Uria", "Neural Information", "Epidemiology", "Effect Modification", "Wehenkel", "Louppe", "Monotonic Neural Networks"]}{"title": ["Measuring and Reducing Model Update Regression in Structured Prediction for NLP"], "authors": ["[arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Elman Mansimov'), arxiv.Result.Author('Yi-An Lai'), arxiv.Result.Author('Yixuan Su'), arxiv.Result.Author('Lei Shu'), arxiv.Result.Author('Yi Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02976v1"], "summary": "Recent advance in deep learning has led to rapid adoption of machine learning\nbased NLP models in a wide range of applications. Despite the continuous gain\nin accuracy, backward compatibility is also an important aspect for industrial\napplications, yet it received little research attention. Backward compatibility\nrequires that the new model does not regress on cases that were correctly\nhandled by its predecessor. This work studies model update regression in\nstructured prediction tasks. We choose syntactic dependency parsing and\nconversational semantic parsing as representative examples of structured\nprediction tasks in NLP. First, we measure and analyze model update regression\nin different model update settings. Next, we explore and benchmark existing\ntechniques for reducing model update regression including model ensemble and\nknowledge distillation. We further propose a simple and effective method,\nBackward-Congruent Re-ranking (BCR), by taking into account the characteristics\nof structured output. Experiments show that BCR can better mitigate model\nupdate regression than model ensemble and knowledge distillation approaches.", "entities_include_in_text": ["Shen et al.,\n2020; Yan et al., 2021", "Xie et al., 2021", "e.g., Ma et al., 2018", "Shen et al., 2020; Yan et al., 2021; Xie et al., 2021", "Yan et al., 2021; Xie et al., 2021", "Smith, 2011", "Yan et al., 2021", "Qi et al., 2020", "Petrov et al., 2012", "Ma et al.,\n2018", "Gupta et al., 2018", "Rongali et al., 2020", "Liu et al., 2019", "Shen et al., 2020; Yan et al., 2021; Xie et al., 2021", "Yan et al.,\n2021; Xie et al., 2021", "Zmigrod et al., 2020; 2021", "Fan et al., 2018; Radford et al.,\n2019", "Holtzman et al., 2019", "Srivastava et al.,\n2014", "Shen et al., 2004;\nYan et al., 2021; Xie et al., 2021", "Ma et al., 2018", "Rongali et al.,\n2020", "Ott et al., 2019", "Liu et al., 2019", "Zmigrod et al., 2020; 2021", "Fan et al., 2018; Radford et al., 2019", "Holtzman et al., 2019", "Yan et al., 2021; Xie\net al., 2021", "Yan et al.,\n2021", "Li et al., 2016", "Geyik et al., 2019", "Falke et al., 2019", "Parisi et al.,\n2019; Biesialska et al., 2020", "Toneva et al., 2019", "Ribeiro et al., 2020"], "entities_from_reference": ["Caruana", "Biesialska", "Bucilua", "Model", "Collins", "Koo", "Rehbein", "Dozat", "Falke", "Annual", "Fan", "Lewis", "Dauphin", "Gepperth", "Geyik", "Ambler", "Kenthapadi", "Gupta", "Shah", "Mohit", "Kumar", "Hinton", "Vinyals", "Holtzman", "Buys", "Choi", "Kim", "Kriz", "Sedoc", "Apidianaki", "Zheng", "Miltsakaki", "Human Language Technologies", "Short Papers", "Nivre", "Synthesis", "Le", "Zuidema", "Galley", "Brockett", "Gao", "Dolan", "Liu", "Ott", "Goyal", "Joshi", "Levy", "Stoyanov", "Zhang", "Data Engineering", "Model Update", "Peng", "Hovy", "Shen", "Sarkar", "Och", "Masana", "Twardowski", "J.", "Xia", "Pattern Recognition", "Edunov", "Baevski", "Gross", "Auli", "Parisi", "J. L.", "Kanan", "Wermter", "Petrov", "Das", "Radford", "Luan", "Ribeiro", "Guestrin", "Singh", "Romero", "Kahou", "Chassang", "Gatta", "Bengio", "Rongali", "Monti", "Salazar", "Liang", "Kirchhoff", "Smith", "Bauer", "Srivastava", "Krizhevsky", "Salakhutdinov", "Toneva", "Trischler", "Gordon", "Kleindessner", "Locatello", "Sch", "Gehler", "Xie", "Lai", "Yan", "Kundu", "Yang", "Deng", "Wang", "Yee", "Joo", "Bae", "Yin", "Zagoruyko", "Komodakis", "Zliobait", "Zmigrod", "Vieira", "Old", "Given"]}{"title": ["Causal Inference Using Tractable Circuits"], "authors": ["[arxiv.Result.Author('Adnan Darwiche')]"], "link": ["http://arxiv.org/pdf/2202.02891v1"], "summary": "The aim of this paper is to discuss a recent result which shows that\nprobabilistic inference in the presence of (unknown) causal mechanisms can be\ntractable for models that have traditionally been viewed as intractable. This\nresult was reported recently to facilitate model-based supervised learning but\nit can be interpreted in a causality context as follows. One can compile a\nnon-parametric causal graph into an arithmetic circuit that supports inference\nin time linear in the circuit size. The circuit is also non-parametric so it\ncan be used to estimate parameters from data and to further reason (in linear\ntime) about the causal graph parametrized by these estimates. Moreover, the\ncircuit size can sometimes be bounded even when the treewidth of the causal\ngraph is not, leading to tractable inference on models that have been deemed\nintractable previously. This has been enabled by a new technique that can\nexploit causal mechanisms computationally but without needing to know their\nidentities (the classical setup in causal inference). Our goal is to provide a\ncausality-oriented exposure to these new results and to speculate on how they\nmay potentially contribute to more scalable and versatile causal inference.", "entities_include_in_text": ["NeurIPS 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Inter-subject Contrastive Learning for Subject Adaptive EEG-based Visual Recognition"], "authors": ["[arxiv.Result.Author('Pilhyeon Lee'), arxiv.Result.Author('Sunhee Hwang'), arxiv.Result.Author('Jewook Lee'), arxiv.Result.Author('Minjung Shin'), arxiv.Result.Author('Seogkyu Jeon'), arxiv.Result.Author('Hyeran Byun')]"], "link": ["http://arxiv.org/pdf/2202.02901v1"], "summary": "This paper tackles the problem of subject adaptive EEG-based visual\nrecognition. Its goal is to accurately predict the categories of visual stimuli\nbased on EEG signals with only a handful of samples for the target subject\nduring training. The key challenge is how to appropriately transfer the\nknowledge obtained from abundant data of source subjects to the subject of\ninterest. To this end, we introduce a novel method that allows for learning\nsubject-independent representation by increasing the similarity of features\nsharing the same class but coming from different subjects. With the dedicated\nsampling principle, our model effectively captures the common knowledge shared\nacross different subjects, thereby achieving promising performance for the\ntarget subject even under harsh problem settings with limited data.\nSpecifically, on the EEG-ImageNet40 benchmark, our model records the top-1 /\ntop-3 test accuracy of 72.6% / 91.6% when using only five EEG samples per class\nfor the target subject. Our code is available at\nhttps://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Inter_Subject_Contrastive_Learning_for_EEG.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Prompt-Guided Injection of Conformation to Pre-trained Protein Model"], "authors": ["[arxiv.Result.Author('Qiang Zhang'), arxiv.Result.Author('Zeyuan Wang'), arxiv.Result.Author('Yuqiang Han'), arxiv.Result.Author('Haoran Yu'), arxiv.Result.Author('Xurui Jin'), arxiv.Result.Author('Huajun Chen')]"], "link": ["http://arxiv.org/pdf/2202.02944v1"], "summary": "Pre-trained protein models (PTPMs) represent a protein with one fixed\nembedding and thus are not capable for diverse tasks. For example, protein\nstructures can shift, namely protein folding, between several conformations in\nvarious biological processes. To enable PTPMs to produce task-aware\nrepresentations, we propose to learn interpretable, pluggable and extensible\nprotein prompts as a way of injecting task-related knowledge into PTPMs. In\nthis regard, prior PTPM optimization with the masked language modeling task can\nbe interpreted as learning a sequence prompt (Seq prompt) that enables PTPMs to\ncapture the sequential dependency between amino acids. To incorporate\nconformational knowledge to PTPMs, we propose an interaction-conformation\nprompt (IC prompt) that is learned through back-propagation with the\nprotein-protein interaction task. As an instantiation, we present a\nconformation-aware pre-trained protein model that learns both sequence and\ninteraction-conformation prompts in a multi-task setting. We conduct\ncomprehensive experiments on nine protein datasets. Results confirm our\nexpectation that using the sequence prompt does not hurt PTPMs' performance on\nsequence-related tasks while incorporating the interaction-conformation prompt\nsignificantly improves PTPMs' performance on tasks where conformational\nknowledge counts. We also show the learned prompts can be combined and extended\nto deal with new complex tasks.", "entities_include_in_text": ["Epstein et al., 1963", "Devlin et al., 2019", "Zhang et al., 2019", "Rao\net al., 2019", "Elnaggar et al., 2021", "Berman et al., 2000", "Dunbar\net al., 2013", "J et al., 2018", "RF et al., 2006", "Dosovitskiy et al.,\n2021) and proteins. Rives et al. (2021", "Devlin et al., 2019", "Jumper et al., 2021", "Brown et al., 2020", "Ham-\nbardzumyan et al., 2021", "Szk-\nlarczyk et al., 2019", "Dunbar\net al., 2013", "Rao et al., 2019", "Dunbar et al., 2013", "Paszke et al., 2019", "Ott et al.,\n2019", "Hashemifar et al., 2018", "H et al.,\n2018", "Chen et al., 2019", "Chen\net al., 2019", "Elnag-\ngar et al., 2021", "Humphreys et al., 2021", "Kruskal, 1964", "Yan et al., 2008", "J et al.,\n2018", "Prendergast FG,\n1978", "M et al., 1996"], "entities_from_reference": ["Elnaggar", "Heinzinger", "Dallago", "Wang", "Pattern Analysis", "Machine Intelligence", "Berman", "Feng", "Bhat", "Weissig", "Shindyalov", "Nucleic", "Epstein", "Goldberger", "Harbor Symposia", "Brown", "Mann", "Ryder", "Subbiah", "Kaplan", "J. D.", "Ranzato", "Hadsell", "Lin", "Protein Structure", "Academic Press", "Chen", "Zhou", "Zhang", "Chang", "Zaniolo", "Devlin", "Lee", "Toutanova", "Human Language Technologies", "Short Papers", "Minnesota", "Dosovitskiy", "Kolesnikov", "Zhai", "Dehghani", "Minderer", "Heigold", "Gelly", "Uszkoreit", "Dunbar", "Leem", "Baker", "Fuchs", "Georges", "Shi", "Deane", "Nucleic Acids Research", "Gao", "Fisch", "Deep Neural Network", "Protein Interactions Using", "Molecules", "Hambardzumyan", "Hashemifar", "Neyshabur", "Khan", "Humphreys", "Pei", "Baek", "Ovchinnikov", "Ness", "Science", "Jumper", "Figurnov", "Ronneberger", "Bates", "Potapenko", "Highly", "Nature", "Protein Model Kruskal", "J. Multidimensional", "Psychometrika", "Paszke", "Gross", "Lerer", "Bradbury", "Red Hook", "Le Scao", "Lester", "Online", "Punta Cana", "Liang", "Liu", "Yuan", "Jiang", "Hayashi", "Zheng", "Yang", "Tang", "J. GPT", "Main Track", "James", "Hutchison", "Ott", "Edunov", "Baevski", "Fan", "Auli", "Biochemistry", "Raffel", "Shazeer", "Roberts", "Narang", "Matena", "Li", "Machine Learning Research", "Rao", "Thomas", "Duan", "Meier", "Verkuil", "Machine Learning", "Goyal", "Guo", "Zitnick", "Schick", "Sch", "Volume", "Szklarczyk", "Gable", "Lyon", "Junge", "Protein Model", "Vig", "Madani", "Rajani", "Yan", "Dobbs", "Lan", "Pang", "Annual", "Zhu", "Shao", "Dauphin", "Vaughan", "J. W", "Deng", "Huang", "Helix"]}{"title": ["Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity"], "authors": ["[arxiv.Result.Author('Lin Guan'), arxiv.Result.Author('Sarath Sreedharan'), arxiv.Result.Author('Subbarao Kambhampati')]"], "link": ["http://arxiv.org/pdf/2202.02886v1"], "summary": "Creating reinforcement learning (RL) agents that are capable of accepting and\nleveraging task-specific knowledge from humans has been long identified as a\npossible strategy for developing scalable approaches for solving long-horizon\nproblems. While previous works have looked at the possibility of using symbolic\nmodels along with RL approaches, they tend to assume that the high-level action\nmodels are executable at low level and the fluents can exclusively characterize\nall desirable MDP states. This need not be true and this assumption overlooks\none of the central technical challenges of incorporating symbolic task\nknowledge, namely, that these symbolic models are going to be an incomplete\nrepresentation of the underlying task. To this end, we introduce Symbolic-Model\nGuided Reinforcement Learning, wherein we will formalize the relationship\nbetween the symbolic model and the underlying MDP that will allow us to capture\nthe incompleteness of the symbolic model. We will use these models to extract\nhigh-level landmarks that will be used to decompose the task, and at the low\nlevel, we learn a set of diverse policies for each possible task sub-goal\nidentified by the landmark. We evaluate our system by testing on three\ndifferent benchmark domains and we show how even with incomplete symbolic model\ninformation, our approach is able to discover the task structure and\nefficiently guide the RL agent towards the goal.", "entities_include_in_text": ["Icarte et al., 2018", "Lyu et al., 2019;\nIllanes et al., 2020", "Illanes et al., 2020; Lyu et al., 2019", "Zhang\net al., 2018; Kambhampati et al., 2021", "Yang et al., 2018; Illanes et al., 2020; Lyu\net al., 2019; Kokel et al., 2021", "Basu et al., 2018; Guan et al.,\n2021", "Andreas et al., 2017", "Goyal et al., 2019", "Haarnoja et al., 2017; Kumar et al., 2020", "Florensa et al., 2017; Achiam et al., 2018; Eysenbach\net al., 2019; Lee et al., 2019", "Kambhampati et al., 1996", "Zhang et al., 2019", "Miller, 2019; Chakraborti et al.,\n2019", "Sreedharan\net al., 2020; Zhang et al., 2018; Lyu et al., 2019", "Srivastava et al., 2016; Marthi et al., 2007", "Illanes et al., 2020", "Kulkarni et al., 2016", "Sutton et al., 1999", "Florensa et al., 2017; Eysenbach et al., 2019;\nAchiam et al., 2018", "Florensa et al., 2017", "Illanes et al., 2020", "Yang et al., 2018", "Andreas\net al., 2017", "Li et al., 2006", "Kokel\net al., 2021)", "Florensa et al., 2017;\nAchiam et al., 2018; Kumar et al., 2020; Eysenbach et al.,\n2019", "Yang et al., 2018", "Schaul et al., 2015"], "entities_from_reference": ["Edwards", "Andreas", "Klein", "Machine Learning", "Basu", "Chakraborti", "Eysenbach", "Gupta", "Ibarz", "Nilsson", "Artif", "Florensa", "Duan", "Geffner", "Bonet", "Synthesis", "Goyal", "Mooney", "Grzes", "Kudenko", "Guan", "Guo", "Zhang", "Haarnoja", "Tang", "Milli", "Bengio", "Wallach", "Garnett", "Icarte", "Klassen", "Valenzano", "Machine Learning Research", "Illanes", "Yan", "Symbolic", "Nancy", "Kambhampati", "Ihrig", "Zha", "Keyder", "Richter", "Helmert", "Wooldridge", "Kokel", "Natarajan", "Tadepalli", "Kulkarni", "Narasimhan", "J. Hierarchical", "Kumar", "Skill Diversity Lee", "Salakhutdinov", "Lerer", "Szlam", "Ballard", "Stone", "Walsh", "Littman", "Symposium", "Lyu", "Yang", "Liu", "Gustafson", "Marthi", "Wolfe", "Fox", "Rhode Island", "Miller", "Schaul", "Quan", "Antonoglou", "Sreedharan", "Srivastava", "Wellman", "Phoenix", "Sutton", "Singh", "Tversky", "Kahneman", "Skill Diversity", "Proof Sketch", "Input", "Initialize Q-values", "Select", "Terminate", "Hence", "Achiam", "Due", "Smeta", "Smeta S", "Mario", "Again", "Similar", "Implementation Details", "Skill Diversity J", "Symbolic Models", "Domain Model", "Problem Model"]}{"title": ["Universality of parametric Coupling Flows over parametric diffeomorphisms"], "authors": ["[arxiv.Result.Author('Junlong Lyu'), arxiv.Result.Author('Zhitang Chen'), arxiv.Result.Author('Chang Feng'), arxiv.Result.Author('Wenjing Cun'), arxiv.Result.Author('Shengyu Zhu'), arxiv.Result.Author('Yanhui Geng'), arxiv.Result.Author('Zhijie Xu'), arxiv.Result.Author('Yongwei Chen')]"], "link": ["http://arxiv.org/pdf/2202.02906v1"], "summary": "Invertible neural networks based on Coupling Flows CFlows) have various\napplications such as image synthesis and data compression. The approximation\nuniversality for CFlows is of paramount importance to ensure the model\nexpressiveness. In this paper, we prove that CFlows can approximate any\ndiffeomorphism in C^k-norm if its layers can approximate certain\nsingle-coordinate transforms. Specifically, we derive that a composition of\naffine coupling layers and invertible linear transforms achieves this\nuniversality. Furthermore, in parametric cases where the diffeomorphism depends\non some extra parameters, we prove the corresponding approximation theorems for\nour proposed parametric coupling flows named Para-CFlows. In practice, we apply\nPara-CFlows as a neural surrogate model in contextual Bayesian optimization\ntasks, to demonstrate its superiority over other neural surrogate models in\nterms of optimization performance.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Jury Learning: Integrating Dissenting Voices into Machine Learning Models"], "authors": ["[arxiv.Result.Author('Mitchell L. Gordon'), arxiv.Result.Author('Michelle S. Lam'), arxiv.Result.Author('Joon Sung Park'), arxiv.Result.Author('Kayur Patel'), arxiv.Result.Author('Jeffrey T. Hancock'), arxiv.Result.Author('Tatsunori Hashimoto'), arxiv.Result.Author('Michael S. Bernstein')]"], "link": ["http://arxiv.org/pdf/2202.02950v1"], "summary": "Whose labels should a machine learning (ML) algorithm learn to emulate? For\nML tasks ranging from online comment toxicity to misinformation detection to\nmedical diagnosis, different groups in society may have irreconcilable\ndisagreements about ground truth labels. Supervised ML today resolves these\nlabel disagreements implicitly using majority vote, which overrides minority\ngroups' labels. We introduce jury learning, a supervised ML approach that\nresolves these disagreements explicitly through the metaphor of a jury:\ndefining which people or groups, in what proportion, determine the classifier's\nprediction. For example, a jury learning model for online toxicity might\ncentrally feature women and Black jurors, who are commonly targets of online\nharassment. To enable jury learning, we contribute a deep learning architecture\nthat models every annotator in a dataset, samples from annotators' models to\npopulate the jury, then runs inference to classify. Our architecture enables\njuries that dynamically adapt their composition, explore counterfactuals, and\nvisualize dissent.", "entities_include_in_text": ["Oct 2016", "Dec. 2017", "oct 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["An Empirical Analysis of AI Contributions to Sustainable Cities (SDG11)"], "authors": ["[arxiv.Result.Author('Shivam Gupta'), arxiv.Result.Author('Auriol Degbelo')]"], "link": ["http://arxiv.org/pdf/2202.02879v1"], "summary": "Artificial Intelligence (AI) presents opportunities to develop tools and\ntechniques for addressing some of the major global challenges and deliver\nsolutions with significant social and economic impacts. The application of AI\nhas far-reaching implications for the 17 Sustainable Development Goals (SDGs)\nin general, and sustainable urban development in particular. However, existing\nattempts to understand and use the opportunities offered by AI for SDG 11 have\nbeen explored sparsely, and the shortage of empirical evidence about the\npractical application of AI remains. In this chapter, we analyze the\ncontribution of AI to support the progress of SDG 11 (Sustainable Cities and\nCommunities). We address the knowledge gap by empirically analyzing the AI\nsystems (N = 29) from the AIxSDG database and the Community Research and\nDevelopment Information Service (CORDIS) database. Our analysis revealed that\nAI systems have indeed contributed to advancing sustainable cities in several\nways (e.g., waste management, air quality monitoring, disaster response\nmanagement, transportation management), but many projects are still working for\ncitizens and not with them. This snapshot of AI's impact on SDG11 is inherently\npartial, yet useful to advance our understanding as we move towards more mature\nsystems and research on the impact of AI systems for social good.", "entities_include_in_text": ["Gupta et al., 2021", "Degbelo et al., \n\n2021;  Chipofya  et  al.,  2020", "Gupta  et  al.,  2018", "Barns, 2019", "Gupta et  al., 2018", "Allam  and Jones, 2020", "Zheng et al., 2020). Furthermore, as Israilidis et al. (2021", "Batty, 2009", "Axinte et al., 2019", "Solecki et al., 2018", "Ismagilova  et  al.,  2019", "Sharda  et  al.,  2021;  Rogers  et  al.,  2020", "Ismagilova et al., 2019", "Barlacchi et al., 2015; Bibri, \n\n2021", "Aust, 2019", "Acuto, 2016", "Allam and Dhunny, 2019", "Firouzi  et  al.,  2021;  Dinh  and \n\nThai,  2018;  Rajan  and  Saffiotti,  2017;  Tajunisa  et  al.,  2021;  Dai,  2019", "Rabah, 2018", "Sougkakis et al., 2020; \n\nVillagra et al., 2020; Majumdar et al., 2021", "Kuffer  et  al.,  2020,  2021", "Hilbert,  2016;  Gupta  et  al.,  2021", "Nilsson  et  al.,  2016", "Nilsson  et  al.,  2016;  Vinuesa  et  al.,  2020", "Allam and Dhunny, 2019", "Reddick et al., 2020; Chase, 2020", "Galaz  et  al.,  2021", "Taddeo  et  al.,  2021;  van  Wynsberghe,  2021", "van \n\nWynsberghe, 2021", "United  Nations,  2015", "Croese  et  al.,  2020", "Yigitcanlar et al., 2019", "Martens,  2019", "Li et al., 2018", "Fritz et al., \n\n2019). Social and cultural information dictates the context in which the AI is \n\nimplemented. Citizen participation provides the public with the opportunity \n\nto  support  policy  development,  leading  to  trust-building,  credibility,  and \n\nultimately  inclusiveness  in  taking  actions  towards  SDGs.  SDGs  require \n\nactions  that  can  transform  existing  practices  across  sectors.  Fraisl  et  al. \n\n(2020", "Kirwan and Zhiyong, 2020", "Munsaka  and  Dube, \n\n2018", "Antweiler,  2019", "Makondo and Thomas, 2018; Magni, \n\n2017", "Micheletti et al., 2014", "Gupta et al., \n\n2018", "Ferri et al., 2020", "Newman  et  al.,  2020", "Saner et al., 2020", "Vinuesa  et  al.,  2020", "Pekmezovic, 2019", "Walker  et  al., \n\n2019", "Guan et al., 2019; Rubio-Mozos et al., 2019; Thinyane, 2018", "Samoili et al., 2020", "Samoili et al., 2020", "Hutson, \n\n2017", "Degbelo  et  al.,  2016", "Salehi  et  al.,  2016)", "e.g., Zook \n\net al. (2010)", "Henderson  et  al., \n\n2020", "KDD  2016"], "entities_from_reference": ["Plusieurs"]}{"title": ["Mental Stress Detection using Data from Wearable and Non-wearable Sensors: A Review"], "authors": ["[arxiv.Result.Author('Aamir Arsalan'), arxiv.Result.Author('Syed Muhammad Anwar'), arxiv.Result.Author('Muhammad Majid')]"], "link": ["http://arxiv.org/pdf/2202.03033v1"], "summary": "This paper presents a comprehensive review of methods covering significant\nsubjective and objective human stress detection techniques available in the\nliterature. The methods for measuring human stress responses could include\nsubjective questionnaires (developed by psychologists) and objective markers\nobserved using data from wearable and non-wearable sensors. In particular,\nwearable sensor-based methods commonly use data from electroencephalography,\nelectrocardiogram, galvanic skin response, electromyography, electrodermal\nactivity, heart rate, heart rate variability, and photoplethysmography both\nindividually and in multimodal fusion strategies. Whereas, methods based on\nnon-wearable sensors include strategies such as analyzing pupil dilation and\nspeech, smartphone data, eye movement, body posture, and thermal imaging.\nWhenever a stressful situation is encountered by an individual, physiological,\nphysical, or behavioral changes are induced which help in coping with the\nchallenge at hand. A wide range of studies has attempted to establish a\nrelationship between these stressful situations and the response of human\nbeings by using different kinds of psychological, physiological, physical, and\nbehavioral measures. Inspired by the lack of availability of a definitive\nverdict about the relationship of human stress with these different kinds of\nmarkers, a detailed survey about human stress detection methods is conducted in\nthis paper. In particular, we explore how stress detection methods can benefit\nfrom artificial intelligence utilizing relevant data from various sources. This\nreview will prove to be a reference document that would provide guidelines for\nfuture research enabling effective detection of human stress conditions.", "entities_include_in_text": ["Dempsey, 2018", "Organization\net al., 2015", "Spoorthy et al., 2020; Ransing et al., 2020", "Association et al., 2017", "Can et al., 2020", "Delmastro et al., 2020", "Zubair and Yoon, 2020", "Pluntke et al., 2019", "Gillani et al., 2021", "Gedam and\nPaul, 2021", "Muthukumar and Nachiappan, 2010", "Ulrich-Lai\nand Herman, 2009", "Kajantie and Phillips, 2006", "Werner, 1993", "Segerstrom and Miller, 2004", "Sincero, 2012", "Hiriyappa, 2013", "Salleh, 2008", "Gurung, 2013", "Gowrisankaran et al., 2012", "Schulte-Mecklenbeck et al., 2011", "Subahni et al., 2012", "Wielgosz et al., 2016", "Liew et al., 2015", "Giannakakis et al., 2019", "Rastgoo et al., 2018", "Carneiro et al., 2017", "Thapliyal et al.,\n2017", "Stroop, 1935", "Pujol et al., 2001", "Dedovic et al., 2005", "Setz et al., 2009; Minguillon et al., 2016; Al-Shargie\net al., 2015, 2016", "Hines,\n1932", "Lovallo, 1975", "Suter et al., 2007", "Previnaire et al., 2012", "Frings et al., 2013", "Hassellund et al., 2010", "Shi et al., 2010", "Bitsika et al., 2014", "Taylor et al., 2000", "Dick-\nerson and Kemeny, 2004", "McEwen, 2005", "Kemp et al.,\n2012", "Glaser and Kiecolt-Glaser, 2005", "Pittig et al.,\n2013", "Wolpe, 2013", "Parsons and Rizzo, 2008", "Bordnick et al., 2012", "Kudielka et al., 2009", "Slater et al., 2006; Felnhofer et al.,\n2014", "Slater et al., 2006; Felnhofer et al., 2014; Pertaub et al., 2002", "Kudielka\net al., 2009", "Kelly et al., 2008", "Hemmeter et al., 2005", "Stroud et al., 2002", "Kothgassner et al., 2016", "Escher et al., 1993", "Suda et al., 2008", "Khalfa et al., 2003", "Bartlett, 1996", "Allen et al., 2001", "Knight and Rickard,\n2001", "Evans, 2002", "Lang et al., 1997", "Baltaci and Gokcay, 2016; Liao et al., 2005; Giannakakis et al., 2017; Nhan\nand Chau, 2009; Khalilzadeh et al., 2010", "Lasaitis et al., 2008", "Lo-\nhani et al., 2013", "Dufey et al., 2011", "Caria et al., 2010", "Hajcak and Dennis, 2009", "Styliadis et al., 2015", "Bradley et al., 2001", "Baglioni et al., 2010", "Kirschbaum et al., 1993", "Kurniawan et al., 2013; Engert et al.,\n2014; Vinkers et al., 2013; Nater et al., 2005", "Healey and Pi-\ncard, 2005", "Schneegass et al., 2013", "Birjandta-\nlab et al., 2016", "Taamneh et al., 2017", "Schmidt\net al., 2018", "Koldijk et al., 2014", "Steeneken and Hansen,\n1999", "Markova et al., 2019", "Meziatisabour et al., 2021", "Cohen et al., 1983", "Bryant et al., 2000", "Brantley et al., 1987", "Derogatis and Spencer, 1993", "Schulz and Schlotz, 1999", "Bryant et al., 2000", "Wang\net al., 2010", "Harvey and Bryant, 1998,\n1999, 2000; Brewin et al., 1999", "Harvey and Bryant, 1998", "Derogatis and Spencer, 1993", "Racine et al., 2018", "Stavropoulos et al., 2017", "Fitting et al., 1986", "Corcoran, 1992", "Kaufer et al., 1998", "Greene et al., 1982", "Maslach et al., 1997", "Brantley\nand Jones, 1993", "Goreczny et al.,\n1988", "Mosley Jr et al., 1991; Waggoner, 1986", "Garrett et al., 1991", "Goetsch et al., 1990", "Sturmbauer\net al., 2019", "Buehl et al., 2012", "Healey and Picard, 2005", "Saeed et al., 2020", "Lin et al., 2016", "Shoker et al.,\n2005", "Ala-\nmudun et al., 2012", "Mozos et al.,\n2017; Gjoreski et al., 2017", "Pan and Tompkins, 1985", "Hovsepian et al., 2015", "Elgendi, 2012", "Choi et al., 2011", "Wijsman et al., 2010", "Willigenburg et al., 2012", "Giakoumis et al., 2012; Setz et al., 2009", "Shon et al., 2018", "Saeed et al.,\n2020", "Deng et al., 2012", "Yerigeri and Ragha, 2019", "Hasan and Kim,\n2019", "Palacios et al., 2019", "Ahuja and Banga, 2019; Saeed et al.,\n2017", "Saeed et al., 2018, 2020; Vanitha and Suresh, 2013; Attallah, 2020", "Asif et al., 2019; Vasavi et al., 2018", "Sardeshpande and Thool, 2019; Masood and\nAlghamdi, 2019", "Uddin and Canavan, 2019", "Garcia and Gustavson, 1997; Northrup, 1997", "Dharmawan, 2007", "Berger, 1929", "Chandra et al., 2017", "Sawangjai et al., 2019", "Troy M et al., 2012", "Technologies, 2012", "Oostenveld and Praamstra, 2001", "Minguillon et al., 2016", "Sanei\nand Chambers, 2013", "Saeed et al., 2015; Hamid et al., 2015", "Hoffmann, 2005", "Gatzke-Kopp et al., 2014; Giannakakis\net al., 2015", "Seo et al., 2008; Lewis et al., 2007", "Qin et al., 2009", "Lopez-Duran et al., 2012", "Peng et al., 2013", "Minguillon et al., 2016", "Acharya et al., 2012", "Giannakakis et al., 2015", "Lopez-Duran et al., 2012; Tomarken et al., 1990", "Peng et al., 2013", "Hosseini et al., 2010; Khosrowabadi\net al., 2011; Sharma and Gedeon, 2014; Ko et al., 2009; Giannakaki et al., 2017", "Hayashi et al., 2009", "Alonso et al., 2015; De-\nmerdzieva, 2011; Al-Shargie et al., 2015; Tran et al., 2007; Seo and Lee, 2010", "Katsis et al., 2011", "Choi et al., 2015", "Alonso et al., 2015", "Huiku et al., 2007", "Marshall et al., 2015", "Choi et al., 2015", "Sharma and Gedeon, 2012", "Secerbegovic et al., 2017", "Hou et al., 2015", "Jun and Smitha, 2016", "Duru et al., 2013", "Calibo\net al., 2013", "Pomer-Escher et al., 2014", "Vijayaragavan et al., 2015", "Reanaree\net al., 2016", "Lin and John,\n2006", "Vanitha\nand Krishnan, 2016", "Kalas and Momin, 2016", "Al-Shargie et al., 2015", "Pandiyan et al., 2013", "Asif et al., 2019", "Halim and Rehan,\n2020", "Saeed et al., 2015", "Saeed et al., 2017", "Saeed et al.,\n2018", "Hamid et al., 2015", "Sulaiman et al., 2011", "Hamid et al.,\n2010", "Luijcks et al., 2015", "Saeed et al., 2020", "Halim and Rehan, 2020", "Dharmawan, 2007", "Secerbegovic et al., 2017", "Hou et al., 2015", "Jun and Smitha, 2016", "Calibo et al., 2013", "Hosseini et al., 2010", "Giannakaki et al., 2017", "Al-Shargie et al., 2015", "Vanitha and Krishnan, 2016", "Asif et al., 2019", "Khosrowabadi et al., 2011", "Saeed et al., 2015", "Saeed et al., 2017", "Saeed et al., 2018", "Saeed et al., 2020", "De Luca, 2002", "Reaz et al., 2006", "Weyers et al., 2006", "Ekman et al., 1978", "Luijcks et al., 2014", "Lundberg et al., 1994; Wijsman et al., 2010; Larsson et al., 1995", "Lund-\nberg et al., 1994", "Wi-\njsman et al., 2010", "Larsson\net al., 1995", "Krantz et al., 2004", "Schleifer et al., 2008", "Engelhardt et al., 2015", "Tyagi et al., 2017", "Farnsworth, 2018", "Wu et al., 2010", "Lidberg\nand Wallin, 1981", "Ayata\net al., 2017", "Critchley, 2002", "Kurniawan et al., 2013", "Dawson\net al., 2007", "Nikula, 1991", "Liao et al., 2005", "Shi et al., 2007", "Healey, 2000", "Giakoumis et al., 2012; Blechert et al., 2006; Ritz et al., 2000;\nReinhardt et al., 2012; Hoehn-Saric et al., 1989", "Setz et al., 2009; Ren et al.,\n2012; Lee et al., 2004; Blechert et al., 2006; Hoehn-Saric et al., 1989; Nomikos et al.,\n1968; Lanzetta et al., 1976", "Nomikos et al., 1968", "Panigrahy\net al., 2017", "Hernandez\net al., 2011", "Healey, 2000", "Blechert et al., 2006", "Setz et al., 2009", "Lee et al., 2004", "Panigrahy et al., 2017", "Hernandez et al., 2011", "Ren et al., 2012", "Kociel-\nnik et al., 2013", "Ahn et al., 2019", "Al Khatib et al., 2007", "Karthikeyan et al., 2012c, 2011", "Karthikeyan et al., 2011", "Charbonnier et al., 2018", "Liu and Ulrich, 2014", "Goldberger et al., 2000", "Tanev et al., 2014", "Bong et al.,\n2012", "Karthikeyan et al.,\n2013", "Karthikeyan et al., 2011", "Charbonnier et al., 2018", "Tanev et al., 2014", "Bong et al., 2012", "Karthikeyan et al., 2013", "Keshan et al., 2015", "Castaldo et al., 2016", "Keshan et al., 2015", "Castaldo et al., 2016", "Giannakakis et al., 2017", "En-\ngert et al., 2014", "Lundberg et al., 1994", "Krantz et al.,\n2004", "Finsen\net al., 2001", "Rein-\nhardt et al., 2012", "Acerbi et al., 2016", "Steptoe et al., 2001", "Schubert et al., 2009", "Clays et al., 2011", "McDuff et al., 2014; Blechert et al., 2006; Hynynen et al.,\n2011; Cinaz et al., 2013; McDuff et al., 2016", "McDuff et al., 2014", "Blechert et al., 2006", "Hynynen et al., 2011", "Mc-\nDuff et al., 2016", "Blechert et al., 2006", "Giannakakis et al., 2017", "McDuff et al., 2014", "McDuff et al., 2016", "McFarland, 1985", "Zhai and Barreto, 2006", "Reisman, 1997", "Yoon et al., 2016", "Lee et al., 2010;\nTorii et al., 1992", "Vinkers et al., 2013", "Herborn et al., 2015", "Marazziti\net al., 1992", "Lee et al., 2004", "Zhai and Barreto, 2006", "Rimm-Kaufman and Kagan, 1996", "Simoes et al., 1991", "Vinkers et al., 2013; McDuff et al., 2014; Gross-\nman, 1983", "Ahmed et al.,\n2015", "Kreibig, 2010", "Stern et al., 2001", "Healey and Picard,\n2005", "Shin et al.,\n1998", "Hosseini and Naghibi-Sistani, 2011", "Seematter et al., 2002", "McDuff et al., 2014", "Ahmed et al., 2015", "Hosseini and Naghibi-Sistani, 2011", "Wijsman et al., 2013", "Rigas et al., 2011", "Wijsman et al., 2011", "Shan et al., 2020", "Shan et al., 2020", "Acharya et al., 2006", "Clifford, 2002", "Sloan et al., 1994", "Karthikeyan et al., 2013", "Taelman et al., 2009", "Lombardo and Vick, 2019", "McDuff et al., 2014", "Oskooei et al., 2019", "Hammoud et al., 2019", "Orsila et al., 2008", "Malik, 1996", "Blechert et al., 2006; Acerbi et al., 2016; Schubert et al., 2009; Clays et al.,\n2011; Hynynen et al., 2011; Cinaz et al., 2013; Bernardi et al., 2000; Taelman et al.,\n2011; Tharion et al., 2009; Visnovcova et al., 2014; Madden and Savard, 1995", "Acerbi et al.,\n2016", "Schubert et al., 2009", "Clays et al., 2011", "Hynynen et al., 2011", "Cinaz et al., 2013", "Bernardi et al., 2000", "Taelman et al., 2011", "Thar-\nion et al., 2009", "Visnovcova et al., 2014", "Madden and Savard, 1995", "Li et al., 2009", "Acerbi et al., 2016; Moriguchi et al., 1992", "Melillo et al., 2011", "Bousefsaf et al., 2013", "Kim et al., 2008", "Boonnithi and Phongsuphap,\n2011", "Melillo et al.,\n2013", "Vanitha and Suresh, 2014", "Munla et al., 2015", "Wang et al., 2013", "Gasperin et al., 2009", "Pickering et al., 1996", "Blechert et al., 2006", "Karthikeyan et al., 2013", "McDuff et al., 2014", "McDuff et al., 2016", "Melillo et al., 2011", "Melillo et al., 2013", "Vanitha and Suresh, 2014", "Munla et al., 2015", "Wang et al., 2013", "Kim et al., 2008", "Schnall et al., 1998", "Lundberg et al., 1994", "Carroll et al., 2003", "Carroll et al., 2011", "Hjortskov et al., 2004", "Krantz et al., 2004", "Vinkers et al., 2013", "Finsen et al., 2001", "Moriguchi\net al., 1992", "Steptoe et al., 2001", "Bernardi et al., 2000", "Hjortskov et al., 2004", "Chal-\nloner, 1979", "Maeda et al., 2011", "Kageyama et al., 2007", "Giannakakis et al., 2019", "Lyu et al., 2015", "Chauhan et al., 2018", "Henelius, 2016", "Poh et al., 2010", "McDuff et al., 2014, 2016", "Charlton et al., 2018", "Mohan et al., 2016", "Li et al., 2018", "Kirschbaum and Hellhammer, 1989", "McDuff et al., 2014", "McDuff et al., 2016", "Chauhan et al., 2018", "Li et al., 2018", "Fink, 2000", "Hellhammer et al., 2009", "Boucher\nand Plusquellec, 2019", "Tu et al., 2019", "Luo et al., 2012", "Nath et al., 2020", "Selvaraj, 2015", "Rey et al., 2014", "Nomura et al., 2009", "Beatty et al., 2000", "Bradley et al., 2008", "Onorati et al., 2013; Partala and Surakka, 2003; Al-\nOmar et al., 2013; Bradley et al., 2008; Ren et al., 2012; Pedrotti et al., 2014", "Honma, 2013; Simpson and Molloy, 1971; Baltaci and Gokcay, 2016; Zhai\nand Barreto, 2006", "Zhai and Bar-\nreto, 2006", "De Berker et al., 2016", "Partala and Surakka, 2003", "Ren et al., 2012", "Zhai and Barreto, 2006", "Pedrotti et al., 2014", "Baltaci and Gokcay, 2016", "Kimble et al., 2010", "Par-\ntala and Surakka, 2003", "Simpson\nand Molloy, 1971", "Wang and Yue, 2011", "Liao et al., 2005", "Torres-Salomao et al., 2015", "Winn et al., 1994", "Winn et al., 1994", "Ellermeier and Westphal,\n1995", "Partala and Surakka, 2003", "Pedrotti\net al., 2014; Reeves, 1920", "Womack and Hansen, 1999", "Lefter et al., 2015", "Williams and Stevens, 1972; Hansen, 1988; Cairns\nand Hansen, 1994; Junqua, 1996; Protopapas and Lieberman, 1997; Hansen and\nPatil, 2007; Gharavian, 2012; Lu et al., 2012; Kurniawan et al., 2013; Sondhi\net al., 2015; Hansen et al., 2011", "Nwe et al., 2003", "Fernandez and Picard, 2003; Healey and Picard,\n2005; Lefter et al., 2011", "Womak and Hansen, 1996", "Cairns and\nHansen, 1994", "Devillers and Vidrascu, 2006", "Hollien, 2002", "Simantiraki et al., 2016", "He et al., 2009", "Hanson et al., 1993", "Sarikaya and Gowdy, 1998", "Fernandez and Picard, 2003", "Hansen et al., 2011", "Yao et al., 2012", "Kadambe, 2007", "Soury and Devillers, 2013", "Haak et al., 2009", "Kurniawan et al., 2013", "Womack and Hansen, 1999", "Lefter et al., 2015", "Cairns and Hansen, 1994", "Hansen and Patil, 2007", "Lu et al., 2012", "Fernandez and Picard, 2003", "Womak and Hansen, 1996", "Simantiraki et al., 2016", "He et al., 2009", "Sarikaya and Gowdy, 1998", "Soury and Devillers, 2013", "Haak et al., 2009; Giannakakis et al., 2017", "Pavlidis et al., 2000", "Giannakakis et al., 2017", "Laretzaki et al., 2011", "Giannakakis et al., 2017", "Mokhayeri and Akbarzadeh-T, 2011", "Fox et al.,\n2007; Staab, 2014", "Fox et al., 2007", "Staab, 2014", "Mokhayeri and Akbarzadeh-T, 2011", "Aigrain\net al., 2015", "Giakoumis et al., 2012", "Carneiro et al., 2012", "Gunes and Piccardi, 2007", "Liao et al., 2005; Bevilacqua et al., 2018", "Bevilacqua et al., 2018", "Liao et al., 2005", "Dinges et al., 2005", "Aigrain et al., 2015", "Carneiro et al., 2012", "Dinges et al., 2005", "Arnrich et al., 2009", "Adams et al., 2015", "Arnrich\net al., 2009", "Liao et al.,\n2005", "Sun et al., 2014", "Carneiro et al., 2015", "Her-\nnandez et al., 2014", "Sun et al., 2014", "Lim et al.,\n2014", "Carneiro et al., 2015", "Rodrigues et al., 2013", "Lim et al., 2014", "Hernandez et al., 2014", "Gunawardhane et al., 2013", "Kailas et al., 2010", "Carneiro\net al., 2017", "Lee and Jung, 2018", "Dillon et al., 2016", "Colunas et al., 2011", "Colunas\net al., 2011", "Gaggioli et al., 2012", "Lu et al., 2012", "Bauer and\nLukowicz, 2012", "Muaremi et al., 2013", "Chang et al.,\n2011", "Garcia-Ceja et al.,\n2015", "Lee and Chung, 2016", "Gjoreski et al., 2015", "Ciman and\nWac, 2016", "Gjoreski et al., 2017", "Gimpel et al., 2015", "Sysoev et al., 2015", "Lu et al., 2012", "Muaremi et al., 2013", "Garcia-Ceja et al., 2015", "Lee and Chung, 2016", "Gjoreski et al., 2015", "Ciman and Wac, 2016", "Gjoreski et al., 2017", "Sysoev et al., 2015", "Vildjiounaite et al., 2018", "Kostopoulos et al., 2017", "Vildjiounaite et al.,\n2018", "Levine et al., 2001", "Nozawa and Tacano,\n2009", "Ioannou et al., 2014", "Nhan and Chau, 2009", "Nhan and Chau, 2009; Shastri et al., 2008", "Hong and Hong, 2016", "Pavlidis and\nLevine, 2002; Pavlidis et al., 2000", "Shastri et al., 2008", "Engert et al., 2014; Vinkers et al., 2013; Kang et al., 2006", "Pavlidis et al., 2012; Shastri et al., 2012", "Dinges et al., 2005", "Gao\net al., 2014", "Derakhshan et al., 2014", "Nhan and Chau, 2009", "Hong and Hong, 2016", "Sharma et al., 2014", "Aigrain et al.,\n2015", "Mohd et al., 2015", "Gao et al., 2014", "Derakhshan et al., 2014", "Sharma et al., 2014", "Aigrain et al., 2015", "Al-Shargie et al., 2016", "Kyriakou et al., 2019", "Lee et al., 2016", "Chen et al., 2017", "Ghaderi et al., 2015", "Gjoreski et al., 2016", "Maier et al., 2014", "Sano and Picard, 2013", "Hovsepian et al., 2015", "Zubair et al., 2015", "de Santos Sierra\net al., 2010", "Sandulescu et al., 2015", "Martinez et al., 2017", "Mo-\nzos et al., 2017", "Kurniawan et al.,\n2013", "Aigrain\net al., 2016", "Baltaci and Gokcay, 2016", "Huang et al., 2016", "Akhonda et al., 2014", "Shi et al., 2010", "Ahn et al., 2019", "Healey and Picard, 2005", "Muaremi et al., 2014", "Xu et al., 2014", "Mohino-Herranz\net al., 2015", "Sriramprakash et al.,\n2017", "Hosseini and Khalilzadeh, 2010", "Wijsman et al.,\n2013", "Rigas et al., 2011", "Picard et al., 2001", "Wijs-\nman et al., 2011", "Choi, 2011", "Gjoreski et al., 2016", "Betti et al., 2017", "Liew et al.,\n2015", "Nater et al., 2005", "Aimie-Salleh et al.,\n2018", "Vizer et al., 2009", "Liao\net al., 2005", "Giakoumis et al., 2012", "Al-Shargie et al., 2016", "Kyriakou et al., 2019", "Lee et al., 2016", "Chen et al., 2017", "Ghaderi et al., 2015", "Gjoreski et al., 2016", "Sano and Picard, 2013", "Hovsepian et al., 2015", "Zubair et al., 2015", "de Santos Sierra et al., 2010", "Sandulescu et al., 2015", "Mozos et al., 2017", "Kurniawan et al., 2013", "Aigrain et al., 2016", "Baltaci and Gokcay, 2016", "Huang et al., 2016", "Akhonda et al., 2014", "Ahn et al., 2019", "Healey and Picard, 2005", "Muaremi et al., 2014", "Xu et al., 2014", "Sriramprakash et al., 2017", "Hosseini and Khalilzadeh, 2010", "Wijsman et al., 2013", "Rigas et al., 2011", "Wijsman et al., 2011", "Gjoreski et al., 2016", "Betti et al., 2017", "Liew et al., 2015", "Vizer et al., 2009", "Zhai and Barreto, 2008", "Giakoumis et al., 2012", "Gjoreski et al., 2016", "Gottlieb, 2013", "Koelstra et al., 2011", "Song and\nKim, 2017", "FG 2018", "DCII 2008", "ICIAS2012"], "entities_from_reference": ["Acerbi", "Rovini E", "Betti S", "Tirri A", "Sirianni A", "Agrimi J", "Eusebi L", "Cavallo F", "Italian Forum", "Springer", "Acharya UR", "Joseph KP", "Kannathal N", "Lim CM", "Suri JS", "Medical", "Sree SV", "Ang PCA", "Yanti R", "Adaler A", "Balkan E", "Global", "Mahmoud M", "Baltrusaitis T", "Robinson P", "Affective Computing", "Khan HM", "Choi J", "Ahn JW", "Ku Y", "Kim HC", "Ahuja R", "Banga A", "Dubuisson S", "Detyniecki M", "Chetouani M", "Workshops", "Gesture Recognition", "Spodenkiewicz M", "Dubuiss S", "Cohen D", "Aamir Arsalan1", "Malarvili M", "Whitttaker AC", "Stress", "Islam SMF", "Khan AS", "Ahmed F", "Rahman MM", "Bertozzi D", "Poletti F", "Benini L", "Jantsch A", "Bechara M", "Khalifeh H", "Hajjar M", "Nabiev R", "Jonsson S", "Fawzi M", "Tang TB", "Badruddin N", "Kiguchi M", "Hani AFM", "Petersen KL", "Adrenocortical", "Alamudun F", "Khan H", "Ahmed B", "Pervasive Computing", "Golden LH", "Izzo Jr JL", "Forrest A", "Niles CR", "Niswander PR", "Barlow JC", "Alonso J", "Romero S", "Ballester M", "Antonijoan R", "Andr", "Funk P", "Arnrich B", "Setz C", "La Marca R", "Ehlert U", "Arsalan A", "Majid M", "Anwar SM", "Bagci U", "Annual International Conference", "Biology Society", "Butt AR", "Data", "Attallah O", "Ayata D", "Yaslan Y", "Kamas", "Istanbul", "Electronics", "Baglioni C", "Lombardo C", "Bux E", "Hansen S", "Salveta C", "Biello S", "Violani C", "Espie CA", "Baltac S", "Baltaci S", "Gokcay D", "Barreto A", "Zhai J", "Adjouadi M", "Rishe N", "Gao Y", "Bartlett DL", "Lukowicz P", "Beatty J", "Berger H", "Archiv", "Bernardi L", "Valenti C", "Castoldi S", "Passino C", "Spadacini G", "Sleight P", "Lova RM", "Acerbi G", "Santarelli L", "Cabiati M", "Del Ry S", "Bevilacqua F", "Engstr", "Backlund P", "Birjandtalab J", "Cogan D", "Pouyan MB", "Nourani M", "Bitsika V", "Sharpley CF", "Sweeney JA", "Physiology", "Blechert J", "Lajtman M", "Michael T", "Margraf J", "Wilhelm FH", "Lepri B", "Ferron M", "Pianesi F", "Daily", "Murugappan M", "Yaacob S", "Phongsuphap S", "Bordnick PS", "Traylor AC", "Carter BL", "Graap KM", "Boucher P", "Plusquellec P", "Acute", "Maaoui C", "Pruski A", "Remote", "Bradley MM", "Codispoti M", "Cuthbert BN", "Lang PJ", "Miccoli L", "Escrig MA", "Brkhus A", "Engedal K", "Laake K", "Brantley PJ", "Jones GN", "Waggoner CD", "Rappaport NB", "Brewin CR", "Andrews B", "Rose S", "Kirk M", "Bryant RA", "Moulds ML", "Guthrie RM", "Kroisamer JS", "Gerendas B", "Roberts P", "Rezar S", "Eibenberger K", "Sacu S", "Visual Science", "Bullinger M", "Naber D", "Pickar D", "Cohen RM", "Kalin NH", "Pert A", "Bunney Jr", "Cairns DA", "Hansen JH", "Calibo TK", "Blanco JA", "Firebaugh SL", "Ersoy C", "Chalabianloo N", "Ekiz D", "Riva G", "Sitaram R", "Veit R", "Begliomini C", "Birbaumer N", "Biological", "Carneiro D", "Castillo JC", "Novais P", "Neves J", "Pego JM", "Sousa N", "Hybrid Artificial Intelligence Systems", "Augusto JC", "Payne N", "Carroll D", "Hunt K", "Macintyre S", "Blood", "Psychosomatic Medicine", "Phillips AC", "Der G", "Benzeval M", "Castaldo R", "Xu W", "Melillo P", "Pecchia L", "Santamaria L", "James C", "Asheeb A", "Dash S", "Retna N", "Teja KVR", "Issac TG", "Indian Journal", "Chang Kh", "Fisher D", "Canny J", "Hartmann B", "Body Area Networks", "Vila G", "Godin C", "Labyt E", "Sakri O", "Campagne A", "Charlton PH", "Celka P", "Farukh B", "Chowienczyk P", "Alastruey J", "Chauhan U", "Reithinger N", "Mackey JR", "Adjunct", "Chen Ll", "Zhao Y", "Ye Pf", "Zhang J", "Zou Jz", "Chen T", "Yuen P", "Richardson M", "Liu G", "Chen X", "Liu A", "Peng H", "Ward RK", "Cho HM", "Park H", "Dong SY", "Cho Y", "Julier SJ", "Marquardt N", "Texas", "Kim M", "Chun C", "Ciman M", "Wac K", "Cinaz B", "Clays E", "De Bacquer D", "Crasset V", "Kittel F", "De Smet P", "Kornitzer M", "Karasek R", "De Backer", "Clifford GD", "Uni", "Oxford Cohen S", "Kamarck T", "Mermelstein R", "Colunas MF", "Fernandes JMA", "Oliveira IC", "Cunha JPS", "Mobile Computing Conference", "Critchley HD", "Schell A", "Filion D", "De Berker AO", "Tirole M", "Rutledge RB", "Cross GF", "Dolan RJ", "Bestmann S", "Scientific", "De Luca CJ", "Csenki L", "Empirical Text", "Culture", "Dedovic K", "Renwick R", "Mahani NK", "Engert V", "Lupien SJ", "Pruessner JC", "Delmastro F", "Di Martino F", "Dolciotti C", "Acta Informatica Medica", "Dempsey LA", "Nature", "Deng Y", "Wu Z", "Chu CH", "Yang T", "Derakhshan A", "Mikaeili M", "Khalilzadeh MA", "Mohammadian A", "Derogatis LR", "Spencer P", "Pearson Up", "Saddle River", "Forget H", "Fiset D", "Blais C", "Vidrascu L", "Ninth International Conference", "Spoken Language", "Master", "Faculty", "Mathematics", "Delft University", "Netherlands DHondt F", "Lassonde M", "Collignon O", "Dubarry AS", "Robert M", "Rigoulot S", "Honor", "Lepore F", "Sequeira H", "Dickerson SS", "Kemeny ME", "Dillon A", "Kelly M", "Robertson IH", "Robertson DA", "Rider RL", "Dorrian J", "Rogers NL", "Cizman Z", "Goldenstein SK", "Vogler C", "Venkataraman S", "Aviation", "B172B182 Dufey M", "Mayol R", "Universitas", "Duru DG", "Duru AD", "Barkana DE", "Sanli O", "Ozkan M", "Ekman P", "Friesen W", "Hager J", "Elgendi M", "Ellermeier W", "Westphal W", "Engelhardt C", "Malfroy Camine V", "Ingram D", "Farron A", "Pioletti D", "Terrier A", "Merla A", "Grant JA", "Cardone D", "Tusche A", "Singer T", "Anthenien L", "Dayer E", "Bosshard C", "Gaillard R", "Music", "Schweizerische Medizinische", "Politti JC", "Felice CJ", "Biomedical", "Farnsworth B", "Kothgassner OD", "Hetterle T", "Beutl L", "Hlavacs H", "Afraid", "Cyberpsychology", "Anishchenko L", "Fernandez R", "Picard RW", "Speech", "Fink G", "Academic", "Finsen L", "Sgaard K", "Jensen C", "Borg V", "Christensen H", "Muscle", "Rabins P", "Lucas MJ", "Eastham J", "Tolvanen A", "Myllym", "Rantala S", "Korpela R", "Peuhkuri K", "Kolehmainen M", "Puttonen S", "Lappalainen R", "Fox E", "Mathews A", "Calder AJ", "Yiend J", "Anxiety", "Frings C", "Larra MF", "Moeller B", "Sch", "Gaggioli A", "Pioggia G", "Tartarisco G", "Baldus G", "Ferro M", "Cipresso P", "Serino S", "Popleteev A", "Gabrielli S", "Maimone R", "Gao H", "Thiran JP", "Image Processing", "Garcia J", "Gustavson AR", "Osmani V", "Mayora O", "Garrett VD", "Gasperin D", "Netuveli G", "Dias-da Costa JS", "Pattussi MP", "Cadernos", "Jetha MK", "Segalowitz SJ", "Gedam S", "Paul S", "Frounchi J", "Farnam A", "Machine", "Gharavian D", "Giakoumis D", "Drosou A", "Tzovaras D", "Hassapis G", "Giannakaki K", "Giannakakis G", "Farmaki C", "Sakkalis V", "Grigoriadis D", "Tsiknakis M", "Pediaditis M", "Manousos D", "Kazantzaki E", "Simos PG", "Marias K", "Biomedical Signal", "Chaniotakis V", "Health Informatics", "Simos P", "Simantiraki O", "Roniotis A", "Review", "Affective Computing Gillani SF", "Saeed SMU", "Shabbir Z", "Habib F", "Regal C", "Schmidt M", "Gjoreski M", "Gjoreski H", "Lutrek M", "Gams M", "Lustrek M", "Nature Reviews", "Goetsch VL", "Wiebe DJ", "Veltum LG", "Van Dorsten", "Goldberger AL", "Glass L", "Hausdorff JM", "Ivanov PC", "Mark RG", "Mietus JE", "Moody GB", "Peng CK", "Stanley HE", "Goreczny AJ", "Buss RR", "Waters WF", "Gottlieb BH", "Springer Science", "Nahar NK", "Hayes JR", "Sheedy JE", "Optometry", "Vision Science", "Greene J", "Smith R", "Gardiner M", "Timbury G", "Gross C", "Seeba K", "Scheibe S", "Behavior Research Methods", "Gunawardhane SD", "De Silva PM", "Kulathunga DS", "Arunatileka SM", "Gunes H", "Piccardi M", "Network", "Gurung RA", "Haak M", "Bos S", "Panic S", "Rothkrantz L", "Steiner TJ", "Grant EC", "Clifford Rose", "Language", "Hajcak G", "Dennis TA", "Halim Z", "Rehan M", "Hall M", "Vasko R", "Buysse D", "Ombao H", "Chen Q", "Cashmere JD", "Kupfer D", "Thayer JF", "Hamid NHA", "Sulaiman N", "Aris SAM", "Murat ZH", "Taib MN", "Karam R", "Mourad R", "Kurdi M", "Patil S", "Kim W", "Rahurkar M", "Ruzanski E", "Meyerhoff J", "Robust", "Hanson HM", "Maragos P", "Potamianos A", "Bryant R", "Clinical Psychology", "Harvey AG", "Hasan MJ", "Kim JM", "Hassellund SS", "Flaa A", "Sandvik L", "Kjeldsen SE", "Rostrup M", "Hayashi T", "Okamoto E", "Nishimura H", "Ishii R", "Ukai S", "Image", "Lech M", "Maddage N", "Allen N", "Healey JA", "Massachusetts Institute", "Kudielka BM", "Hemmeter U", "St", "Mager R", "Kuntze M", "Hennig J", "Amditis A", "Bekiaris A", "Bullinger A", "Henelius A", "Herborn KA", "Graves JL", "Jerem P", "Evans NP", "Nager R", "Skin", "Hernandez J", "Morris RR", "Paredes P", "Roseway A", "Czerwinski M", "Hines EA", "Clin Proc", "Stress Management", "Booktango Hjortskov N", "Blangsted AK", "Fallentin N", "Lundberg U", "Zimmerli WD", "Hoffmann E", "Stress Report", "Hollien HF", "Academic Press Hong K", "Honma M", "Applied Biomedical", "Niazmand V", "Hou X", "Liu Y", "Sourina O", "Tan YRE", "Wang L", "Man", "Ertin E", "Nakajima M", "Kumar S", "Huang MX", "Li J", "Ngai G", "Leong HV", "Uutela K", "Van Gils M", "Kym", "Paloheimo M", "Rantanen M", "Takala P", "Vierti", "Hynynen E", "Konttinen N", "Kinnunen U", "Kyr", "Rusko H", "Ioannou S", "Gallese V", "Jobb", "Majn", "Nagy P", "Periodica Polytechnica Electrical Engineering", "Jun G", "Smitha KG", "Kadambe S", "Study", "Odagaki M", "Hosaka H", "Chong CC", "Watanabe F", "Kajantie E", "Phillips DI", "Kalas MS", "Momin B", "Kang J", "Karthikeyan P", "Katsis", "Katertsidis NS", "Fotiadis DI", "Kaufer DI", "Cummings JL", "Christine D", "Bray T", "Castellon S", "Masterman D", "Ketchel P", "Kelly MM", "Tyrka AR", "Anderson GM", "Carpenter LL", "Kemp AH", "Quintana DS", "Felmingham KL", "Matthews S", "Jelinek HF", "Keshan N", "Parimi P", "Khalfa S", "Bella SD", "Roy M", "Peretz", "Homam SM", "Khosrowabadi R", "Quek C", "Ang KK", "Tung SW", "Heijnen M", "Kim D", "Seo Y", "Cho J", "Cho CH", "Kimble MO", "Fleming K", "Bandy C", "Kim J", "Zambetti A", "Kirschbaum C", "Pirke KM", "Knight WE", "Rickard NS", "Yang HC", "Sim KB", "Kocielnik R", "Sidorova N", "Maggi FM", "Ouwerkerk M", "Westerink JH", "Muhl C", "Soleymani M", "Lee JS", "Yazdani A", "Ebrahimi T", "Pun T", "Nijholt A", "Patras", "Koldijk S", "Sappelli M", "Verberne S", "Neerincx MA", "Kraaij W", "Kyritsis AI", "Deriaz M", "Konstantas D", "Felnhofer A", "Palme R", "Glenk LM", "Krantz G", "Forsman M", "Kreibig SD", "Biolog", "Kurniawan H", "Maslov AV", "Pechenizkiy M", "Resch B", "Sagl G", "Petutschnig A", "Werner C", "Niederseer D", "Osborne T", "Pykett J", "Lackner HK", "Papousek", "Batzel JJ", "Roessler A", "Scharfetter H", "Eleck RE", "Laretzaki G", "Plainis S", "Vrettos", "Chrisoulakis A", "Bitsios P", "Larsson SE", "Larsson R", "Zhang Q", "Cai H", "Oberg P", "Lasaitis C", "Ribeiro RL", "Bueno OFA", "Jornal Brasileiro", "Lee BG", "Chung WY", "Lee DS", "Chong TW", "Lee Mh", "Yang G", "Lee HK", "Bang S", "Lee RA", "Jung ME", "Lee Y", "Lee B", "Lee M", "Telemedicine", "Rothkrantz LJ", "Van Leeuwen DA", "Wiggers P", "Burghouts GJ", "Levine JA", "Cooper M", "Lewis RS", "Weekes NY", "Wang TH", "Li F", "Xu P", "Zheng S", "Chen W", "Lu S", "Liu Z", "Li Z", "Snieder H", "Su S", "Ding X", "Treiber FA", "Wang X", "Liao W", "Zhang W", "Zhu Z", "Ji Q", "Pattern Recognition", "Katsanos C", "Sotiropoulos D", "Xenos M", "Karousos N", "Lidberg L", "Wallin BG", "Lien R", "Neijts M", "Willemsen G", "Geus EJ", "Liew WS", "Seera M", "Loo CK", "Lim E", "Kubota N", "Lim YM", "Ayesh A", "Stacey M", "Wu J", "Chan SC", "John L", "Linden W", "Liu D", "Ulrich M", "Lohani M", "Gupta R", "Srinivasan N", "Lombardo DM", "Vick RS", "Nusslock R", "George C", "Kovacs M", "Lovallo W", "Frauendorfer D", "Rabbi M", "Mast MS", "Campbell AT", "Choudhury T", "Lucini D", "Clerici M", "Pagani M", "Luijcks R", "Hermens HJ", "Bodar L", "Vossen CJ", "Lousberg R", "Experi", "Kadefors R", "Melin B", "Palmerud G", "Hassm", "Dohns IE", "Luo L", "Xiao L", "Miao D", "Luo X", "Zhou J", "Yu C", "Miao C", "Wang T", "Shi Y", "Kameyama Ki", "Annual ACM Conference", "Systems", "Madden K", "Savard G", "Clinical Physiology", "Maeda Y", "Sekine M", "Tamura T", "Maier E", "Reimer U", "Laurenzi E", "Ridinger M", "Ulmer T", "Malik M", "Marazziti D", "Di Muro A", "Castrogiovanni P", "Markova V", "Ganchev T", "Kalinkov K", "Cooper NR", "Segrave R", "Geeraert N", "Martinez R", "Irigoyen E", "Arruti A", "Mart", "Muguerza J", "Maslach C", "Jackson S", "Leiter M", "Zalaquett C", "Wood R", "Scarecrow Masood K", "Alghamdi MA", "Gontarek S", "Picard R", "Bracale M", "Formisano C", "Bracale U", "Benezeth Y", "De Oliveira P", "Chappe J", "Yang F", "Affective Computing Minguillon J", "Pelayo F", "Mohan PM", "Nagarajan V", "Das SR", "Kashima M", "Sato K", "Watanabe M", "Affective", "Ferreira J", "Seoane F", "Mokhayeri F", "Otsuka A", "Kohara K", "Mikami H", "Katahira K", "Tsunetoshi T", "Higashimori K", "Ohishi M", "Yo Y", "Ogihara T", "Clinical Autonomic", "Mosley Jr TH", "Penzien DB", "Johnson CA", "Wittrock DA", "Andrew ME", "Payne TJ", "Mozos OM", "Sandulescu V", "Andrews S", "Ellis D", "Bellotto N", "Dobrescu R", "Ferrandez JM", "Muaremi A", "Bexheti A", "Gravenhorst F", "Khalil M", "Shahin A", "Mourad A", "Murray IR", "Baber C", "Muthukumar K", "Nachiappan V", "Nater UM", "Rohleder N", "Gaab J", "Berger S", "Jud A", "Nath RK", "Thapliyal H", "Nhan BR", "Chau T", "Nikula R", "Nomikos MS", "Opton Jr E", "Averill JR", "Nomura S", "Mizuno T", "Nozawa A", "Asano H", "Ide H", "Kansei Engineering", "Institute", "York University Nozawa A", "Tacano M", "P01007 Nwe TL", "Foo SW", "De Silva LC", "Onorati F", "Barbieri R", "Mauri M", "Russo V", "Mainardi L", "Praamstra P", "Clinical", "Orsila R", "Virtanen M", "Luukkaala T", "Tarvainen M", "Karjalainen P", "Viik J", "Savinainen M", "Nyg", "Oskooei A", "Chau SM", "Weiss J", "Sridhar A", "Michel B", "Palacios D", "Rodellar V", "Tompkins WJ", "Pandiyan PM", "Panigrahy SK", "Jena SK", "Turuk AK", "Research Journal", "Parsons TD", "Rizzo AA", "Partala T", "Surakka V", "Pupil", "Levine J", "Biology Magazine", "Baukol P", "Tsiamyrtzis P", "Shastri D", "Wesley A", "Zhou Y", "Lindner P", "Buddharaju P", "Joseph R", "Mandapati A", "Dunkin B", "Mirzaei MA", "Tedesco A", "Chardonnet JR", "Benedetto S", "Baccino T", "Pehlivano", "Durmazlar N", "Balkanc D", "Erciyes Medical", "Hu B", "Zheng F", "Fan D", "Zhao W", "Yang Y", "Cai Q", "Pertaub DP", "Slater M", "Barker C", "Virtual", "Pflanzer R", "Vyzas E", "Healey J", "Toward", "Devereux RB", "James GD", "Gerin W", "Landsbergis P", "Schnall PL", "Schwartz JE", "S17985 Mental Stress", "Arch JJ", "Lam CW", "Craske MG", "Pluntke U", "Gerke S", "Optics", "Souza MDP", "Bastos Filho TF", "Better", "Safer Living", "Soler JM", "Leclercq V", "Denys P", "Lieberman P", "Pujol J", "Vendrell P", "Deus J", "Junqu", "Bello J", "Capdevila A", "Puri C", "Olson L", "Starren J", "Hermans EJ", "Marle HJ", "Luo J", "Racine N", "Khu M", "Reynolds K", "Guilcher G", "Schulte F", "Current Oncology", "Rahman T", "Ghosh AK", "Shuvo M", "Ramalho R", "Orsolini L", "Adiukwu F", "Larnaout A", "Costa MP", "Grandinetti P", "Shalbafan M", "Brain", "Rastgoo MN", "Nakisa B", "Rakotonirainy A", "Chandran V", "Tjondronegoro D", "Reanaree P", "Tananchana P", "Narongwongwathana W", "Pintavirooj C", "Reaz MBI", "Hussain MS", "Mohd-Yasin F", "Reinhardt T", "Schmahl C", "Bohus M", "Reisman S", "Ren P", "Renaud P", "Blondin JP", "Rey JMG", "Arza A", "Aguilo J", "Kagan J", "Burns VE", "Drayson M", "Walkey DG", "Dale S", "Riss", "Sandsj", "Dohns", "Ritz T", "Steptoe A", "Costa M", "Rodrigues M", "Gonc", "Keystrokes", "Bhatti AM", "Khalid H", "Saeed U", "Muhammad S", "Awais M", "Alnowami M", "Jeong MG", "Lim SK", "Won K", "Woo JM", "Sanei S", "John Wiley", "Sons Sano A", "Humaine Association Conference", "Santos Sierra A", "Avila CS", "Casanova JG", "Pozo GB", "Vera VJ", "Sardeshpande K", "Thool VR", "Sarikaya R", "Gowdy JN", "Sawangjai P", "Hompoonsup S", "Leelaarporn P", "Kongwudhikunakorn S", "Wilaiprasitporn T", "Schleifer LM", "Kerick SE", "Cram JR", "Ley R", "Hatfield BD", "Schmidt P", "Reiss A", "Duerichen R", "Marberger C", "Van Laerhoven", "Landsbergis PA", "Warren K", "Schneegass S", "Broy N", "Heinrich F", "Schmidt A", "Schubert C", "Lambertz M", "Nelesen R", "Bardwell W", "Choi JB", "Dimsdale J", "Johnson JG", "Psychology Press Schulz P", "Schlotz W", "Ibric S", "Nisic J", "Suljanovic N", "Mujcic A", "Dirlewanger M", "Rey V", "Schneiter P", "Tappy L", "Miller GE", "Selvaraj N", "Gil Y", "Lee J", "Hybrid Information Technology", "Lee JT", "Seraganian P", "Szabo A", "Brown TG", "Schumm J", "Shan Y", "Li S", "Machine Learning", "Sharma N", "Gedeon T", "Applied Soft", "Dhall A", "Goecke R", "Video", "Papadakis M", "Bass B", "Ruiz N", "Taib R", "Choi E", "Chen F", "Nguyen MH", "Blitz P", "Fisk S", "Torre F", "Smailagic A", "Siewiorek DP", "Seongo H", "Cha D", "Yoon Y", "Yoon H", "Vol", "Shon D", "Im K", "Park JH", "Lim DS", "Jang B", "Pampouchidou A", "Simoes E", "Roark R", "Berman S", "Esler L", "Murphy J", "Simpson H", "Molloy F", "Psychophys", "Sincero SM", "Singh M", "Queyam AB", "Clark DM", "Sloan R", "Shapiro P", "Bagiella E", "Boni S", "Paik M", "Bigger Jr J", "Steinman R", "Gorman J", "Sondhi S", "Khan M", "Vijay R", "Salhan AK", "Song SH", "Kim DK", "Soury M", "Pratapa SK", "Mahant S", "Prasanna VD", "Murthy OR", "Stavropoulos V", "Gomez R", "Steen E", "Beard C", "Liew L", "Griffiths MD", "Steeneken HJ", "Owen N", "Flower L", "Stern RM", "Ray WJ", "Quigley KS", "Oxford University Press", "Stroop", "Stroud LR", "Salovey P", "Epel ES", "Stula T", "Sturmbauer SC", "Shields GS", "Hetzel EL", "Slavich GM", "Styliadis C", "Ioannides AA", "Bamidis PD", "Papadelis C", "Xia L", "Malik AS", "Advanced Systems", "Subhani AR", "Mumtaz W", "Kamil N", "Saad NM", "Nandagopal N", "Saad MNBM", "Kamel N", "Morimoto K", "Obata A", "Koizumi H", "Maki A", "Lias S", "Mustafa M", "Rashid NA", "Networks", "Huggenberger HJ", "Svetlak M", "Bob P", "Cernik M", "Kukleta M", "Autonomic Neuroscience", "Sysoev M", "Kos A", "Pogacnik M", "Taamneh S", "Dcosta M", "Khatri A", "Manser M", "Ferris T", "Wunderlich R", "Taelman J", "Vandeput S", "Spaepen A", "Van Huffel S", "Vlemincx E", "Tanev G", "Saadi DB", "Hoppe K", "Sorensen HB", "Klein LC", "Lewis BP", "Gruenewald TL", "Updegraff JA", "Khalus V", "Labrado C", "Natl Med", "Thommessen B", "Braekhus A", "Oksengaard AR", "Tomaka J", "Blascovich J", "Swart L", "Tomarken AJ", "Davidson RJ", "Henriques JB", "Torii M", "Yamasaki M", "Sasaki T", "Nakayama H", "Mahfouf M", "Thuraisingham R", "Wijesuriya N", "Nguyen H", "Craig A", "Joseph T G", "Daniel P", "Brain Science", "Begdache L", "Won D", "Koh A", "Tugade MM", "Fredrickson BL", "Tulen J", "Moleman P", "Van Steenis H", "Boomsma F", "Pharmacology Biochemistry", "Tyagi P", "Arora AS", "Rastogi V", "Uddin MT", "Canavan S", "Russo M", "Sikora M", "Health Care", "Herman JP", "Ulstein", "Bruun Wyller T", "Wyller TB", "Ushiyama K", "Ogawa T", "Ishii M", "Ajisaka R", "Sugishita Y", "Vanitha L", "Suresh G", "Hybrid", "Advanced Computing", "Vanitha V", "Krishnan P", "Vasavi S", "Neeharica P", "Wadhwa B", "Annual Information Technology", "Mobile Communication Conference", "Raghav R", "Phani K", "Vaidyanathan V", "Eeg", "Green Computing", "Kallio J", "Kyll", "Nieminen M", "Lindholm M", "Gimelfarb G", "Vinkers CH", "Penning R", "Verster JC", "Klaessens JH", "Olivier B", "Kalkman CJ", "Visnovcova Z", "Mestanik M", "Javorka M", "Mokra D", "Gala M", "Jurko A", "Calkovska A", "Vizer LM", "Zhou L", "Sears A", "Vuksanovi", "Gal V", "Waggoner C", "Louisiana State University", "Baton Rouge", "Louisiana Wang JS", "Lin CW", "Yang YTC", "Shi Z", "Zhang Y", "Shen J", "Wang Ls", "Yue X", "V1347 Weidner G", "Friend R", "Ficarrotto TJ", "Mendell NR", "Psychosomatic Medicine Werner EE", "Weyers P", "Hefele C", "Pauli P", "Wielgosz J", "Schuyler BS", "Lutz A", "Wijsman J", "Grundlehner B", "Hermens H", "Trapezius", "Wireless Health", "Liu H", "Williams CE", "Stevens KN", "Willigenburg NW", "Daffertshofer A", "Kingma", "Van Die", "Winn B", "Whitaker D", "Elliott DB", "Phillips NJ", "Wolpe J", "Audio", "Womak B", "Hao M", "Nwe TL", "Guan C", "Yao X", "Jitsuhiro T", "Miyajima C", "Kitaoka N", "Takeda K", "Ragha L", "Speech Technology", "Sim JK", "Cho YH", "Pastor JM", "Zubair M", "Yoon C", "Kim H", "Smart"]}{"title": ["Data set creation and empirical analysis for detecting signs of depression from social media postings"], "authors": ["[arxiv.Result.Author('Kayalvizhi S'), arxiv.Result.Author('Thenmozhi D')]"], "link": ["http://arxiv.org/pdf/2202.03047v1"], "summary": "Depression is a common mental illness that has to be detected and treated at\nan early stage to avoid serious consequences. There are many methods and\nmodalities for detecting depression that involves physical examination of the\nindividual. However, diagnosing mental health using their social media data is\nmore effective as it avoids such physical examinations. Also, people express\ntheir emotions well in social media, it is desirable to diagnose their mental\nhealth using social media data. Though there are many existing systems that\ndetects mental illness of a person by analysing their social media data,\ndetecting the level of depression is also important for further treatment.\nThus, in this research, we developed a gold standard data set that detects the\nlevels of depression as `not depressed', `moderately depressed' and `severely\ndepressed' from the social media postings. Traditional learning algorithms were\nemployed on this data set and an empirical analysis was presented in this\npaper. Data augmentation technique was applied to overcome the data imbalance.\nAmong the several variations that are implemented, the model with Word2Vec\nvectorizer and Random Forest classifier on augmented data outperforms the other\nvariations with a score of 0.877 for both accuracy and F1 measure.", "entities_include_in_text": [], "entities_from_reference": ["Tuka Al Hanai", "Mohammad M Ghassemi", "James R Glass", "Hamdi Dibeklio", "Zakia Hammal", "Ying Yang", "Jeffrey F Cohn", "Multimodal", "Sharifa Alghowinem", "Michael Wagner", "Julien Epps", "Matthew Hyett", "Gordon Parker", "Michael Breakspear", "Affective Computing", "Arindam Jati", "Prashanth Gurunath Shivakumar", "Sandeep Nallan Chakravarthula", "Panayiotis Georgiou", "Jana M Havigerova", "Jiri Haviger", "Dalibor Kucera", "Petra Hoffmannova", "Maxim Stankevich", "Andrey Latyshev", "Evgenia Kuminskaya", "Ivan Smirnov", "Oleg Grigoriev", "Michelle Renee Morales", "Rivka Levitan", "Speech", "Misato Hiraga", "Atreyee Mukherjee", "Zeeshan Ali Sayyed", "Matthew Millard", "Language Cognition", "Johannes C Eichstaedt", "Robert J Smith", "Raina M Merchant", "Lyle H Ungar", "Patrick Crutchley", "Daniel Preo", "David A Asch", "Andrew Schwartz", "Facebook", "Andrew G Reece", "Andrew J Reagan", "Katharina LM Lix", "Peter Sheridan Dodds", "Christopher M Danforth", "Ellen J Langer", "Sho Tsugawa", "Yusuke Kikuchi", "Fumio Kishino", "Kosuke Nakajima", "Yuichi Itoh", "Hiroyuki Ohsaki", "Mandar Deshpande", "Vignesh Rao", "Chenhao Lin", "Pengwei Hu", "Hui Su", "Shaochun Li", "Jing Mei", "Jie Zhou", "Henry Leung", "Thin Nguyen", "Dinh Phung", "Bo Dao", "Svetha Venkatesh", "Michael Berk", "Affective", "Yevhen Tyshchenko", "Nature Precis", "Inst", "Tartu", "Science", "David E Losada", "Fabio Crestani", "Javier Parapar", "Springer", "Michael M. Tadesse", "Hongfei Lin", "Bo Xu", "Liang Yang", "Inna Pirina", "Coltekin", "Health Applications Workshop", "Shared Task", "Hannah Yao", "Sina Rashidian", "Xinyu Dong", "Hongyi Duanmu", "Richard N Rosenthal", "Fusheng Wang", "Machine", "Nick Boettcher", "Massimo Poesio", "Jacob Cohen", "Machine Learning", "Richard Landis", "Gary G Koch", "Health", "Michel", "J. Vanderplas", "Machine Learning Research", "Martin F Porter", "Jeffrey Pennington", "Richard Socher", "Christopher D Manning", "Glove", "Smote"]}{"title": ["Tractable Boolean and Arithmetic Circuits"], "authors": ["[arxiv.Result.Author('Adnan Darwiche')]"], "link": ["http://arxiv.org/pdf/2202.02942v1"], "summary": "Tractable Boolean and arithmetic circuits have been studied extensively in AI\nfor over two decades now. These circuits were initially proposed as \"compiled\nobjects,\" meant to facilitate logical and probabilistic reasoning, as they\npermit various types of inference to be performed in linear-time and a\nfeed-forward fashion like neural networks. In more recent years, the role of\ntractable circuits has significantly expanded as they became a computational\nand semantical backbone for some approaches that aim to integrate knowledge,\nreasoning and learning. In this article, we review the foundations of tractable\ncircuits and some associated milestones, while focusing on their core\nproperties and techniques that make them particularly useful for the broad aims\nof neuro-symbolic AI.", "entities_include_in_text": [], "entities_from_reference": ["Durgesh Agrawal", "Yash Pote", "Kuldeep S. Meel", "Jatin Arora", "Supratik Chakraborty", "Shankara Narayanan Krishna", "Divya Raghunathan", "Antoine Amarilli", "Marcelo Arenas", "Pablo Barcelo Leopoldo Bertossi", "Mikael Monet", "Gilles Audemard", "Frederic Koriche", "Pierre Marquis", "Paul Beame", "Jerry Li", "Sudeepa Roy", "Dan Suciu", "Exact", "Database Syst.", "Vaishak Belle", "Luc De Raedt", "Tarek R. Besold", "Artur", "Sebastian Bader", "Howard Bowman", "Pedro Domingos", "Pascal Hitzler", "Luis C. Lamb", "Daniel Lowd", "Priscila Machado Vieira Lima", "Leo", "Gadi Pinkas", "Hoifung Poon", "Gerson Zaverucha", "Matthias Buttkus", "Florent Capelli", "Stefan Mengel", "Friedrich Slivovsky", "Marco Cadoli", "Francesco M. Donini", "Adnan Darwiche", "Artif", "Mark Chavira", "Alessandro Saffiotti", "Book Center", "Lecture Notes", "Springer", "Arthur Choi", "Machine Learning Research", "Dynamic", "Machine Learning", "Doga Kisa", "Yujia Shen", "Nazgol Tavabi", "Structured", "Guy Van", "Broeck", "Ruocheng Wang", "Yexiang Xue", "Stephen", "Meihua Dang", "Antonio Vergari", "Strudel", "Non Class", "Logics", "Dieter Fensel", "Fausto Giunchiglia", "Deborah L. McGuinness", "Williams", "Knowledge Representation", "Morgan Kaufmann", "Bayesian Networks", "Cambridge University Press", "Practical Approaches", "Hard Problems", "Causal", "Auguste Hirth", "Stefan Szeider", "Dagstuhl Reports", "Anton Lykov", "Maximilian Schleich", "Pierfrancesco Ardino", "Jacopo Gobbi", "Paolo Morettin", "Stefano Teso", "Andrea Passerini", "Paulius Dilkas", "Anton Dries", "Angelika Kimmig", "Wannes Meert", "Joris Renkens", "Jonas Vlasselaer", "Dimitar Sht", "Shterionov", "Bernd Gutmann", "Ingo Thon", "Gerda Janssens", "Abram L. Friesen", "Pedro M. Domingos", "Garcez", "Jordan Gergov", "Christoph Meinel", "Hitzler", "Md Kamruzzaman Sarker", "Steven Holtzen", "Todd D. Millstein", "Huang", "Xuanxiang Huang", "Yacine Izza", "Alexey Ignatiev", "Martin C. Cooper", "Nicholas Asher", "Joao", "John T. Gill III", "Kumar Jha", "Kimmig", "Algebraic", "Applied Logic", "Roland H.", "Kushagra Chandak", "Akshat Kumar", "Robin Manhaeve", "Sebastijan Dumancic", "Thomas Demeester", "Denis Deratani Maua", "Cassio Polpo", "Alejandro Molina", "Karl Stelzner", "Robert Peharz", "Pranav Subramani", "Nicola Di Mauro", "Pascal Poupart", "Kristian Kersting", "Christian J. Muise", "J. Christopher Beck", "Eric I. Hsu", "Dsharp", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata", "James P. Delgrande", "Frank Wolter", "Cape Town", "James D. Park", "Robert Gens", "Franz Pernkopf", "Pattern Analysis", "Machine Intelligence", "Steven Lang", "Martin Trapp", "Zoubin Ghahramani", "Einsum", "Bound Computation Algorithm", "Giuseppe Marra", "Dan Roth", "Rajhans Samdani", "Mach", "Saad", "Martin C. Rinard", "Vikash K. Mansinghka", "Tian Sang", "Fahiem Bacchus", "Henry A. Kautz", "Toniann Pitassi", "Bart Selman", "Aman Bansal", "Shubham Sharma", "Rahul Gupta", "Subhajit Roy", "Anchal Goyanka", "Haiying Huang", "Weijia Shi", "Andy Shih", "Solomon Eyal Shimony", "Leslie G. Valiant", "Anji Liu", "Klaus W. Wagner", "Acta Informatica", "Ziwei Xu", "Mohan S. Kankanhalli", "Harold Soh", "Jingyi Xu", "Zilu Zhang", "Yitao Liang", "Zhao", "Geoffrey J. Gordon"]}{"title": ["Deep Convolutional Learning-Aided Detector for Generalized Frequency Division Multiplexing with Index Modulation"], "authors": ["[arxiv.Result.Author('Merve Turhan'), arxiv.Result.Author('Ersin \u00d6zt\u00fcrk'), arxiv.Result.Author('Hakan Ali \u00c7\u0131rpan')]"], "link": ["http://arxiv.org/pdf/2202.02876v1"], "summary": "In this paper, a deep convolutional neural network-based symbol detection and\ndemodulation is proposed for generalized frequency division multiplexing with\nindex modulation (GFDM-IM) scheme in order to improve the error performance of\nthe system. The proposed method first pre-processes the received signal by\nusing a zero-forcing (ZF) detector and then uses a neural network consisting of\na convolutional neural network (CNN) followed by a fully-connected neural\nnetwork (FCNN). The FCNN part uses only two fully-connected layers, which can\nbe adapted to yield a trade-off between complexity and bit error rate (BER)\nperformance. This two-stage approach prevents the getting stuck of neural\nnetwork in a saddle point and enables IM blocks processing independently. It\nhas been demonstrated that the proposed deep convolutional neural network-based\ndetection and demodulation scheme provides better BER performance compared to\nZF detector with a reasonable complexity increase. We conclude that\nnon-orthogonal waveforms combined with IM schemes with the help of deep\nlearning is a promising physical layer (PHY) scheme for future wireless\nnetworks", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Locally Differentially Private Distributed Deep Learning via Knowledge Distillation"], "authors": ["[arxiv.Result.Author('Di Zhuang'), arxiv.Result.Author('Mingchen Li'), arxiv.Result.Author('J. Morris Chang')]"], "link": ["http://arxiv.org/pdf/2202.02971v1"], "summary": "Deep learning often requires a large amount of data. In real-world\napplications, e.g., healthcare applications, the data collected by a single\norganization (e.g., hospital) is often limited, and the majority of massive and\ndiverse data is often segregated across multiple organizations. As such, it\nmotivates the researchers to conduct distributed deep learning, where the data\nuser would like to build DL models using the data segregated across multiple\ndifferent data owners. However, this could lead to severe privacy concerns due\nto the sensitive nature of the data, thus the data owners would be hesitant and\nreluctant to participate. We propose LDP-DL, a privacy-preserving distributed\ndeep learning framework via local differential privacy and knowledge\ndistillation, where each data owner learns a teacher model using its own\n(local) private dataset, and the data user learns a student model to mimic the\noutput of the ensemble of the teacher models. In the experimental evaluation, a\ncomprehensive comparison has been made among our proposed approach (i.e.,\nLDP-DL), DP-SGD, PATE and DP-FL, using three popular deep learning benchmark\ndatasets (i.e., CIFAR10, MNIST and FashionMNIST). The experimental results show\nthat LDP-DL consistently outperforms the other competitors in terms of privacy\nbudget and model accuracy.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Graph Self-supervised Learning with Accurate Discrepancy Learning"], "authors": ["[arxiv.Result.Author('Dongki Kim'), arxiv.Result.Author('Jinheon Baek'), arxiv.Result.Author('Sung Ju Hwang')]"], "link": ["http://arxiv.org/pdf/2202.02989v1"], "summary": "Self-supervised learning of graph neural networks (GNNs) aims to learn an\naccurate representation of the graphs in an unsupervised manner, to obtain\ntransferable representations of them for diverse downstream tasks. Predictive\nlearning and contrastive learning are the two most prevalent approaches for\ngraph self-supervised learning. However, they have their own drawbacks. While\nthe predictive learning methods can learn the contextual relationships between\nneighboring nodes and edges, they cannot learn global graph-level similarities.\nContrastive learning, while it can learn global graph-level similarities, its\nobjective to maximize the similarity between two differently perturbed graphs\nmay result in representations that cannot discriminate two similar graphs with\ndifferent properties. To tackle such limitations, we propose a framework that\naims to learn the exact discrepancy between the original and the perturbed\ngraphs, coined as Discrepancy-based Self-supervised LeArning (D-SLA).\nSpecifically, we create multiple perturbations of the given graph with varying\ndegrees of similarity and train the model to predict whether each graph is the\noriginal graph or a perturbed one. Moreover, we further aim to accurately\ncapture the amount of discrepancy for each perturbed graph using the graph edit\ndistance. We validate our method on various graph-related downstream tasks,\nincluding molecular property prediction, protein function prediction, and link\nprediction tasks, on which our model largely outperforms relevant baselines.", "entities_include_in_text": ["Fan et al., 2019", "Baek et al., 2020", "Muzio\net al., 2021", "Xie et al., 2021", "Gilmer et al., 2017", "Hamilton et al., 2017", "Velickovic et al., 2018", "Xu et al., 2019", "Velickovic et al., 2019; Sun et al., 2020", "You et al.,\n2020; Zhu et al., 2021; You et al., 2021", "Suresh et al., 2021", "Yang et al., 2021", "Gilmer et al., 2017", "Ying et al.,\n2018; Baek et al., 2021", "You\net al., 2020; Zhu et al., 2021; You et al., 2021", "Zeng et al.,\n2009", "Zeng et al., 2009", "You et al., 2020; 2021", "Hamilton et al., 2017", "Velickovic\net al., 2019", "You et al., 2020", "You et al.,\n2021", "Xu et al., 2019", "Morris et al., 2020", "Velickovic et al., 2019", "You et al., 2020", "You et al., 2021", "You et al., 2021", "Zitnik et al., 2019", "Xu et al., 2019", "Morris et al., 2020", "You et al., 2020; 2021", "Morris et al., 2020"], "entities_from_reference": ["Lee", "Hwang", "Baek", "Kang", "Chen", "Machine Learning", "Fan", "Li", "Zhao", "Tang", "Yin", "Gao", "Kipf", "J.", "Leman", "Weisfeiler", "Morris", "Bause", "Neumann", "Muzio", "Borgwardt", "Briefings Bioinform.", "Hahn", "Gilmer", "Schoenholz", "Riley", "Vinyals", "Dahl", "Rong", "Xie", "Wei", "Huang", "Hamilton", "Leskovec", "J. Inductive", "Girshick", "Momentum", "Pattern Recognition", "Liu", "Gomes", "Liang", "Pande", "Wang", "Chang", "Data Mining", "Kwon", "Kim", "Sanfeliu", "Schroff", "Kalenichenko", "Philbin", "J. Facenet", "Irwin", "J. Zinc", "J. Infograph", "Peng", "Hao", "J. Adversarial", "Graph", "Tung", "Feng", "Zhang", "Xia", "Zhu", "Zitnik", "Sosic", "Feldman", "J. Evolution", "Van", "Maaten", "Velickovic", "Casanova", "Bengio", "Fedus", "Hjelm", "Pappu", "Chemical", "Lin", "Shi", "Zhou", "Yang", "Luo", "Jegelka", "Guo", "Ren", "J. Hierarchical", "Sui", "Experimental Details", "Graph Classification Dataset", "Dataset Tasks Graphs Avg", "Nodes Avg", "Edges Chemical Domain", "Domain PPI", "Edges", "Graph Isomorphism Networks", "Implementation Details", "Molecular Property", "Double", "Triple", "Protein Function", "Link", "Astro Physics", "Dataset Table", "Graphs Avg", "Edges Pert", "Strategy", "Appendix", "Analysis Rank Correlation", "Dataset Analysis", "Contrarily", "Tox21 Figure", "Green", "Lmargin", "Anchor", "Molecules Graph", "Ledit", "Lmargin Figure"]}{"title": ["Soft Actor-Critic with Inhibitory Networks for Faster Retraining"], "authors": ["[arxiv.Result.Author('Jaime S. Ide'), arxiv.Result.Author('Daria Mi\u0107ovi\u0107'), arxiv.Result.Author('Michael J. Guarino'), arxiv.Result.Author('Kevin Alcedo'), arxiv.Result.Author('David Rosenbluth')]"], "link": ["http://arxiv.org/pdf/2202.02918v1"], "summary": "Reusing previously trained models is critical in deep reinforcement learning\nto speed up training of new agents. However, it is unclear how to acquire new\nskills when objectives and constraints are in conflict with previously learned\nskills. Moreover, when retraining, there is an intrinsic conflict between\nexploiting what has already been learned and exploring new skills. In soft\nactor-critic (SAC) methods, a temperature parameter can be dynamically adjusted\nto weight the action entropy and balance the explore $\\times$ exploit\ntrade-off. However, controlling a single coefficient can be challenging within\nthe context of retraining, even more so when goals are contradictory. In this\nwork, inspired by neuroscience research, we propose a novel approach using\ninhibitory networks to allow separate and adaptive state value evaluations, as\nwell as distinct automatic entropy tuning. Ultimately, our approach allows for\ncontrolling inhibition to handle conflict between exploiting less risky,\nacquired behaviors and exploring novel ones to overcome more challenging tasks.\nWe validate our method through experiments in OpenAI Gym environments.", "entities_include_in_text": ["Schaul et al., 2015", "Bacon et al., 2017", "Vezhnevets et al., 2017", "Nachum et al., 2018", "Van Seijen et al., 2017; Sahni et al., 2017; Haarnoja et al., 2017; Hansen et al., 2020;\nBarreto et al., 2020", "Diamond, 2013", "Haarnoja et al., 2017; Van Niekerk et al., 2019", "Botvinick et al., 2019", "Hong et al., 2018", "Puterman, 1994", "Ziebart, 2010", "Thomas, 2014", "Lillicrap et al., 2016", "Haarnoja et al., 2017", "Schul-\nman et al., 2015", "Mnih et al., 2015", "Aron, 2007; Diamond, 2013). In Shenoy et al. (2011", "Logan\net al., 2015", "Shenoy et al., 2011", "Ide et al., 2013", "Haarnoja et al., 2017; Van Niekerk\net al., 2019), the goal is to model a new task by composing value functions previously trained\non sub-tasks. In Todorov (2009", "Todorov, 2007", "Van Seijen et al., 2017", "Mnih et al., 2015", "Brys et al., 2015", "Schaal, 1996", "Ammar et al., 2012", "Aron, 2007", "Fujimoto et al., 2018", "Tulving, 2002", "Blundell et al., 2016; Lin et al., 2018; Botvinick et al., 2019", "Brockman et al., 2016", "Logan et al., 2015", "Braver et al., 2001", "AAMAS 2010", "ICLR\n2016", "Seymour et al., 2007", "Martin, 2012). Negative rewards are often successfully used in RL.\nHowever, there are studies showing that intermixing positive and sparse negative rewards might\nhave adverse effects in learning, as reported in a recent work using DDPG in robotic environ-\nment Vargas et al. (2020", "Hu et al., 2020"], "entities_from_reference": ["Plusieurs"]}{"title": ["Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks"], "authors": ["[arxiv.Result.Author('Seyyedali Hosseinalipour'), arxiv.Result.Author('Su Wang'), arxiv.Result.Author('Nicolo Michelusi'), arxiv.Result.Author('Vaneet Aggarwal'), arxiv.Result.Author('Christopher G. Brinton'), arxiv.Result.Author('David J. Love'), arxiv.Result.Author('Mung Chiang')]"], "link": ["http://arxiv.org/pdf/2202.02947v1"], "summary": "Federated learning (FedL) has emerged as a popular technique for distributing\nmodel training over a set of wireless devices, via iterative local updates (at\ndevices) and global aggregations (at the server). In this paper, we develop\n\\textit{parallel successive learning} (PSL), which expands the FedL\narchitecture along three dimensions: (i) Network, allowing decentralized\ncooperation among the devices via device-to-device (D2D) communications. (ii)\nHeterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers\nheterogeneous number of stochastic gradient descent iterations with different\nmini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic\nenvironment with data arrival and departure, where the distributions of local\ndatasets evolve over time, captured via a new metric for model/concept drift.\n(ii-c) Device: PSL considers devices with different computation and\ncommunication capabilities. (iii) Proximity, where devices have different\ndistances to each other and the access point. PSL considers the realistic\nscenario where global aggregations are conducted with idle times in-between\nthem for resource efficiency improvements, and incorporates data dispersion and\nmodel dispersion with local model condensation into FedL. Our analysis sheds\nlight on the notion of cold vs. warmed up models, and model inertia in\ndistributed machine learning. We then propose network-aware dynamic model\ntracking to optimize the model learning vs. resource efficiency tradeoff, which\nwe show is an NP-hard signomial programming problem. We finally solve this\nproblem through proposing a general optimization solver. Our numerical results\nreveal new findings on the interdependencies between the idle times in-between\nthe global aggregations, model/concept drift, and D2D cooperation\nconfiguration.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Comprehensive survey of computational learning methods for analysis of gene expression data in genomics"], "authors": ["[arxiv.Result.Author('Nikita Bhandari'), arxiv.Result.Author('Rahee Walambe'), arxiv.Result.Author('Ketan Kotech'), arxiv.Result.Author('Satyajeet Khare')]"], "link": ["http://arxiv.org/pdf/2202.02958v1"], "summary": "Computational analysis methods including machine learning have a significant\nimpact in the fields of genomics and medicine. High-throughput gene expression\nanalysis methods such as microarray technology and RNA sequencing produce\nenormous amounts of data. Traditionally, statistical methods are used for\ncomparative analysis of the gene expression data. However, more complex\nanalysis for classification and discovery of feature genes or sample\nobservations requires sophisticated computational approaches. In this review,\nwe compile various statistical and computational tools used in analysis of\nexpression microarray data. Even though, the methods are discussed in the\ncontext of expression microarray data, they can also be applied for the\nanalysis of RNA sequencing or quantitative proteomics datasets. We specifically\ndiscuss methods for missing value (gene expression) imputation, feature gene\nscaling, selection and extraction of features for dimensionality reduction, and\nlearning and analysis of expression data. We discuss the types of missing\nvalues and the methods and approaches usually employed in their imputation. We\nalso discuss methods of data transformation and feature scaling viz.\nnormalization and standardization. Various approaches used in feature selection\nand extraction are also reviewed. Lastly, learning and analysis methods\nincluding class comparison, class prediction, and class discovery along with\ntheir evaluation parameters are described in detail. We have described the\nprocess of generation of a microarray gene expression data along with\nadvantages and limitations of the above-mentioned techniques. We believe that\nthis detailed review will help the users to select appropriate methods based on\nthe type of data and the expected outcome.", "entities_include_in_text": ["Segundo-Val  and  Sanz-Lozano \n2016", "Smyth  et  al.  2005", "Newton \n2001; Rubinstein et al. 2003; Svensson 2016", "Bilban  et  al.  2002", "Freyhult  et  al.  2010", "Cheng et al. 2016", "Agrahari  et  al.  2018", "Collobert and Weston 2008; Neubauer 1998"], "entities_from_reference": ["Plusieurs"]}{"title": ["Redactor: Targeted Disinformation Generation using Probabilistic Decision Boundaries"], "authors": ["[arxiv.Result.Author('Geon Heo'), arxiv.Result.Author('Steven Euijong Whang')]"], "link": ["http://arxiv.org/pdf/2202.02902v1"], "summary": "Information leakage is becoming a critical problem as various information\nbecomes publicly available by mistake, and machine learning models train on\nthat data to provide services. As a result, one's private information could\neasily be memorized by such trained models. Unfortunately, deleting information\nis out of the question as the data is already exposed to the Web or third-party\nplatforms. Moreover, we cannot necessarily control the labeling process and the\nmodel trainings by other parties either. In this setting, we study the problem\nof targeted disinformation where the goal is to lower the accuracy of inference\nattacks on a specific target (e.g., a person's profile) only using data\ninsertion. While our problem is related to data privacy and defenses against\nexploratory attacks, our techniques are inspired by targeted data poisoning\nattacks with some key differences. We show that our problem is best solved by\nfinding the closest points to the target in the input space that will be\nlabeled as a different class. Since we do not control the labeling process, we\ninstead conservatively estimate the labels probabilistically by combining\ndecision boundaries of multiple classifiers using data programming techniques.\nWe also propose techniques for making the disinformation realistic. Our\nexperiments show that a probabilistic decision boundary can be a good proxy for\nlabelers, and that our approach outperforms other targeted poisoning methods\nwhen using end-to-end training on real datasets.", "entities_include_in_text": [], "entities_from_reference": ["How Photos", "Accessed", "Identify Risky Clients", "Jeff Larson", "Surya Mattu", "Lauren Kirchner", "Machine", "Xavier Renard", "Jonathan Aigrain", "Thibault Laugel", "Pascal Frossard", "Marcin Detyniecki", "Tabular Data", "Lise Getoor", "Louis Licamele", "Vadim Borisov", "Tobias Leemann", "Kathrin Seler", "Johannes Haug", "Martin Pawelczyk", "Gjergji Kasneci", "Lucas Bourtoule", "Varun Chandrasekaran", "Christopher", "Hengrui Jia", "Adelin Travers", "Baiwu Zhang", "David Lie", "Nicolas Papernot", "Machine Unlearning", "Haipeng Chen", "Sushil Jajodia", "Jing Liu", "Noseong Park", "Vadim Sokolov", "Bounded Real Data", "Xinyun Chen", "Chang Liu", "Bo Li", "Kimberly Lu", "Dawn Song", "Backdoor Attacks", "Deep Learning Systems Using Data Poisoning", "Edward Choi", "Siddharth Biswal", "Bradley A. Malin", "Jon Duke", "Walter F. Stewart", "Jimeng Sun", "Vol", "Florian Tramer", "Nicholas Carlini", "Peter Christen", "Data Matching", "Record Linkage", "Entity Resolution", "Springer", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam D. Smith", "Noise", "Aaron Roth", "Found", "Trends Theor", "Ahmed K. Elmagarmid", "Panagiotis G. Ipeirotis", "Vassilios S. Verykios", "Duplicate Record", "Knowl", "Data Eng", "Fredrikson", "Somesh Jha", "Thomas Ristenpart", "Model", "Matthew Fredrikson", "Eric Lantz", "Simon Lin", "David Page", "Antonio Ginart", "Melody Y. Guan", "Gregory Valiant", "James Zou", "Aditya Golatkar", "Alessandro Achille", "Stefano Soatto", "Deep Networks", "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David WardeFarley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio", "Laura Graves", "Vineel Nagisetty", "Vijay Ganesh", "Amnesiac Machine Learning", "Chuan Guo", "Tom Goldstein", "Awni Y. Hannun", "Maaten", "Data Removal", "Machine Learning Models", "Jamie Hayes", "Luca Melis", "George Danezis", "Emiliano De Cristofaro", "Dorjan Hitaj", "Luigi V Mancini", "Mary Anne Smart", "Kamalika Chaudhuri", "James Y. Zou", "Jinyuan Jia", "Ahmed Salem", "Michael Backes", "Yang Zhang", "Neil Zhenqiang Gong", "Mehran Mozaffari Kermani", "Susmita", "Anand Raghunathan", "Niraj K. Jha", "Machine Learning", "Health Informatics", "Ron Kohavi", "Jiacheng Li", "Ninghui Li", "Bruno Ribeiro", "Membership Inference Attacks", "Jialin Pan", "Qiang Yang", "Panagiotis Papadimitriou", "Hector Garcia-Molina", "Data Leakage Detec", "Park", "Mahmoud Mohammadi", "Kshitij Gorde", "Hongkyu Park", "Youngmin Kim", "Data Synthesis", "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer", "Fabian Pedregosa", "Gael Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg", "Erwin Quiring", "Daniel Arp", "Konrad Rieck", "Alexander Ratner", "Stephen H. Bach", "Henry R. Ehrenberg", "Jason Alan Fries", "Christopher Re", "Snorkel", "Rapid Training Data Creation", "Weak Supervision", "Jason", "Alexander J. Ratner", "Set Generation", "Mathias Humbert", "Pascal Berrang", "Mario Fritz", "Data Independent Membership Inference Attacks", "Defenses", "Sebastian Schelter", "Can Forget User Data Very Fast", "Ali Shafahi", "Ronny Huang", "Mahyar Najibi", "Octavian Suciu", "Christoph Studer", "Tudor Dumitras", "Poison Frogs", "Marco Stronati", "Congzheng Song", "Vitaly Shmatikov", "Akash Srivastava", "Lazar Valkov", "Chris Russell", "Michael U. Gutmann", "Charles Sutton", "Mode Collapse", "Implicit Variational Learning", "Beata Strack", "Jonathan P. DeShazo", "Chris Gennings", "Juan L. Olmo", "Sebastian Ventura", "Krzysztof J. Cios", "John N. Clore", "Clinical Database Patient Records", "Radu Marginean", "Yigitcan Kaya", "Hal Daume III", "Does Machine Learning", "Steven Euijong Whang", "Yinjun Wu", "Edgar Dobriban", "Susan B. Davidson", "Maria Skoularidou", "Alfredo Cuesta-Infante", "Kalyan Veeramachaneni", "Tabular", "Chen Zhu", "Hengduo Li", "Gavin Taylor", "Deep Neural Nets"]}{"title": ["Trusted Approximate Policy Iteration with Bisimulation Metrics"], "authors": ["[arxiv.Result.Author('Mete Kemertas'), arxiv.Result.Author('Allan Jepson')]"], "link": ["http://arxiv.org/pdf/2202.02881v1"], "summary": "Bisimulation metrics define a distance measure between states of a Markov\ndecision process (MDP) based on a comparison of reward sequences. Due to this\nproperty they provide theoretical guarantees in value function approximation.\nIn this work we first prove that bisimulation metrics can be defined via any\n$p$-Wasserstein metric for $p\\geq 1$. Then we describe an approximate policy\niteration (API) procedure that uses $\\epsilon$-aggregation with\n$\\pi$-bisimulation and prove performance bounds for continuous state spaces. We\nbound the difference between $\\pi$-bisimulation metrics in terms of the change\nin the policies themselves. Based on these theoretical results, we design an\nAPI($\\alpha$) procedure that employs conservative policy updates and enjoys\nbetter performance bounds than the naive API approach. In addition, we propose\na novel trust region approach which circumvents the requirement to explicitly\nsolve a constrained optimization problem. Finally, we provide experimental\nevidence of improved stability compared to non-conservative alternatives in\nsimulated continuous control.", "entities_include_in_text": ["Li et al., 2006", "Lan et al.,\n2021", "Singh et al., 1995; Bertsekas, 2019", "Ferns et al., 2004; 2011", "Castro, 2020", "Zhang et al., 2021).\nZhang et al. (2021", "Zhang et al., 2021", "Ferns et al., 2011)", "Ferns et al., 2011). A dis-\ncrete analogue of this metric was also outlined previously\nby Ferns et al. (2004), which is omitted here for brevity.\nFerns et al. (2011", "Ferns et al., 2004; 2011)", "Ferns et al.,\n2011", "Ferns et al., 2011", "Castro, 2020)", "Haarnoja et al., 2018", "see Bertsekas (2011)", "Schulman\net al., 2015", "Zhang et al., 2021", "Zhang et al.,\n2021", "Munos, 2003; Farahmand et al., 2010). Indeed, as\nnoted by Bertsekas (2011", "Schul-\nman et al., 2015", "Wu et al., 2017", "Schul-\nman et al., 2017)", "Castro,\n2020; Zhang et al., 2021", "Haarnoja et al., 2018", "Ziebart et al.,\n2008", "Nachum et al., 2017;\nHaarnoja et al., 2018", "Nachum et al., 2017", "Haarnoja et al., 2018", "Nachum et al., 2018", "Nachum et al., 2018", "Pinsker, 1964; Kullback, 1967", "see Duchi (2007", "Haarnoja et al., 2018", "Zhang et al., 2021", "Villani, 2008)", "Villani, 2008", "Ferns et al., 2011", "Villani, 2008", "Bertsekas, 2018)", "Bertsekas, 2018)", "Bertsekas, 2018"], "entities_from_reference": ["Bertsekas", "Athena Bertsekas", "Sinica", "Tsitsiklis", "J. N.", "Athena Scientific", "Brockman", "Pettersson", "Schneider", "Schulman", "Tang", "Castro", "Markov", "Duchi", "J. Derivations", "Berkeley", "Red Hook", "Ferns", "Panangaden", "Arlington", "Haarnoja", "Zhou", "Machine Learning", "Kakade", "San Francisco", "Morgan Kaufmann Publishers", "Kemertas", "Kullback", "Lan", "Walsh", "Littman", "Munos", "Nachum", "Bengio", "Wallach", "Garnett", "Gibbs", "Statistical Review", "Pinsker", "Qiao", "Minematsu", "Bisimulation Metrics Ravindran", "Barto", "Scherrer", "Blei", "Machine Learning Research", "Wolski", "Singh", "Touretzky", "Leen", "Villani", "Springer Science", "Liao", "J. Scalable", "Systems", "Zhang", "Gal", "Ziebart", "Maas", "Dey", "Bisimulation Metrics", "Trusted Approximate Policy Iteration", "Definition", "Follows", "Lemma", "Fubinis", "Bisimulation Metrics Lemma", "Corollary", "Thm", "Em", "Rj", "Otherwise", "V V", "Lemma A.8", "Lemma A.10", "V B", "Tg V", "T V", "T V Tg V", "V TV", "Lemma A.11", "Corollary A.4", "Ek", "D TV", "Ek1 Plugging"]}{"title": ["Evaluation Methods and Measures for Causal Learning Algorithms"], "authors": ["[arxiv.Result.Author('Lu Cheng'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Raha Moraffah'), arxiv.Result.Author('Paras Sheth'), arxiv.Result.Author('K. Selcuk Candan'), arxiv.Result.Author('Huan Liu')]"], "link": ["http://arxiv.org/pdf/2202.02896v1"], "summary": "The convenient access to copious multi-faceted data has encouraged machine\nlearning researchers to reconsider correlation-based learning and embrace the\nopportunity of causality-based learning, i.e., causal machine learning (causal\nlearning). Recent years have therefore witnessed great effort in developing\ncausal learning algorithms aiming to help AI achieve human-level intelligence.\nDue to the lack-of ground-truth data, one of the biggest challenges in current\ncausal learning research is algorithm evaluations. This largely impedes the\ncross-pollination of AI and causal inference, and hinders the two fields to\nbenefit from the advances of the other. To bridge from conventional causal\ninference (i.e., based on statistical methods) to causal learning with big data\n(i.e., the intersection of causal inference and machine learning), in this\nsurvey, we review commonly-used datasets, evaluation methods, and measures for\ncausal learning using an evaluation pipeline similar to conventional machine\nlearning. We focus on the two fundamental causal-inference tasks and\ncausality-aware machine learning tasks. Limitations of current evaluation\nprocedures are also discussed. We then examine popular causal inference\ntools/packages and conclude with primary challenges and opportunities for\nbenchmarking causal learning algorithms in the era of big data. The survey\nseeks to bring to the forefront the urgency of developing publicly available\nbenchmarks and consensus-building standards for causal learning evaluation with\nobservational data. In doing so, we hope to broaden the discussions and\nfacilitate collaboration to advance the innovation and application of causal\nlearning.", "entities_include_in_text": ["NOAA 2008"], "entities_from_reference": ["Plusieurs"]}{"title": ["Applications of Machine Learning in Healthcare and Internet of Things (IOT): A Comprehensive Review"], "authors": ["[arxiv.Result.Author('Farid Ghareh Mohammadi'), arxiv.Result.Author('Farzan Shenavarmasouleh'), arxiv.Result.Author('Hamid R. Arabnia')]"], "link": ["http://arxiv.org/pdf/2202.02868v1"], "summary": "In recent years, smart healthcare IoT devices have become ubiquitous, but\nthey work in isolated networks due to their policy. Having these devices\nconnected in a network enables us to perform medical distributed data analysis.\nHowever, the presence of diverse IoT devices in terms of technology, structure,\nand network policy, makes it a challenging issue while applying traditional\ncentralized learning algorithms on decentralized data collected from the IoT\ndevices. In this study, we present an extensive review of the state-of-the-art\nmachine learning applications particularly in healthcare, challenging issues in\nIoT, and corresponding promising solutions. Finally, we highlight some\nopen-ended issues of IoT in healthcare that leaves further research studies and\ninvestigation for scientists.", "entities_include_in_text": ["Ullah et al., 2021", "Szegedy et al., 2015", "Cho et al., 2014", "DiFrancesco, 1993", "Schmerber et\n\nal., 2017", "Csoeregi et al., 1994", "Iandola et al., 2016", "J. Wang et al., 2018", "F. Wang et al., 2017", "Miller, 1998", "Gondalia et al., 2018", "Gondalia et al., 2018", "Yao et al.,\n\n2019", "Basingab, 2020", "Basingab, 2020; Mambou et al., 2016", "Shahrestani, 2017", "Gupta et al., 2018", "Jourdan et al., 2020", "Jourdan et al., 2020", "Jourdan et al., 2020", "Mitchell et al., 2018", "Fontenla-Romero et al., 2013", "Hassan et al., 2020", "Mohammadi et al., 2020", "Lim et al., 2020", "Xu et al., 2021", "Lim et al., 2020", "Lim et al., 2020", "Truex et al.,\n\n2019", "Wu et al., 2020", "Chen et al., 2020", "Truex et al., 2019", "Truex et al., 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors"], "authors": ["[arxiv.Result.Author('Christina G\u00f6pfert'), arxiv.Result.Author('Yinlam Chow'), arxiv.Result.Author('Chih-wei Hsu'), arxiv.Result.Author('Ivan Vendrov'), arxiv.Result.Author('Tyler Lu'), arxiv.Result.Author('Deepak Ramachandran'), arxiv.Result.Author('Craig Boutilier')]"], "link": ["http://arxiv.org/pdf/2202.02830v1"], "summary": "Interactive recommender systems (RSs) allow users to express intent,\npreferences and contexts in a rich fashion, often using natural language. One\nchallenge in using such feedback is inferring a user's semantic intent from the\nopen-ended terms used to describe an item, and using it to refine\nrecommendation results. Leveraging concept activation vectors (CAVs) [21], we\ndevelop a framework to learn a representation that captures the semantics of\nsuch attributes and connects them to user preferences and behaviors in RSs. A\nnovel feature of our approach is its ability to distinguish objective and\nsubjective attributes and associate different senses with different users.\nUsing synthetic and real-world datasets, we show that our CAV representation\naccurately interprets users' subjective semantics, and can improve\nrecommendations via interactive critiquing", "entities_include_in_text": ["ICDM 2008"], "entities_from_reference": ["Plusieurs"]}{"title": ["Block shuffling learning for Deepfake Detection"], "authors": ["[arxiv.Result.Author('Sitong Liu'), arxiv.Result.Author('Zhichao Lian'), arxiv.Result.Author('Siqi Gu'), arxiv.Result.Author('Liang Xiao')]"], "link": ["http://arxiv.org/pdf/2202.02819v1"], "summary": "Although the deepfake detection based on convolutional neural network has\nachieved good results, the detection results show that these detectors show\nobvious performance degradation when the input images undergo some common\ntransformations (like resizing, blurring), which indicates that the\ngeneralization ability of the detector is insufficient. In this paper, we\npropose a novel block shuffling learning method to solve this problem.\nSpecifically, we divide the images into blocks and then introduce the random\nshuffling to intra-block and inter-block. Intra-block shuffling increases the\nrobustness of the detector and we also propose an adversarial loss algorithm to\novercome the over-fitting problem brought by the noise introduced by shuffling.\nMoreover, we encourage the detector to focus on finding differences among the\nlocal features through inter-block shuffling, and reconstruct the spatial\nlayout of the blocks to model the semantic associations between them.\nEspecially, our method can be easily integrated with various CNN models.\nExtensive experiments show that our proposed method achieves state-of-the-art\nperformance in forgery face detection, including good generalization ability in\nthe face of common image transformations.", "entities_include_in_text": [], "entities_from_reference": ["Darius Afchar", "Vincent Nozick", "Junichi Yamagishi", "Isao Echizen", "Maungmaung Aprilpyone", "Hitoshi Kiya", "Encryption", "Image Processing", "Valentin Bazarevsky", "Yury Kartynnik", "Andrey Vakunov", "Karthik Raveendran", "Matthias Grundmann", "Blazeface", "Third Workshop", "Scott McCloskey", "Jingyi Yu", "Focus", "Pattern Recognition", "Salt Lake City", "Yalong Bai", "Wei Zhang", "Tao Mei", "Giovanni Chierchia", "Sara Parrilli", "Giovanni Poggi", "Luisa Verdoliva", "Carlo Sansone", "Chollet", "Tatsuya Chuman", "Warit Sirichotedumrong", "Deepfakes", "Mengnan Du", "Shiva K. Pentyala", "Li", "Xia Hu", "Towards", "Stefan Dietze", "Claudia Hauff", "Edward Curry", "Philippe Cudr", "Knowledge Management", "Virtual Event", "Ricard Durall", "Margret Keuper", "Janis Keuper", "Faceswap", "Jessica J. Fridrich", "Jan Kodovsk", "Rich", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun", "Deep", "Las Vegas", "Shuiwang Li", "Yuchao Yang", "Feiyu Zhu", "Qijun Zhao", "Li Lu", "Remote Sensing Letters", "Hyeonseong Jeon", "Youngoh Bang", "Simon S. Woo", "Jimmy Ba", "Adam", "San Diego", "Lingzhi Li", "Jianmin Bao", "Ting Zhang", "Hao Yang", "Dong Chen", "Fang Wen", "Face", "Yuezun Li", "Siwei Lyu", "Pattern Recognition Workshops", "Xin Yang", "Pu Sun", "Honggang Qi", "Honggu Liu", "Xiaodan Li", "Wenbo Zhou", "Yuefeng Chen", "Hui Xue", "Weiming Zhang", "Nenghai Yu", "Jan Luk", "Miroslav Goljan", "Electronic Imaging", "Yucheng Luo", "Yong Zhang", "Junchi Yan", "Wei Liu", "Huy Hoang Nguyen", "Fuming Fang", "Huy H. Nguyen", "Yuyang Qian", "Guojun Yin", "Lu Sheng", "Zixuan Chen", "Andreas R", "Davide Cozzolino", "Christian Riess", "Justus Thies", "Matthias Niener", "Sara Sabour", "Nicholas Frosst", "Geoffrey E. Hinton", "Dynamic", "Ulrike", "Samy Bengio", "Hanna M. Wallach", "Rob Fergus", "Roman Garnett", "Yuma Kinoshita", "Michael Zollh", "Marc Stamminger", "Christian Theobalt", "Chengrui Wang", "Weihong Deng", "Junke Wang", "Zuxuan Wu", "Jingjing Chen", "Yugang Jiang", "Zhao", "Dongdong Chen", "Tianyi Wei", "Zhun Zhong", "Liang Zheng", "Guoliang Kang", "Shaozi Li", "Yi Yang", "Random", "Apr", "Peng Zhou", "Xintong Han", "Vlad I. Morariu", "Larry S. Davis"]}{"title": ["Energy-Aware Edge Association for Cluster-based Personalized Federated Learning"], "authors": ["[arxiv.Result.Author('Y. Li'), arxiv.Result.Author('X. Qin'), arxiv.Result.Author('H. Chen'), arxiv.Result.Author('K. Han'), arxiv.Result.Author('P. Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02727v1"], "summary": "Federated Learning (FL) over wireless network enables data-conscious services\nby leveraging the ubiquitous intelligence at network edge for\nprivacy-preserving model training. As the proliferation of context-aware\nservices, the diversified personal preferences causes disagreeing conditional\ndistributions among user data, which leads to poor inference performance. In\nthis sense, clustered federated learning is proposed to group user devices with\nsimilar preference and provide each cluster with a personalized model. This\ncalls for innovative design in edge association that involves user clustering\nand also resource management optimization. We formulate an accuracy-cost\ntrade-off optimization problem by jointly considering model accuracy,\ncommunication resource allocation and energy consumption. To comply with\nparameter encryption techniques in FL, we propose an iterative solution\nprocedure which employs deep reinforcement learning based approach at cloud\nserver for edge association. The reward function consists of minimized energy\nconsumption at each base station and the averaged model accuracy of all users.\nUnder our proposed solution, multiple edge base station are fully exploited to\nrealize cost efficient personalized federated learning without any prior\nknowledge on model parameters. Simulation results show that our proposed\nstrategy outperforms existing strategies in achieving accurate learning at low\nenergy cost.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Deep Learning-Aided Spatial Multiplexing with Index Modulation"], "authors": ["[arxiv.Result.Author('Merve Turhan'), arxiv.Result.Author('Ersin Ozturk'), arxiv.Result.Author('Hakan Ali Cirpan')]"], "link": ["http://arxiv.org/pdf/2202.02856v1"], "summary": "In this paper, deep learning (DL)-aided data detection of spatial\nmultiplexing (SMX) multiple-input multiple-output (MIMO) transmission with\nindex modulation (IM) (Deep-SMX-IM) has been proposed. Deep-SMX-IM has been\nconstructed by combining a zero-forcing (ZF) detector and DL technique. The\nproposed method uses the significant advantages of DL techniques to learn\ntransmission characteristics of the frequency and spatial domains. Furthermore,\nthanks to using subblockbased detection provided by IM, Deep-SMX-IM is a\nstraightforward method, which eventually reveals reduced complexity. It has\nbeen shown that Deep-SMX-IM has significant error performance gains compared to\nZF detector without increasing computational complexity for different system\nconfigurations.", "entities_include_in_text": ["Jun 2017", "Dec 2017", "Nov 2013", "Jun 2017)\n\n(2018", "Feb\n2020", "Aug 2019", "Sep 2014", "Dec 2016", "Jun 2016", "Oct 2017", "Jun 2017", "Oct 2018", "Dec 2017", "Feb 2014"], "entities_from_reference": ["Basar", "Ayg", "Panayrc", "Poor", "Corlay", "Multilevel MIMO", "Adam", "Goodfellow", "Bengio", "Huang", "Guo", "Gui", "Yang", "Zhang", "Sari", "Adachi", "Luong", "Vien", "Matthaiou", "Deep", "Michailow", "Ozturk", "Black Sea Conf", "Varna", "Istanbul", "Samuel", "Turhan", "Mobile Radio Communications", "Index", "Balkan Conf", "Skopje"]}{"title": ["SFMGNet: A Physics-based Neural Network To Predict Pedestrian Trajectories"], "authors": ["[arxiv.Result.Author('Sakif Hossain'), arxiv.Result.Author('Fatema T. Johora'), arxiv.Result.Author('J\u00f6rg P. M\u00fcller'), arxiv.Result.Author('Sven Hartmann'), arxiv.Result.Author('Andreas Reinhardt')]"], "link": ["http://arxiv.org/pdf/2202.02791v1"], "summary": "Autonomous robots and vehicles are expected to soon become an integral part\nof our environment. Unsatisfactory issues regarding interaction with existing\nroad users, performance in mixed-traffic areas and lack of interpretable\nbehavior remain key obstacles. To address these, we present a physics-based\nneural network, based on a hybrid approach combining a social force model\nextended by group force (SFMG) with Multi-Layer Perceptron (MLP) to predict\npedestrian trajectories considering its interaction with static obstacles,\nother pedestrians and pedestrian groups. We quantitatively and qualitatively\nevaluate the model with respect to realistic prediction, prediction performance\nand prediction \"interpretability\". Initial results suggest, the model even when\nsolely trained on a synthetic dataset, can predict realistic and interpretable\ntrajectories with better than state-of-the-art accuracy.", "entities_include_in_text": ["MABS 2020"], "entities_from_reference": ["Johora", "J. P. Muller", "Springer", "Clausthal University", "Yang", "Vehicles Symposium", "Technical Report", "Modelling", "Trajectory", "Scholler", "Wang", "Jiang", "Alahi", "Gupta", "J. Johnson", "Pattern Recognition", "Vemula", "J. Oh", "Huang", "Stgat", "Toward", "Ohio St.", "Kroll", "Kim", "Karpatne", "Atluri", "J. H. Faghmous", "Steinbach", "Antonucci", "Papini", "Deep", "Towards", "Science China", "Zhao", "John Wiley", "Sons", "Design", "Artech House", "Schindler", "Van Gool", "Youll", "Lerner", "Wiley Online Library", "Zhang", "J. J. Baldelomar", "Hayet", "J. Pettre", "Opentraj", "Sadeghian", "Sophie"]}{"title": ["(Almost) Envy-Free, Proportional and Efficient Allocations of an Indivisible Mixed Manna"], "authors": ["[arxiv.Result.Author('Vasilis Livanos'), arxiv.Result.Author('Ruta Mehta'), arxiv.Result.Author('Aniket Murhekar')]"], "link": ["http://arxiv.org/pdf/2202.02672v1"], "summary": "We study the problem of finding fair and efficient allocations of a set of\nindivisible items to a set of agents, where each item may be a good (positively\nvalued) for some agents and a bad (negatively valued) for others, i.e., a mixed\nmanna. As fairness notions, we consider arguably the strongest possible\nrelaxations of envy-freeness and proportionality, namely envy-free up to any\nitem (EFX and EFX$_0$), and proportional up to the maximin good or any bad\n(PropMX and PropMX$_0$). Our efficiency notion is Pareto-optimality (PO).\n  We study two types of instances:\n  (i) Separable, where the item set can be partitioned into goods and bads, and\n(ii) Restricted mixed goods (RMG), where for each item $j$, every agent has\neither a non-positive value for $j$, or values $j$ at the same $v_j>0$. We\nobtain polynomial-time algorithms for the following:\n  (i) Separable instances: PropMX$_0$ allocation.\n  (ii) RMG instances: Let pure bads be the set of items that everyone values\nnegatively.\n  - PropMX allocation for general pure bads.\n  - EFX+PropMX allocation for identically-ordered pure bads.\n  - EFX+PropMX+PO allocation for identical pure bads.\n  Finally, if the RMG instances are further restricted to binary mixed goods\nwhere all the $v_j$'s are the same, we strengthen the results to guarantee\nEFX$_0$ and PropMX$_0$ respectively.", "entities_include_in_text": [], "entities_from_reference": ["Martin Aleksandrov", "Toby Walsh", "Georgios Amanatidis", "Georgios Birmpas", "Aris Filos-Ratsikas", "Alexandros Hollender", "Alexandros A. Voudouris", "Maximum Nash", "Haris Aziz", "Ioannis Caragiannis", "Ayumi Igarashi", "Fair", "Herv", "Moulin", "Fedor Sandomirskiy", "Artem Baklanov", "Pranav Garimidi", "Vasilis Gkatzelis", "Daniel Schoepflin", "Nikhil Bansal", "Maxim Sviridenko", "Machinery", "Sanath Kumar Krishnamurthy", "Econ", "Rohit Vaish", "Barman", "Umang Bhaskar", "Anna Bogomolnaia", "Elena Yanovskaya", "Sylvain Bouveret", "Michel Lematre", "David Kurokawa", "Ariel D. Procaccia", "Nisarg Shah", "Bhaskar Ray Chaudhury", "Jugal Garg", "Kurt Mehlhorn", "Andreas Darmann", "Joachim Schauer", "Nash", "Foley", "Yale Economic Essays", "Peter McGlaughlin", "Aniket Murhekar", "Algorithmic Game Theory", "John Qin", "Daniel Halpern", "Alexandros Psomas", "Nikolai Gravin", "Martin Hoefer", "Ruta Mehta", "Web", "Cham", "Springer International Publishing", "Huang", "Pinyan Lu", "Rucha Kulkarni", "Setareh Taki", "Yingkai Li", "Xiaowei Wu", "Almost", "Lipton", "Annual Review", "Benjamin Plaut", "Tim Roughgarden", "Annual", "Econometrica", "Equity", "Goods", "Sandomirskiy"]}{"title": ["Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal"], "authors": ["[arxiv.Result.Author('David Leslie'), arxiv.Result.Author('Christopher Burr'), arxiv.Result.Author('Mhairi Aitken'), arxiv.Result.Author('Michael Katell'), arxiv.Result.Author('Morgan Briggs'), arxiv.Result.Author('Cami Rincon')]"], "link": ["http://arxiv.org/pdf/2202.02776v1"], "summary": "Following on from the publication of its Feasibility Study in December 2020,\nthe Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and\nits subgroups initiated efforts to formulate and draft its Possible Elements of\na Legal Framework on Artificial Intelligence, based on the Council of Europe's\nstandards on human rights, democracy, and the rule of law. This document was\nultimately adopted by the CAHAI plenary in December 2021. To support this\neffort, The Alan Turing Institute undertook a programme of research that\nexplored the governance processes and practical tools needed to operationalise\nthe integration of human right due diligence with the assurance of trustworthy\nAI innovation practices.\n  The resulting framework was completed and submitted to the Council of Europe\nin September 2021. It presents an end-to-end approach to the assurance of AI\nproject lifecycles that integrates context-based risk analysis and appropriate\nstakeholder engagement with comprehensive impact assessment, and transparent\nrisk management, impact mitigation, and innovation assurance practices. Taken\ntogether, these interlocking processes constitute a Human Rights, Democracy and\nthe Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the\nprocedural requirements for principles-based human rights due diligence with\nthe governance mechanisms needed to set up technical and socio-technical\nguardrails for responsible and trustworthy AI innovation practices. Its purpose\nis to provide an accessible and user-friendly set of mechanisms for\nfacilitating compliance with a binding legal framework on artificial\nintelligence, based on the Council of Europe's standards on human rights,\ndemocracy, and the rule of law, and to ensure that AI innovation projects are\ncarried out with appropriate levels of public accountability, transparency, and\ndemocratic governance.", "entities_include_in_text": ["Levine 2012", "Duijm 2015", "Rausand and Haugen 2020", "quantified in loss function \n\n                                       \nS. (2012", "Aven \n2017", "Krisper  2021", "Cox 2008", "Smith et al 2009", "Levine 2012", "Duijm \n2015", "Baybutt  2017", "Krisper  2021", "Esteves, et al 2017", "UNGP,  2011", "UNHROHC \n2020", "Ashmore,  Calinescu,  and  Paterson  2019", "Sweenor et al. 2020", "ICO  and  Institute  2020", "see  Burton  et  al. \n2020", "Mitchell  et  al.  2019; \nAshmore,  Calinescu,  and  Paterson  2019", "see Burton et al. 2020"], "entities_from_reference": ["Plusieurs"]}{"title": ["Pipe Overflow: Smashing Voice Authentication for Fun and Profit"], "authors": ["[arxiv.Result.Author('Shimaa Ahmed'), arxiv.Result.Author('Yash Wani'), arxiv.Result.Author('Ali Shahin Shamsabadi'), arxiv.Result.Author('Mohammad Yaghini'), arxiv.Result.Author('Ilia Shumailov'), arxiv.Result.Author('Nicolas Papernot'), arxiv.Result.Author('Kassem Fawaz')]"], "link": ["http://arxiv.org/pdf/2202.02751v1"], "summary": "Recent years have seen a surge of popularity of acoustics-enabled personal\ndevices powered by machine learning. Yet, machine learning has proven to be\nvulnerable to adversarial examples. Large number of modern systems protect\nthemselves against such attacks by targeting the artificiality, i.e., they\ndeploy mechanisms to detect the lack of human involvement in generating the\nadversarial examples. However, these defenses implicitly assume that humans are\nincapable of producing meaningful and targeted adversarial examples. In this\npaper, we show that this base assumption is wrong. In particular, we\ndemonstrate that for tasks like speaker identification, a human is capable of\nproducing analog adversarial examples directly with little cost and\nsupervision: by simply speaking through a tube, an adversary reliably\nimpersonates other speakers in eyes of ML models for speaker identification.\nOur findings extend to a range of other acoustic-biometric tasks such as\nliveness, bringing into question their use in security-critical settings in\nreal life, such as phone banking.", "entities_include_in_text": [], "entities_from_reference": ["Source", "Anatomy", "Shimaa Ahmed", "Ilia Shumailov", "Nicolas Papernot", "Kassem Fawaz", "Abdulaziz M Aljalal", "Sound", "European Journal", "Andrew Allen", "Nikunj Raghuvanshi", "Edinburgh", "Dublin Philosophical Magazine", "Pratyusha Kalluri", "Dallas Card", "William Agnew", "Ravit Dotan", "Michelle Bao", "Hadi Abdullah", "Luis Vargas", "Patrick Traynor", "Hello", "Mobile Networks", "Machinery", "Nicholas Boucher", "Ross Anderson", "Stephen Boyd", "Lieven Vandenberghe", "Convex Optimization", "Cambridge University Press", "Timnit Gebru", "Christo Wilson", "Machine Learning Research", "Kui Ren", "Sixu Piao", "Qian Wang", "Jian Weng", "Lu Su", "Aziz Mohaisen", "Hongwei Luo", "Yijie Shen", "Feng Lin", "Guoai Xu", "Xinyun Chen", "Chang Liu", "Bo Li", "Kimberly Lu", "Dawn Song", "Chung", "Voxceleb2", "John R. Deller", "John H.", "John G. Proakis", "Speech Production", "Jenthe Thienpondt", "Kris Demuynck", "Shelley", "Benoit Fabre", "Gilbert", "Avraham Hirschberg", "Xavier Pelorson", "Annual", "Morgan Klaus Scheuerman", "Stacy M. Branham", "Gender Recognition", "Reductionism", "Tian Tan", "Andre Kassis", "Urs Hengartner", "George Kersta", "Jong Wook Kim", "Justin Salamon", "Peter Li", "Juan Pablo Bello", "Crepe", "Kinsler", "Austin R Frey", "Alan B Coppens", "James V Sanders", "John", "Tom Ko", "Vijayaditya Peddinti", "Daniel Povey", "Michael L Seltzer", "Sanjeev Khudanpur", "Stepan Komkov", "Aleksandr Petiushko", "Pattern Recognition", "Harold Levine", "Julian Schwinger", "Songxiang Liu", "Haibin Wu", "Hung", "Lee", "Helen Meng", "Leena Mary", "Springer", "Quatieri", "Pitch", "Zichang Wang", "Wei Zhang", "Peilin Wu", "Haojin Zhu", "Xiaohui Liang", "Yao Liu", "Wivo", "Mobile Ad Hoc Networking", "Michael J Moloney", "Daniel L Hatten", "Acoustic", "Mumtaj Begam", "Nagrani", "J. S. Chung", "Voxceleb", "Joon Son Chung", "Weidi Xie", "Andrew Zisserman", "Johannes Nederveen", "Jason Yosinski", "Jeff Clune", "Deep", "Krzysztof Slot", "Ajith Abraham", "Manuel Gra", "Hybrid Artificial", "Berlin", "Heidelberg", "Springer Berlin Heidelberg", "Ayesha Pervaiz", "Fawad Hussain", "Huma Israr", "Muhammad Ali Tahir", "Fawad Riasat Raja", "Naveed Khan Baloch", "Farruh Ishmanov", "Yousaf Bin Zikria", "Nicholas Carlini", "Garrison Cottrell", "Ian Goodfellow", "Colin Raffel", "Joaquin Qui", "Masashi Sugiyama", "Anton Schwaighofer", "Neil D Lawrence", "Dataset", "Mit Press", "Leann Down", "Adam Jonas", "Elham Tabassi", "Shahin Shamsabadi", "Francisco Sep", "Alberto Abad", "Bhiksha Raj", "Andrea Cavallaro", "Isabel Trancoso", "Foolhd", "Jiacheng Shang", "Si Chen", "Jie Wu", "Mobile Ad Hoc", "Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K. Reiter", "Berrak Sisman", "Mingyang Zhang", "Sakriani Sakti", "Haizhou Li", "Satoshi Nakamura", "Technology Workshop", "Tamara Smyth", "Jonathan S Abel", "Acta Acustica", "Acustica", "Robust DNN", "David Snyder", "Daniel Garcia-Romero", "Kenneth N Stevens", "Rainer Storn", "Kenneth Price", "Global Optimization", "Jiawei Su", "Danilo Vasconcellos Vargas", "Kouichi Sakurai", "Motoaki Kawanabe", "Machine", "Hemlata Tak", "Jose Patino", "Massimiliano Todisco", "Andreas Nautsch", "Nicholas Evans", "Anthony Larcher", "Paul Taylor", "Cambridge", "Wiebe Van Ranst", "Toon Goedem", "Fooling", "Umetani", "Ryan Schmidt", "Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Andrew Senior", "Koray Kavukcuoglu", "Chen Wang", "S Abhishek Anand", "Jian Liu", "Payton Walker", "Chen", "Nitesh Saxena", "Annual Computer Security Applications Conference", "Yao Wang", "Wandong Cai", "Tao Gu", "Wei Shao", "Yannan Li", "Yong Yu", "Yuxuan Wang", "Daisy Stanton", "Yonghui Wu", "Ron J. Weiss", "Navdeep Jaitly", "Zongheng Yang", "Xiao", "Zhifeng Chen", "Samy Bengio", "Quoc Le", "Yannis Agiomyrgiannakis", "Rob Clark", "Rif A. Saurous", "Tacotron", "Eric Wong", "Leslie Rice", "Fast", "Zhizheng Wu", "Tomi Kinnunen", "Junichi Yamagishi", "Federico Alegre", "Speech Communication", "Zuxuan Wu", "Larry Davis", "Tom Goldstein", "Gaoyuan Zhang", "Sijia Liu", "Quanfu Fan", "Mengshu Sun", "Hongge Chen", "Yanzhi Wang", "Xue Lin", "Weilin Xu", "Yanjun Qi", "David Evans", "Chen Yan", "Xiaoyu Ji", "Wenyuan Xu", "Siyang Yuan", "Pengyu Cheng", "Ruiyi Zhang", "Weituo Hao", "Zhe Gan", "Lawrence Carin", "Linghan Zhang", "Sheng Tan", "Jie Yang", "Di Tang", "Xiaofeng Wang", "Weili Han", "Xiangyu Liu", "Kehuan Zhang", "Philip Steels", "Tom", "Low", "Lord", "Phil", "Ill", "Gad", "Gregson", "Philip", "Input"]}{"title": ["The Self-Driving Car: Crossroads at the Bleeding Edge of Artificial Intelligence and Law"], "authors": ["[arxiv.Result.Author('Scott McLachlan'), arxiv.Result.Author('Evangelia Kyrimi'), arxiv.Result.Author('Kudakwashe Dube'), arxiv.Result.Author('Norman Fenton'), arxiv.Result.Author('Burkhard Schafer')]"], "link": ["http://arxiv.org/pdf/2202.02734v1"], "summary": "Artificial intelligence (AI) features are increasingly being embedded in cars\nand are central to the operation of self-driving cars (SDC). There is little or\nno effort expended towards understanding and assessing the broad legal and\nregulatory impact of the decisions made by AI in cars. A comprehensive\nliterature review was conducted to determine the perceived barriers, benefits\nand facilitating factors of SDC in order to help us understand the suitability\nand limitations of existing and proposed law and regulation. (1) existing and\nproposed laws are largely based on claimed benefits of SDV that are still\nmostly speculative and untested; (2) while publicly presented as issues of\nassigning blame and identifying who pays where the SDC is involved in an\naccident, the barriers broadly intersect with almost every area of society,\nlaws and regulations; and (3) new law and regulation are most frequently\nidentified as the primary factor for enabling SDC. Research on assessing the\nimpact of AI in SDC needs to be broadened beyond negligence and liability to\nencompass barriers, benefits and facilitating factors identified in this paper.\nResults of this paper are significant in that they point to the need for deeper\ncomprehension of the broad impact of all existing law and regulations on the\nintroduction of SDC technology, with a focus on identifying only those areas\ntruly requiring ongoing legislative attention.", "entities_include_in_text": ["\n \ns\ne\nl\nc\ni\nh\ne\nV\n\n \nd\nn\na\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \n,\nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \nd\ne\nt\nc\ne\nn\nn\no\nC\n\n \ns\nr\na\nC\n\n \nt\nn\ne\ng\ni\nl\nl\ne\nt\nn\nI\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ns\nu\no\nm\no\nn\no\nt\nu\na\n-\ni\n\nm\ne\nS\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\nr\na\nC\n \ns\nu\no\nm\no\nn\no\nt\nu\na\n-\ni\n\nm\ne\nS\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\nr\na\nC\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ns\ns\ne\nl\nr\ne\nv\ni\nr\n\nD\n\n \ns\nr\na\nC\n \ns\ns\ne\nl\nr\ne\nv\ni\nr\n\nD\n\n \ns\nr\na\nC\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \nr\no\n \ns\nu\no\nm\no\nn\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \nr\no\n \nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\nr\na\nC\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n \nd\ne\nt\na\nm\no\nt\nu\nA\n\n \ns\ne\nl\nc\ni\nh\ne\nV\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n\n \ns\nr\na\nC\n \ng\nn\ni\nv\ni\nr\nd\n-\nf\nl\ne\nS\n\n \ns\nr\na\nC\n \nd\ne\nt\nc\ne\nn\nn\no\nC\n\n \n\n \n\nD\nE\nS\nU\nS\nM\nR\nE\nT\nL\nA\nT\nO\nT\n\n \n\n \n\nTOTAL \n\nGaeta (2019", "ICCESE 2020). \nLeiman, T. (2020", "Reprinted from. \nEC. (2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Aligning Eyes between Humans and Deep Neural Network through Interactive Attention Alignment"], "authors": ["[arxiv.Result.Author('Yuyang Gao'), arxiv.Result.Author('Tong Sun'), arxiv.Result.Author('Liang Zhao'), arxiv.Result.Author('Sungsoo Hong')]"], "link": ["http://arxiv.org/pdf/2202.02838v1"], "summary": "While Deep Neural Networks (DNNs) are deriving the major innovations in\nnearly every field through their powerful automation, we are also witnessing\nthe peril behind automation as a form of bias, such as automated racism, gender\nbias, and adversarial bias. As the societal impact of DNNs grows, finding an\neffective way to steer DNNs to align their behavior with the human mental model\nhas become indispensable in realizing fair and accountable models. We propose a\nnovel framework of Interactive Attention Alignment (IAA) that aims at realizing\nhuman-steerable Deep Neural Networks (DNNs). IAA leverages DNN model\nexplanation method as an interactive medium that humans can use to unveil the\ncases of biased model attention and directly adjust the attention. In improving\nthe DNN using human-generated adjusted attention, we introduce GRADIA, a novel\ncomputational pipeline that jointly maximizes attention quality and prediction\naccuracy. We evaluated IAA framework in Study 1 and GRADIA in Study 2 in a\ngender classification problem. Study 1 found applying IAA can significantly\nimprove the perceived quality of model attention from human eyes. In Study 2,\nwe found using GRADIA can (1) significantly improve the perceived quality of\nmodel attention and (2) significantly improve model performance in scenarios\nwhere the training samples are limited. We present implications for future\ninteractive user interfaces design towards human-alignable AI.", "entities_include_in_text": ["February 2022", "Tesla 2020", "Hong et al. 2020", "Kendall 2020", "Feng and Wu 2020", "Guidotti et al. 2019", "Hong et al. 2020", "Chouldechova and Roth 2018; Saxena\net al. 2019", "Baeza-Yates 2018; Burns et al. 2018;\nChouldechova and Roth 2018", "Barlas et al. 2021; Burns\net al. 2018; Chouldechova and Roth 2018", "Kim et al. 2019; Weiss et al. 2007", "Chawla et al. 2002; He and Garcia 2009", "Wang et al. 2019", "Burns et al. 2018", "Wang et al. 2019", "Burns et al. 2018", "Zhao et al. 2017", "Sagawa et al. 2020", "Burns et al. 2018; Quadrianto et al. 2019", "Selvaraju et al. 2017]", "Selvaraju et al. 2017", "Mitsuhara et al. 2019", "Lin et al. 2014", "Hu et al. 2016; Zhang et al. 2016", "Fukui\net al. 2019", "Mitsuhara\net al. 2019", "Liu and Avci 2019", "Mitsuhara et al. 2019", "Liu and Avci 2019", "Mitsuhara et al. 2019; Shao et al. 2020", "Jacovi and Goldberg 2020; Ross et al. 2017", "Visotsky et al. 2019", "Liu and Avci 2019]", "Kahng\net al. 2018; Pezzotti et al. 2017", "Hall et al. 2009", "Fails and Olsen Jr 2003", "Talbot et al. 2009", "Choo et al. 2010", "Alsallakh et al. 2014", "Ren\net al. 2016", "Dingen et al. 2019", "Brooks et al. 2015", "Amershi et al. 2015", "Kahng et al. 2016", "Krause et al. 2016", "Ming et al. 2018", "Britton 2019", "Kahng et al. 2018; Pezzotti et al. 2017", "Strobelt\net al. 2017", "Ming et al. 2017", "Liu et al. 2016", "Bilal\net al. 2017", "Liu\net al. 2017", "Kahng et al. 2018", "Wongsuphasawat et al. 2017", "Cabrera et al. 2019", "Yan et al.\n2020", "Chouldechova and Roth 2018", "Pedreshi et al.\n2008; Zliobaite 2015", "Kim et al. 2019; Quadrianto et al. 2019", "Calders et al. 2009; Kamiran and Calders 2009", "Aghaei et al. 2019", "Wang et al. 2019", "Calders and Verwer 2010;\nHardt et al. 2016", "Kamiran et al. 2010", "Nushi et al. 2018", "Burns et al. 2018; Wang et al. 2019", "Wang et al. 2019", "Nushi et al. 2018", "Zhou et al. 2016", "Fukui et al. 2019", "Selvaraju et al. 2017", "Mitsuhara et al. 2019", "Liu and Avci 2019", "Mitsuhara et al. 2019", "Barlas et al. 2021; Zhao et al. 2017", "Lin et al. 2014", "Barlas et al. 2021; Burns et al. 2018; Zhao et al. 2017", "Lin et al. 2014", "Zhao et al. 2017", "He et al. 2016", "Bau et al. 2017", "Barlas et al. 2021", "Mitsuhara\net al. 2019", "Fukui et al. 2019; Mitsuhara et al. 2019", "He et al. 2016", "Fukui et al. 2019", "Zhou 2018", "Chung et al. 2021", "Mitsuhara et al. 2019", "Burns et al.\n2018", "Kosti et al. 2017", "Zellers et al. 2019", "Kipf and Welling 2016", "Pope et al. 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Triangle Graph Interest Network for Click-through Rate Prediction"], "authors": ["[arxiv.Result.Author('Wensen Jiang'), arxiv.Result.Author('Yizhu Jiao'), arxiv.Result.Author('Qingqin Wang'), arxiv.Result.Author('Chuanming Liang'), arxiv.Result.Author('Lijie Guo'), arxiv.Result.Author('Yao Zhang'), arxiv.Result.Author('Zhijun Sun'), arxiv.Result.Author('Yun Xiong'), arxiv.Result.Author('Yangyong Zhu')]"], "link": ["http://arxiv.org/pdf/2202.02698v1"], "summary": "Click-through rate prediction is a critical task in online advertising.\nCurrently, many existing methods attempt to extract user potential interests\nfrom historical click behavior sequences. However, it is difficult to handle\nsparse user behaviors or broaden interest exploration. Recently, some\nresearchers incorporate the item-item co-occurrence graph as an auxiliary. Due\nto the elusiveness of user interests, those works still fail to determine the\nreal motivation of user click behaviors. Besides, those works are more biased\ntowards popular or similar commodities. They lack an effective mechanism to\nbreak the diversity restrictions. In this paper, we point out two special\nproperties of triangles in the item-item graphs for recommendation systems:\nIntra-triangle homophily and Inter-triangle heterophiy. Based on this, we\npropose a novel and effective framework named Triangle Graph Interest Network\n(TGIN). For each clicked item in user behavior sequences, we introduce the\ntriangles in its neighborhood of the item-item graphs as a supplement. TGIN\nregards these triangles as the basic units of user interests, which provide the\nclues to capture the real motivation for a user clicking an item. We\ncharacterize every click behavior by aggregating the information of several\ninterest units to alleviate the elusive motivation problem. The attention\nmechanism determines users' preference for different interest units. By\nselecting diverse and relative triangles, TGIN brings in novel and\nserendipitous items to expand exploration opportunities of user interests.\nThen, we aggregate the multi-level interests of historical behavior sequences\nto improve CTR prediction. Extensive experiments on both public and industrial\ndatasets clearly verify the effectiveness of our framework.", "entities_include_in_text": [], "entities_from_reference": ["Andrei Broder", "Michael Mitzenmacher", "Network", "Chen", "Guoxin Zhang", "Fast", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton", "Levent Koc", "Jeremiah Harmsen", "Tushar Chandra", "Hrishi Aradhye", "Glen Anderson", "Greg Corrado", "Wei Chai", "Mustafa Ispir", "Virtual Event", "Fernando Diaz", "Torsten Suel", "Pablo Castells", "Rosie Jones", "Tetsuya Sakai", "Paul Covington", "Jay Adams", "Emre Sargin", "Deng", "Junwei Pan", "Tian Zhou", "Deguang Kong", "Aaron Flores", "Guang Lin", "Data Mining", "Liane Lewin-Eytan", "David Carmel", "Elad Yom-Tov", "Eugene Agichtein", "Evgeniy Gabrilovich", "Jacob Devlin", "Kenton Lee", "Kristina Toutanova", "Hongliang Fei", "Jingyuan Zhang", "Xingxuan Zhou", "Junhao Zhao", "Xinyang Qi", "Feature Interaction Learning", "Binbin Hu", "Fuyu Lv", "Qingwen Liu", "Zhiqiang Zhang", "Wenwu Ou", "Effective Recommendation", "Fei Sun", "Kun Kuang", "Yang Liu", "Multiplex", "Knowledge Management", "Weichen Shen", "Menghan Wang", "Yu Zhu", "Huifeng Guo", "Ruiming Tang", "Ye", "Zhenguo Li", "Jianqiang Huang", "Ke Hu", "Qingtao Tang", "Mingjian Chen", "Yi Qi", "Jia Cheng", "Jun Lei", "Deep Position-wise", "Gary L Miller", "Richard Peng", "Charalampos E Tsourakakis", "Daniel Huttenlocher", "Jon Kleinberg", "Feng Li", "Zhenrui Chen", "Pengjie Wang", "Yi Ren", "Di Zhang", "Xiaoyu Zhu", "Graph Intention Network", "Huichuan Duan", "Yuanjie Zheng", "Qianqian Wang", "Yu Wang", "Appl", "Taiwei Jin", "Changlong Yu", "Quan Lin", "Keping Yang", "Wilfred Ng", "Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton Van Den Hengel", "Mark EJ Newman", "Duncan J Watts", "Steven H Strogatz", "Random", "Weijie Bian", "Guorui Zhou", "Xiaoqiang Zhu", "Kun Gai", "Yanru Qu", "Han Cai", "Kan Ren", "Weinan Zhang", "Yong Yu", "Wen", "Jun Wang", "Yang Song", "Ali Mamdouh Elkahky", "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Kaiser", "Illia Polosukhin", "Hongwei Wang", "Fuzheng Zhang", "Jialin Wang", "Miao Zhao", "Wenjie Li", "Xie", "Minyi Guo", "Wang", "Pipei Huang", "Huan Zhao", "Zhibo Zhang", "Binqiang Zhao", "Dik Lun Lee", "Xiang Wang", "Yixin Cao", "Meng Liu", "Stanley Wasserman", "Katherine Faust", "Social", "Andreas Wimmer", "Kevin Lewis", "Beyond", "Luwei Yang", "Wen Jiang", "Yi Wei", "Yi Hu", "Hao Wang", "Feng Yu", "Qiang Liu", "Shu Wu", "Liang Wang", "Tieniu Tan", "Zhang", "Hao Qian", "Qing Cui", "Qi Liu", "Longfei Li", "Jun Zhou", "Jianhui Ma", "Enhong Chen", "Feature Learning", "Chang Zhou", "Jinze Bai", "Junshuai Song", "Xiaofei Liu", "Zhengchao Zhao", "Xiusi Chen", "Jun Gao", "Atrank", "Vol", "Na Mou", "Fan", "Qi Pi", "Chenru Song", "Han Zhu", "Xiao Ma", "Yanghui Yan", "Junqi Jin", "Han Li"]}{"title": ["A survey of top-down approaches for human pose estimation"], "authors": ["[arxiv.Result.Author('Thong Duy Nguyen'), arxiv.Result.Author('Milan Kresovic')]"], "link": ["http://arxiv.org/pdf/2202.02656v1"], "summary": "Human pose estimation in two-dimensional images videos has been a hot topic\nin the computer vision problem recently due to its vast benefits and potential\napplications for improving human life, such as behaviors recognition, motion\ncapture and augmented reality, training robots, and movement tracking. Many\nstate-of-the-art methods implemented with Deep Learning have addressed several\nchallenges and brought tremendous remarkable results in the field of human pose\nestimation. Approaches are classified into two kinds: the two-step framework\n(top-down approach) and the part-based framework (bottom-up approach). While\nthe two-step framework first incorporates a person detector and then estimates\nthe pose within each box independently, detecting all body parts in the image\nand associating parts belonging to distinct persons is conducted in the\npart-based framework. This paper aims to provide newcomers with an extensive\nreview of deep learning methods-based 2D images for recognizing the pose of\npeople, which only focuses on top-down approaches since 2016. The discussion\nthrough this paper presents significant detectors and estimators depending on\nmathematical background, the challenges and limitations, benchmark datasets,\nevaluation metrics, and comparison between methods.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Doing Right by Not Doing Wrong in Human-Robot Collaboration"], "authors": ["[arxiv.Result.Author('Laura Londo\u00f1o'), arxiv.Result.Author('Adrian R\u00f6fer'), arxiv.Result.Author('Tim Welschehold'), arxiv.Result.Author('Abhinav Valada')]"], "link": ["http://arxiv.org/pdf/2202.02654v1"], "summary": "As robotic systems become more and more capable of assisting humans in their\neveryday lives, we must consider the opportunities for these artificial agents\nto make their human collaborators feel unsafe or to treat them unfairly. Robots\ncan exhibit antisocial behavior causing physical harm to people or reproduce\nunfair behavior replicating and even amplifying historical and societal biases\nwhich are detrimental to humans they interact with. In this paper, we discuss\nthese issues considering sociable robotic manipulation and fair robotic\ndecision making. We propose a novel approach to learning fair and sociable\nbehavior, not by reproducing positive behavior, but rather by avoiding negative\nbehavior. In this study, we highlight the importance of incorporating\nsociability in robot manipulation, as well as the need to consider fairness in\nhuman-robot interactions.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network"], "authors": ["[arxiv.Result.Author('Xiaomin Li'), arxiv.Result.Author('Vangelis Metsis'), arxiv.Result.Author('Huangyingrui Wang'), arxiv.Result.Author('Anne Hee Hiong Ngu')]"], "link": ["http://arxiv.org/pdf/2202.02691v1"], "summary": "Signal measurements appearing in the form of time series are one of the most\ncommon types of data used in medical machine learning applications. However,\nsuch datasets are often small, making the training of deep neural network\narchitectures ineffective. For time-series, the suite of data augmentation\ntricks we can use to expand the size of the dataset is limited by the need to\nmaintain the basic properties of the signal. Data generated by a Generative\nAdversarial Network (GAN) can be utilized as another data augmentation tool.\nRNN-based GANs suffer from the fact that they cannot effectively model long\nsequences of data points with irregular temporal relations. To tackle these\nproblems, we introduce TTS-GAN, a transformer-based GAN which can successfully\ngenerate realistic synthetic time-series data sequences of arbitrary length,\nsimilar to the real ones. Both the generator and discriminator networks of the\nGAN model are built using a pure transformer encoder architecture. We use\nvisualizations and dimensionality reduction techniques to demonstrate the\nsimilarity of real and generated time-series data. We also compare the quality\nof our generated data with the best existing alternative, which is an RNN-based\ntime-series GAN.", "entities_include_in_text": [], "entities_from_reference": ["Bousmalis", "Silberman", "Dohan", "Erhan", "Krishnan", "Bousseljot", "Kreiseler", "Schnabel", "Nutzung", "Brophy", "Wang", "Ward", "Devlin", "Chang", "Lee", "Toutanova", "Bert", "Diao", "Shum", "Zhang", "Dosovitskiy", "Kolesnikov", "Zhai", "Dehghani", "Minderer", "Heigold", "Gelly", "Esteban", "Goldberger", "Glass", "Hausdorff", "Ivanov", "Mark", "Moody", "Peng", "Stanley", "Physiobank", "Goodfellow", "Mirza", "Ozair", "Bengio", "Huang", "Li", "Global", "Jiang", "Ledig", "Huszar", "Caballero", "Aitken", "Tejani", "Totz", "Van", "Maaten", "Mao", "Xie", "Lau", "Paul Smolley", "Micucci", "Mobilio", "Liao", "Ratliff", "Annual Allerton Conference", "Vaswani", "Shazeer", "Uszkoreit", "Jones", "Gomez", "Polosukhin", "Wold", "Yoon", "Jarrett", "Schaar", "Feature", "Average Cosine Similarity", "Normal ECG", "Abnormal ECG", "Abnormal ECG Fig"]}{"title": ["Learning Synthetic Environments and Reward Networks for Reinforcement Learning"], "authors": ["[arxiv.Result.Author('Fabio Ferreira'), arxiv.Result.Author('Thomas Nierhoff'), arxiv.Result.Author('Andreas Saelinger'), arxiv.Result.Author('Frank Hutter')]"], "link": ["http://arxiv.org/pdf/2202.02790v1"], "summary": "We introduce Synthetic Environments (SEs) and Reward Networks (RNs),\nrepresented by neural networks, as proxy environment models for training\nReinforcement Learning (RL) agents. We show that an agent, after being trained\nexclusively on the SE, is able to solve the corresponding real environment.\nWhile an SE acts as a full proxy to a real environment by learning about its\nstate dynamics and rewards, an RN is a partial proxy that learns to augment or\nreplace rewards. We use bi-level optimization to evolve SEs and RNs: the inner\nloop trains the RL agent, and the outer loop trains the parameters of the SE /\nRN via an evolution strategy. We evaluate our proposed new concept on a broad\nrange of RL algorithms and classic control environments. In a one-to-one\ncomparison, learning an SE proxy requires more interactions with the real\nenvironment than training agents only on the real environment. However, once\nsuch an SE has been learned, we do not need any interactions with the real\nenvironment to train new agents. Moreover, the learned SE proxies allow us to\ntrain agents with fewer interactions while maintaining the original task\nperformance. Our empirical results suggest that SEs achieve this result by\nlearning informed representations that bias the agents towards relevant states.\nMoreover, we find that these proxies are robust against hyperparameter\nvariation and can also transfer to unseen agents.", "entities_include_in_text": ["Such et al., 2020", "Jhang et al., 2020", "Rechenberg, 1973;\nSalimans et al., 2017", "Sutton, 1990)", "Ng et al., 1999", "Brockman et al., 2016", "Hutter et al., 2019", "Paszke et al., 2019", "Sutton, 1990", "Silver et al., 2017; Moerland et al., 2020", "Togelius et al., 2011", "Matiisen et al., 2020", "Volz et al., 2018; Shaker et al., 2016; Wang et al., 2019; Cobbe et al., 2020", "Tobin et al., 2017", "Dennis et al., 2020", "Such et al., 2020", "Pathak et al., 2017; Burda et al., 2019; Singh\net al., 2010; Bellemare et al., 2016; Tang et al., 2017", "Judah et al., 2014; Brys et al., 2015; Ibarz et al., 2018", "Faust et al., 2019; Hu et al., 2020; Jaderberg et al., 2019", "Zheng et al., 2018", "Zou\net al., 2019", "Zheng et al., 2020", "Zheng et al., 2018;\nZou et al., 2019", "Zheng et al.,\n2018", "Zou et al., 2019", "Zheng et al., 2020", "Wierstra et al., 2008", "Metz et al.,\n2019", "Salimans et al., 2017", "Salimans et al.,\nIt consists of an Evolutionary Strategy in the outer loop\n2017", "Salimans et al., 2017", "Ng et al., 1999", "Ng et al., 1999", "Ng et al., 1999", "van Hasselt et al., 2016", "Wang et al., 2016", "Jang et al., 2017", "Falkner et al., 2018", "Watkins, 1989", "Brockman et al., 2016", "Schulman et al., 2017", "Pathak et al., 2017", "Ng et al., 1999", "Salimans et al., 2017", "Jabbari et al., 2017", "Salimans et al.,\n2017", "Sutton et al., 2020", "Pathak et al., 2017", "Pathak et al., 2017", "Pathak et al., 2017", "Pathak et al., 2017", "Wierstra et al., 2008"], "entities_from_reference": ["Plusieurs"]}{"title": ["Ethics, Rules of Engagement, and AI: Neural Narrative Mapping Using Large Transformer Language Models"], "authors": ["[arxiv.Result.Author('Philip Feldman'), arxiv.Result.Author('Aaron Dant'), arxiv.Result.Author('David Rosenbluth')]"], "link": ["http://arxiv.org/pdf/2202.02647v1"], "summary": "The problem of determining if a military unit has correctly understood an\norder and is properly executing on it is one that has bedeviled military\nplanners throughout history. The advent of advanced language models such as\nOpenAI's GPT-series offers new possibilities for addressing this problem. This\npaper presents a mechanism to harness the narrative output of large language\nmodels and produce diagrams or \"maps\" of the relationships that are latent in\nthe weights of such models as the GPT-3. The resulting \"Neural Narrative Maps\"\n(NNMs), are intended to provide insight into the organization of information,\nopinion, and belief in the model, which in turn provide means to understand\nintent and response in the context of physical distance. This paper discusses\nthe problem of mapping information spaces in general, and then presents a\nconcrete implementation of this concept in the context of OpenAI's GPT-3\nlanguage model for determining if a subordinate is following a commander's\nintent in a high-risk situation. The subordinate's locations within the NNM\nallow a novel capability to evaluate the intent of the subordinate with respect\nto the commander. We show that is is possible not only to determine if they are\nnearby in narrative space, but also how they are oriented, and what\n\"trajectory\" they are on. Our results show that our method is able to produce\nhigh-quality maps, and demonstrate new ways of evaluating intent more\ngenerally.", "entities_include_in_text": ["LREC 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["A Graph Neural Network Framework for Grid-Based Simulation"], "authors": ["[arxiv.Result.Author('Haoyu Tang'), arxiv.Result.Author('Wennan Long')]"], "link": ["http://arxiv.org/pdf/2202.02652v1"], "summary": "Reservoir simulations are computationally expensive in the well control and\nwell placement optimization. Generally, numerous simulation runs (realizations)\nare needed in order to achieve the optimal well locations. In this paper, we\npropose a graph neural network (GNN) framework to build a surrogate\nfeed-forward model which replaces simulation runs to accelerate the\noptimization process. Our GNN framework includes an encoder, a process, and a\ndecoder which takes input from the processed graph data designed and generated\nfrom the simulation raw data. We train the GNN model with 6000 samples\n(equivalent to 40 well configurations) with each containing the previous step\nstate variable and the next step state variable. We test the GNN model with\nanother 6000 samples and after model tuning, both one-step prediction and\nrollout prediction achieve a close match with the simulation results. Our GNN\nframework shows great potential in the application of well-related subsurface\noptimization including oil and gas as well as carbon capture sequestration\n(CCS).", "entities_include_in_text": [], "entities_from_reference": ["Haoyu Tang", "Louis J Durlofsky", "Alvaro Sanchez-Gonzalez", "Jonathan Godwin", "Tobias Pfaff", "Rex Ying", "Jure Leskovec", "Peter Battaglia", "Machine Learning", "Nicolas Heess", "Jost Tobias Springenberg", "Josh Merel", "Martin Riedmiller", "Raia Hadsell", "Meire Fortunato", "Peter W Battaglia"]}{"title": ["Exploration with Multi-Sample Target Values for Distributional Reinforcement Learning"], "authors": ["[arxiv.Result.Author('Michael Teng'), arxiv.Result.Author('Michiel van de Panne'), arxiv.Result.Author('Frank Wood')]"], "link": ["http://arxiv.org/pdf/2202.02693v1"], "summary": "Distributional reinforcement learning (RL) aims to learn a value-network that\npredicts the full distribution of the returns for a given state, often modeled\nvia a quantile-based critic. This approach has been successfully integrated\ninto common RL methods for continuous control, giving rise to algorithms such\nas Distributional Soft Actor-Critic (DSAC). In this paper, we introduce\nmulti-sample target values (MTV) for distributional RL, as a principled\nreplacement for single-sample target value estimation, as commonly employed in\ncurrent practice. The improved distributional estimates further lend themselves\nto UCB-based exploration. These two ideas are combined to yield our\ndistributional RL algorithm, E2DC (Extra Exploration with Distributional\nCritics). We evaluate our approach on a range of continuous control tasks and\ndemonstrate state-of-the-art model-free performance on difficult tasks such as\nHumanoid control. We provide further insight into the method via visualization\nand analysis of the learned distributions and their evolution during training.", "entities_include_in_text": ["Lee\net al., 2020", "Barth-Maron et al., 2018", "Ma et al., 2020", "Haarnoja et al.,\n2017", "Van Hasselt\net al., 2016; Wang et al., 2016; Lan et al., 2020", "Mavrin et al., 2018", "Todorov\net al., 2012", "Tassa et al., 2020", "Brockman et al., 2016", "Kuznetsov et al., 2020", "Yue\net al., 2020; Ward et al., 2019", "Gangwani\net al., 2018"], "entities_from_reference": ["Gabriel", "Matthew W Hoffman", "David Budden", "Will Dabney", "Dan Horgan", "Dhruva Tb", "Alistair Muldal", "Nicolas Heess", "Timothy Lillicrap", "Gangwani", "Standard", "Extra Exploration", "Machine Learning", "Marc G Bellemare", "Remi Munos", "Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba", "Openai", "Tuomas Haarnoja", "Aurick Zhou", "Kristian Hartikainen", "George Tucker", "Sehoon Ha", "Jie Tan", "Vikash Kumar", "Henry Zhu", "Abhishek Gupta", "Pieter Abbeel", "Sergey Levine", "Kumar", "Justin Fu", "Richard Y Chen", "Szymon Sidor", "Thanard Kurutach", "Ignasi Clavera", "Aviv Tamar", "Kamil Ciosek", "Quan Vuong", "Robert Loftin", "Katja Hofmann", "Better", "Georg Ostrovski", "David Silver", "Implicit", "Dabney", "Mark Rowland", "Marc Bellemare", "Scott Fujimoto", "Herke Hoof", "David Meger", "Tanmay Gangwani", "Qiang Liu", "Jian Peng", "Shixiang Gu", "Zoubin Ghahramani", "Richard E Turner", "Haoran Tang", "Arsenii Kuznetsov", "Pavel Shvechikov", "Alexander Grishin", "Dmitry Vetrov", "Qingfeng Lan", "Yangchen Pan", "Alona Fyshe", "Martha White", "Maxmin", "Kimin Lee", "Michael Laskin", "Aravind Srinivas", "Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Tom Erez", "Yuval Tassa", "Daan Wierstra", "Xiaoteng Ma", "Qiyuan Zhang", "Li Xia", "Zhengyuan Zhou", "Jun Yang", "Qianchuan Zhao", "Borislav Mavrin", "Hengshuai Yao", "Linglong Kong", "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Tim Harley", "Koray Kavukcuoglu", "Thomas M Moerland", "Joost Broekens", "Charles Blundell", "Benjamin Van Roy", "Deep", "M Dalal", "S Lin", "Rlkit", "Philipp Moritz", "Michael Jordan", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov", "Yunhao Tang", "Shipra Agrawal", "Saran Tunyasuvunakool", "Yotam Doron", "Siqi Liu", "Steven Bohez", "Josh Merel", "Deepmind", "Emanuel Todorov", "Mujoco", "Systems", "Hado Van Hasselt", "Arthur Guez", "Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado Hasselt", "Marc Lanctot", "Nando Freitas", "Patrick Nadeem Ward", "Ariella Smofsky", "Avishek Joey Bose", "Yuguang Yue", "Zhendong Wang"]}{"title": ["The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training"], "authors": ["[arxiv.Result.Author('Shiwei Liu'), arxiv.Result.Author('Tianlong Chen'), arxiv.Result.Author('Xiaohan Chen'), arxiv.Result.Author('Li Shen'), arxiv.Result.Author('Decebal Constantin Mocanu'), arxiv.Result.Author('Zhangyang Wang'), arxiv.Result.Author('Mykola Pechenizkiy')]"], "link": ["http://arxiv.org/pdf/2202.02643v1"], "summary": "Random pruning is arguably the most naive way to attain sparsity in neural\nnetworks, but has been deemed uncompetitive by either post-training pruning or\nsparse training. In this paper, we focus on sparse training and highlight a\nperhaps counter-intuitive finding, that random pruning at initialization can be\nquite powerful for the sparse training of modern neural networks. Without any\ndelicate pruning criteria or carefully pursued sparsity structures, we\nempirically demonstrate that sparsely training a randomly pruned network from\nscratch can match the performance of its dense equivalent. There are two key\nfactors that contribute to this revival: (i) the network sizes matter: as the\noriginal dense networks grow wider and deeper, the performance of training a\nrandomly pruned sparse network will quickly grow to matching that of its dense\nequivalent, even at high sparsity ratios; (ii) appropriate layer-wise sparsity\nratios can be pre-chosen for sparse training, which shows to be another\nimportant performance booster. Simple as it looks, a randomly pruned subnetwork\nof Wide ResNet-50 can be sparsely trained to outperforming a dense Wide\nResNet-50, on ImageNet. We also observed such randomly pruned networks\noutperform dense counterparts in other favorable aspects, such as\nout-of-distribution detection, uncertainty estimation, and adversarial\nrobustness. Overall, our results strongly suggest there is larger-than-expected\nroom for sparse training at scale, and the benefits of sparsity might be more\nuniversal beyond carefully designed pruning. Our source code can be found at\nhttps://github.com/VITA-Group/Random_Pruning.", "entities_include_in_text": ["Neyshabur et al., 2019; Novak et al., 2018; Allen-Zhu et al., 2019", "Dai et al., 2018", "Molchanov et al., 2016", "Sanh et al., 2020", "Lee\net al., 2019", "Mocanu et al., 2018; Lee et al., 2019; Gale et al., 2019; Wang et al., 2020; Tanaka et al.,\n2020", "Mocanu et al., 2016;\nGale et al., 2019; Lee et al., 2019; Wang et al., 2020", "Gale et al., 2019; Lee et al., 2019; Frankle et al., 2021; Tanaka et al.,\n2020", "Gale et al., 2019", "Mocanu et al., 2018", "Mocanu et al., 2018", "de Jorge et al., 2021; Verdenius et al., 2020). Su et al. (2020); Frankle et al. (2021", "Su et al., 2020", "Kusupati et al., 2020", "He et al., 2016", "Krizhevsky et al., 2009", "Deng et al., 2009", "Hendrycks et al., 2021", "Goodfellow et al., 2014", "Lakshminarayanan et al., 2016", "shown by Tsipras et al. (2019); Zhang et al. (2019", "Deng\net al., 2009", "Tessera et al., 2021", "Wang et al., 2020", "Frankle et al., 2020; Renda et al., 2020", "ICLR 2020", "Goodfellow et al., 2014", "Hendrycks\net al., 2021", "Lakshminarayanan et al., 2016", "Szegedy\net al., 2013; Papernot et al., 2016", "Guo et al., 2018; Ye et al., 2019; Gui et al., 2019; Hu et al.,\n2020). As the arguably most naive method of inducing sparsity, we are also interested in if training a\nrandomly pruned subnetwork can improve the adversarial robustness of deep networks. We follow\nthe classical method proposed in Goodfellow et al. (2014", "Netzer et al., 2011", "Hein et al., 2019", "Guo et al., 2017", "Guo et al., 2017", "Friedman\net al., 2001"], "entities_from_reference": ["Plusieurs"]}{"title": ["Privacy-preserving Speech Emotion Recognition through Semi-Supervised Federated Learning"], "authors": ["[arxiv.Result.Author('Vasileios Tsouvalas'), arxiv.Result.Author('Tanir Ozcelebi'), arxiv.Result.Author('Nirvana Meratnia')]"], "link": ["http://arxiv.org/pdf/2202.02611v1"], "summary": "Speech Emotion Recognition (SER) refers to the recognition of human emotions\nfrom natural speech. If done accurately, it can offer a number of benefits in\nbuilding human-centered context-aware intelligent systems. Existing SER\napproaches are largely centralized, without considering users' privacy.\nFederated Learning (FL) is a distributed machine learning paradigm dealing with\ndecentralization of privacy-sensitive personal data. In this paper, we present\na privacy-preserving and data-efficient SER approach by utilizing the concept\nof FL. To the best of our knowledge, this is the first federated SER approach,\nwhich utilizes self-training learning in conjunction with federated learning to\nexploit both labeled and unlabeled on-device data. Our experimental evaluations\non the IEMOCAP dataset shows that our federated approach can learn\ngeneralizable SER models even under low availability of data labels and highly\nnon-i.i.d. distributions. We show that our approach with as few as 10% labeled\ndata, on average, can improve the recognition rate by 8.67% compared to the\nfully-supervised federated counterparts.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Graph Neural Network with Curriculum Learning for Imbalanced Node Classification"], "authors": ["[arxiv.Result.Author('Xiaohe Li'), arxiv.Result.Author('Lijie Wen'), arxiv.Result.Author('Yawen Deng'), arxiv.Result.Author('Fuli Feng'), arxiv.Result.Author('Xuming Hu'), arxiv.Result.Author('Lei Wang'), arxiv.Result.Author('Zide Fan')]"], "link": ["http://arxiv.org/pdf/2202.02529v1"], "summary": "Graph Neural Network (GNN) is an emerging technique for graph-based learning\ntasks such as node classification. In this work, we reveal the vulnerability of\nGNN to the imbalance of node labels. Traditional solutions for imbalanced\nclassification (e.g. resampling) are ineffective in node classification without\nconsidering the graph structure. Worse still, they may even bring overfitting\nor underfitting results due to lack of sufficient prior knowledge. To solve\nthese problems, we propose a novel graph neural network framework with\ncurriculum learning (GNN-CL) consisting of two modules. For one thing, we hope\nto acquire certain reliable interpolation nodes and edges through the novel\ngraph-based oversampling based on smoothness and homophily. For another, we\ncombine graph classification loss and metric learning loss which adjust the\ndistance between different nodes associated with minority class in feature\nspace. Inspired by curriculum learning, we dynamically adjust the weights of\ndifferent modules during training process to achieve better ability of\ngeneralization and discrimination. The proposed framework is evaluated via\nseveral widely used graph datasets, showing that our proposed model\nconsistently outperforms the existing state-of-the-art methods.", "entities_include_in_text": [], "entities_from_reference": ["Scott", "John", "Carrington", "Peter J", "Thomas N.", "Max Welling", "Hamilton", "Will", "Zhitao Ying", "Jure Leskovec", "Nathalie", "Shaju Stephen", "Gary M", "Robert C. Holte", "Workshop", "Vol", "Kai Ming", "Machine Learning", "Nitesh V.", "Jianxin Wu", "Man", "Part B", "Shaogang Gong", "Xiatian Zhu", "Micha", "Xavier Bresson", "Pierre Vandergheynst", "Joan", "Petar", "Graph", "Hui", "Wang", "Springer", "Berlin", "Heidelberg", "Yifan", "Zhao", "Tianxiang", "Xiang Zhang", "Suhang Wang", "Imbalanced Node", "Graph Neural Networks", "Data Mining", "Yiru", "Dynamic", "Galileo Namata", "Bilgic", "Model Performance", "True Positive", "Cora", "Amazon", "Coauthor CS", "Adam", "Sage TP", "Citeeer Class Id", "Amazon Comp", "Dataset Cora Citeseer"]}{"title": ["Communication Efficient Federated Learning via Ordered ADMM in a Fully Decentralized Setting"], "authors": ["[arxiv.Result.Author('Yicheng Chen'), arxiv.Result.Author('Rick S. Blum'), arxiv.Result.Author('Brian M. Sadler')]"], "link": ["http://arxiv.org/pdf/2202.02580v1"], "summary": "The challenge of communication-efficient distributed optimization has\nattracted attention in recent years. In this paper, a communication efficient\nalgorithm, called ordering-based alternating direction method of multipliers\n(OADMM) is devised in a general fully decentralized network setting where a\nworker can only exchange messages with neighbors. Compared to the classical\nADMM, a key feature of OADMM is that transmissions are ordered among workers at\neach iteration such that a worker with the most informative data broadcasts its\nlocal variable to neighbors first, and neighbors who have not transmitted yet\ncan update their local variables based on that received transmission. In OADMM,\nwe prohibit workers from transmitting if their current local variables are not\nsufficiently different from their previously transmitted value. A variant of\nOADMM, called SOADMM, is proposed where transmissions are ordered but\ntransmissions are never stopped for each node at each iteration. Numerical\nresults demonstrate that given a targeted accuracy, OADMM can significantly\nreduce the number of communications compared to existing algorithms including\nADMM. We also show numerically that SOADMM can accelerate convergence,\nresulting in communication savings compared to the classical ADMM.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Intent Contrastive Learning for Sequential Recommendation"], "authors": ["[arxiv.Result.Author('Yongjun Chen'), arxiv.Result.Author('Zhiwei Liu'), arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Julian McAuley'), arxiv.Result.Author('Caiming Xiong')]"], "link": ["http://arxiv.org/pdf/2202.02519v1"], "summary": "Users' interactions with items are driven by various intents (e.g., preparing\nfor holiday gifts, shopping for fishing equipment, etc.).However, users'\nunderlying intents are often unobserved/latent, making it challenging to\nleverage such latent intents forSequentialrecommendation(SR). To investigate\nthe benefits of latent intents and leverage them effectively for\nrecommendation, we proposeIntentContrastiveLearning(ICL), a general learning\nparadigm that leverages a latent intent variable into SR. The core idea is to\nlearn users' intent distribution functions from unlabeled user behavior\nsequences and optimize SR models with contrastive self-supervised learning\n(SSL) by considering the learned intents to improve recommendation.\nSpecifically, we introduce a latent variable to represent users' intents and\nlearn the distribution function of the latent variable via clustering. We\npropose to leverage the learned intents into SR models via contrastive SSL,\nwhich maximizes the agreement between a view of sequence and its corresponding\nintent. The training is alternated between intent representation learning and\nthe SR model optimization steps within the generalized expectation-maximization\n(EM) framework. Fusing user intent information into SR also improves model\nrobustness. Experiments conducted on four real-world datasets demonstrate the\nsuperiority of the proposed learning paradigm, which improves performance, and\nrobustness against data sparsity and noisy interaction issues.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["TorchMD-NET: Equivariant Transformers for Neural Network based Molecular Potentials"], "authors": ["[arxiv.Result.Author('Philipp Th\u00f6lke'), arxiv.Result.Author('Gianni De Fabritiis')]"], "link": ["http://arxiv.org/pdf/2202.02541v1"], "summary": "The prediction of quantum mechanical properties is historically plagued by a\ntrade-off between accuracy and speed. Machine learning potentials have\npreviously shown great success in this domain, reaching increasingly better\naccuracy while maintaining computational efficiency comparable with classical\nforce fields. In this work we propose TorchMD-NET, a novel equivariant\ntransformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1,\nand many QM9 targets in both accuracy and computational efficiency. Through an\nextensive attention weight analysis, we gain valuable insights into the black\nbox predictor and show differences in the learned representation of conformers\nversus conformations sampled from molecular dynamics or normal modes.\nFurthermore, we highlight the importance of datasets including off-equilibrium\nconformations for the evaluation of molecular potentials.", "entities_include_in_text": ["Pfau et al., 2020; Hermann et al., 2020", "Hermann et al., 2020", "Wang et al., 2019; Husic et al., 2020; Doerr et al., 2021", "Luong et al., 2015", "Vaswani\net al., 2017", "Ramakrishnan et al., 2014)", "Chmiela et al., 2017", "Ba et al., 2016", "Ramakrishnan et al., 2014", "Chmiela\net al., 2017", "Anderson et al., 2019", "Sator-\nras et al., 2021", "Hutchinson et al., 2020", "Hutchinson\net al., 2020", "Batzner\net al., 2021", "Lu et al., 2019", "Paszke et al., 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques"], "authors": ["[arxiv.Result.Author('Sreenivasa Hikkal Venugopala')]"], "link": ["http://arxiv.org/pdf/2202.02521v1"], "summary": "Estimating and understanding the surroundings of the vehicle precisely forms\nthe basic and crucial step for the autonomous vehicle. The perception system\nplays a significant role in providing an accurate interpretation of a vehicle's\nenvironment in real-time. Generally, the perception system involves various\nsubsystems such as localization, obstacle (static and dynamic) detection, and\navoidance, mapping systems, and others. For perceiving the environment, these\nvehicles will be equipped with various exteroceptive (both passive and active)\nsensors in particular cameras, Radars, LiDARs, and others. These systems are\nequipped with deep learning techniques that transform the huge amount of data\nfrom the sensors into semantic information on which the object detection and\nlocalization tasks are performed. For numerous driving tasks, to provide\naccurate results, the location and depth information of a particular object is\nnecessary. 3D object detection methods, by utilizing the additional pose data\nfrom the sensors such as LiDARs, stereo cameras, provides information on the\nsize and location of the object. Based on recent research, 3D object detection\nframeworks performing object detection and localization on LiDAR data and\nsensor fusion techniques show significant improvement in their performance. In\nthis work, a comparative study of the effect of using LiDAR data for object\ndetection frameworks and the performance improvement seen by using sensor\nfusion techniques are performed. Along with discussing various state-of-the-art\nmethods in both the cases, performing experimental analysis, and providing\nfuture research directions.", "entities_include_in_text": [], "entities_from_reference": ["Feng", "Rosenbaum", "Hertlein", "Timm", "Object Detection", "Methods", "Dianati", "Fallah", "Mouzakitis", "Mao", "Lang", "Vora", "Caesar", "Zhou", "Yang", "Fast Encoders", "Pattern Recognition", "Acharya", "Rafii", "Range Camera", "Rosique", "Navarro", "Fernandez", "Padilla", "Feng S. Sensor", "Castanedo", "Data Fusion Techniques", "Vasudevan S. Data", "Robotics Auton", "Basic Engineering", "Deng", "Socher", "Li", "Shape Recognition", "Zhang", "Xia", "Vehicle Detection", "Network", "Yue", "Keutzer", "Recurrent CRF", "Milz", "Gross", "Point Clouds", "Guindel", "Moreno", "Cruzado", "Garcia", "Dietmayer", "Towards Safe Autonomous Driving", "Rao", "Wang", "Posner", "Tuzel", "Point Cloud", "Pattern Object Detection", "Liu", "Jia", "Object Detector", "Shi", "Vazquez", "Lopez", "Amores", "Multicue", "Multimodal", "Multiview Random Forest", "Man", "Enzweiler", "Gavrila", "Pedestrian Classification", "Image Processing", "Lee", "Harakeh", "View Aggregation", "Liang", "Urtasun", "Deep Continuous Fusion", "Hegde", "Sensor Fusion", "Pattern Recognition Workshops", "Guibas", "Frustum", "Geiger", "Lenz", "Stiller", "Vision", "Robotics Research", "Gool", "Williams", "Zisserman"]}{"title": ["A Coalition Formation Game Approach for Personalized Federated Learning"], "authors": ["[arxiv.Result.Author('Leijie Wu')]"], "link": ["http://arxiv.org/pdf/2202.02502v1"], "summary": "Facing the challenge of statistical diversity in client local data\ndistribution, personalized federated learning (PFL) has become a growing\nresearch hotspot. Although the state-of-the-art methods with model\nsimilarity-based pairwise collaboration have achieved promising performance,\nthey neglect the fact that model aggregation is essentially a collaboration\nprocess within the coalition, where the complex multiwise influences take place\namong clients. In this paper, we first apply Shapley value (SV) from coalition\ngame theory into the PFL scenario. To measure the multiwise collaboration among\na group of clients on the personalized learning performance, SV takes their\nmarginal contribution to the final result as a metric. We propose a novel\npersonalized algorithm: pFedSV, which can 1. identify each client's optimal\ncollaborator coalition and 2. perform personalized model aggregation based on\nSV. Extensive experiments on various datasets (MNIST, Fashion-MNIST, and\nCIFAR-10) are conducted with different Non-IID data settings (Pathological and\nDirichlet). The results show that pFedSV can achieve superior personalized\naccuracy for each client, compared to the state-of-the-art benchmarks.", "entities_include_in_text": ["McMahan et al., 2017", "Kairouz et al., 2019", "Cortes and Mohri, 2014; Mansour et al., 2020;\nWang et al., 2019", "Donahue and Klein-\nberg, 2021", "Myerson, 2013", "Mansour et al., 2020; Wang et al., 2019", "T Dinh et\nal., 2020", "Li et al., 2020", "Jiang et al., 2019", "Shamsian et al., 2021", "Huang et al., 2021", "Kairouz et al., 2019;\nZhao et al., 2018", "Zhao et al., 2018; Li et al., 2020;\nLi et al., 2019; Karimireddy et al., 2020", "Mann and\nShapley, 1962; Castro et al., 2009; Maleki et al., 2013", "Maleki et al., 2013", "T Dinh et al., 2020", "Shamsian et al., 2021", "Huang et al., 2021", "McMahan et al., 2017", "Li et al.,\n\n2020", "LeCun, 1998", "Xiao et al., 2017", "Krizhevsky et al., 2009", "Collins et al., 2021", "Abadi et al.,\n2016", "Abadi et al., 2016", "Castro et al., 2009", "Collins et al., 2021", "Cortes and Mohri, 2014", "Donahue and Kleinberg, 2021", "Huang et al., 2021", "Jiang et al., 2019", "Kairouz et al., 2019", "Karimireddy et al., 2020", "Krizhevsky et al., 2009", "LeCun, 1998", "Li et al., 2019", "Li et al., 2020", "Maleki et al., 2013", "Mann and Shapley, 1962", "Mansour et al., 2020", "McMahan et al., 2017", "Myerson, 2013", "Shamsian et al., 2021", "T Dinh et al., 2020", "Wang et al., 2019", "Xiao et al., 2017", "Zhao et al., 2018"], "entities_from_reference": ["Martin Abadi", "Andy Chu", "Ian Goodfellow", "H Brendan McMahan", "Ilya Mironov", "Kunal Talwar", "Li Zhang", "Javier Castro", "Daniel G", "Juan Polynomial", "Tejada", "Collins", "Liam Collins", "Hamed Hassani", "Aryan Mokhtari", "Sanjay Shakkottai", "Mohri", "Corinna Cortes", "Mehryar Mohri", "Domain", "Donahue", "Kleinberg", "Jon Kleinberg", "Huang", "Yutao Huang", "Lingyang Chu", "Zirui Zhou", "Lanjun Wang", "Jiangchuan Liu", "Jian Pei", "Yong Zhang", "Jiang", "Yihan Jiang", "Jakub Konecn", "Keith Rush", "Sreeram Kannan", "Kairouz", "Peter Kairouz", "Brendan Avent", "Aur", "Mehdi Bennis", "Arjun Nitin Bhagoji", "Kallista Bonawitz", "Zachary Charles", "Graham Cormode", "Rachel Cummings", "Karimireddy", "Satyen Kale", "Sashank Reddi", "Ananda Theertha Suresh", "Stochastic", "Machine Learning", "Praneeth", "Alex Krizhevsky", "Geoffrey Hinton", "Xiang Li", "Kaixuan Huang", "Wenhao Yang", "Shusen Wang", "Zhihua Zhang", "Tian Li", "Anit Kumar Sahu", "Manzil Zaheer", "Maziar Sanjabi", "Ameet Talwalkar", "Systems", "Maleki", "Sasan Maleki", "Greg Hines", "Talal Rahwan", "Alex Rogers", "Mann", "Shapley", "Irwin Mann", "Lloyd S Shapley", "Yishay Mansour", "Jae Ro", "Brendan McMahan", "Eider Moore", "Daniel Ramage", "Seth Hampson", "Blaise Aguera", "Arcas", "Myerson", "Roger B Myerson", "Aviv Shamsian", "Aviv Navon", "Ethan Fetaya", "Gal Chechik", "Personalized", "Canh T Dinh", "Nguyen Tran", "Tuan Dung Nguyen", "Wang", "Kangkang Wang", "Rajiv Mathews", "Chlo", "Kiddon", "Hubert Eichner", "Beaufays", "Xiao", "Han Xiao", "Kashif Rasul", "Zhang", "Michael Zhang", "Karan Sapra", "Sanja Fidler", "Serena Yeung", "Jose M Alvarez", "Xinwei Zhang", "Mingyi Hong", "Sairaj Dhople", "Wotao Yin", "Yang Liu", "Fedpd", "Zhao", "Yue Zhao", "Meng Li", "Liangzhen Lai", "Naveen Suda", "Damon Civin", "Vikas Chandra"]}{"title": ["Unsupervised Learning on 3D Point Clouds by Clustering and Contrasting"], "authors": ["[arxiv.Result.Author('Guofeng Mei'), arxiv.Result.Author('Litao Yu'), arxiv.Result.Author('Qiang Wu'), arxiv.Result.Author('Jian Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02543v1"], "summary": "Learning from unlabeled or partially labeled data to alleviate human labeling\nremains a challenging research topic in 3D modeling. Along this line,\nunsupervised representation learning is a promising direction to auto-extract\nfeatures without human intervention. This paper proposes a general unsupervised\napproach, named \\textbf{ConClu}, to perform the learning of point-wise and\nglobal features by jointly leveraging point-level clustering and instance-level\ncontrasting. Specifically, for one thing, we design an Expectation-Maximization\n(EM) like soft clustering algorithm that provides local supervision to extract\ndiscriminating local features based on optimal transport. We show that this\ncriterion extends standard cross-entropy minimization to an optimal transport\nproblem, which we solve efficiently using a fast variant of the Sinkhorn-Knopp\nalgorithm. For another, we provide an instance-level contrasting method to\nlearn the global geometry, which is formulated by maximizing the similarity\nbetween two augmentations of one point cloud. Experimental evaluations on\ndownstream applications such as 3D object classification and semantic\nsegmentation demonstrate the effectiveness of our framework and show that it\ncan outperform state-of-the-art techniques.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Science Facing Interoperability as a Necessary Condition of Success and Evil"], "authors": ["[arxiv.Result.Author('Remy Demichelis')]"], "link": ["http://arxiv.org/pdf/2202.02540v1"], "summary": "Artificial intelligence (AI) systems, such as machine learning algorithms,\nhave allowed scientists, marketers and governments to shed light on\ncorrelations that remained invisible until now. Beforehand, the dots that we\nhad to connect in order to imagine a new knowledge were either too numerous,\ntoo sparse or not even detected. Sometimes, the information was not stored in\nthe same data lake or format and was not able to communicate. But in creating\nnew bridges with AI, many problems appeared such as bias reproduction, unfair\ninferences or mass surveillance. Our aim is to show that, on one hand, the AI's\ndeep ethical problem lays essentially in these new connections made possible by\nsystems interoperability. In connecting the spheres of our life, these systems\nundermine the notion of justice particular to each of them, because the new\ninteractions create dominances of social goods from a sphere to another. These\nsystems make therefore spheres permeable to one another and, in doing so, they\nopen to progress as well as to tyranny. On another hand, however, we would like\nto emphasize that the act to connect what used to seem a priori disjoint is a\nnecessary move of knowledge and scientific progress.", "entities_include_in_text": ["Botsman,  2017", "Dastin, \n2018", "Angwin et al., 2016"], "entities_from_reference": ["Plusieurs"]}{"title": ["LyaNet: A Lyapunov Framework for Training Neural ODEs"], "authors": ["[arxiv.Result.Author('Ivan Dario Jimenez Rodriguez'), arxiv.Result.Author('Aaron D. Ames'), arxiv.Result.Author('Yisong Yue')]"], "link": ["http://arxiv.org/pdf/2202.02526v1"], "summary": "We propose a method for training ordinary differential equations by using a\ncontrol-theoretic Lyapunov condition for stability. Our approach, called\nLyaNet, is based on a novel Lyapunov loss formulation that encourages the\ninference dynamics to converge quickly to the correct prediction.\nTheoretically, we show that minimizing Lyapunov loss guarantees exponential\nconvergence to the correct solution and enables a novel robustness guarantee.\nWe also provide practical algorithms, including one that avoids the cost of\nbackpropagating through a solver or using the adjoint method. Relative to\nstandard Neural ODE training, we empirically find that LyaNet can offer\nimproved prediction performance, faster convergence of inference dynamics, and\nimproved adversarial robustness. Our code available at\nhttps://github.com/ivandariojr/LyapunovLearning .", "entities_include_in_text": ["Chen et al., 2019", "Chen et al., 2019; Rozen et al.,\n2021; Song et al., 2020", "Chen et al., 2019; Kidger et al., 2021", "Khalil, 2002; Ames et al., 2014", "Dupont et al., 2019", "Massaroli et al., 2020", "Chen et al., 2019; E, 2017", "Taylor et al., 2019", "Taylor et al., 2019)", "Ames et al. (2014))", "Ames et al., 2014", "Queiruga et al., 2020", "He et al., 2015", "Zhang et al., 2009", "Chen et al., 2019; Antonelo et al., 2021). The\nproposal by E (2017", "Ames et al., 2016", "Liu et al., 2021"], "entities_from_reference": ["Grizzle", "J. W. Rapidly", "J. W.", "Amos", "Rodriguez", "Sacks", "Boots", "Kolter", "Antonelo", "Seman", "J. P.", "Hubner", "J. F.", "Bai", "Koltun", "Dean", "Matni", "Recht", "Dupont", "Doucet", "Teh", "Inverse Problems", "Zhang", "Ren", "Deep", "Khalil", "Patience Hall", "Chang", "Roohi", "Gao", "Kidger", "Chen", "Rubanova", "Duvenaud", "Nickel", "Fazlyab", "Pappas", "Cheng", "Orosz", "Chaudhuri", "Yue", "Burdick", "Control", "Machine Learning", "Chow", "Cohen", "Rosenfeld", "Kim", "Liu", "Bernstein", "Meister", "Li", "Beyond", "Manek", "Massaroli", "Poli", "Yamashita", "Asama", "Muller", "Peng", "Lee", "Queiruga", "Erichson", "Mahoney", "Raghunathan", "Steinhardt", "Liang", "Wilson", "Wong", "Vitus", "Automatica", "Richards", "Berkenkamp", "Robot Learning", "Robey", "Chamon", "Hassani", "Rosolia", "Borrelli", "Rozen", "Lipman", "Ruthotto", "Schlaginhaufen", "Wenk", "Dorfler", "Song", "Kumar", "Ermon", "Poole", "Sontag", "Wang", "Systems", "Taylor", "Dorobantu", "Krishnamoorthy", "Le", "Tsukamoto", "Chung", "Slotine", "Williams", "Wagener", "Drews", "Rehg", "J. M.", "Proofs", "Vy Vy", "Global Uniform Lipschitz", "Lyapunov Exponential Stability", "Correct Classification", "Monte Carlo Method", "Sample", "Titan RTX"]}{"title": ["MarkovGNN: Graph Neural Networks on Markov Diffusion"], "authors": ["[arxiv.Result.Author('Md. Khaledur Rahman'), arxiv.Result.Author('Abhigya Agrawal'), arxiv.Result.Author('Ariful Azad')]"], "link": ["http://arxiv.org/pdf/2202.02470v1"], "summary": "Most real-world networks contain well-defined community structures where\nnodes are densely connected internally within communities. To learn from these\nnetworks, we develop MarkovGNN that captures the formation and evolution of\ncommunities directly in different convolutional layers. Unlike most Graph\nNeural Networks (GNNs) that consider a static graph at every layer, MarkovGNN\ngenerates different stochastic matrices using a Markov process and then uses\nthese community-capturing matrices in different layers. MarkovGNN is a general\napproach that could be used with most existing GNNs. We experimentally show\nthat MarkovGNN outperforms other GNNs for clustering, node classification, and\nvisualization tasks. The source code of MarkovGNN is publicly available at\n\\url{https://github.com/HipGraph/MarkovGNN}.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Transfer Reinforcement Learning for Differing Action Spaces via Q-Network Representations"], "authors": ["[arxiv.Result.Author('Nathan Beck'), arxiv.Result.Author('Abhiramon Rajasekharan'), arxiv.Result.Author('Trung Hieu Tran')]"], "link": ["http://arxiv.org/pdf/2202.02442v1"], "summary": "Transfer learning approaches in reinforcement learning aim to assist agents\nin learning their target domains by leveraging the knowledge learned from other\nagents that have been trained on similar source domains. For example, recent\nresearch focus within this space has been placed on knowledge transfer between\ntasks that have different transition dynamics and reward functions; however,\nlittle focus has been placed on knowledge transfer between tasks that have\ndifferent action spaces. In this paper, we approach the task of transfer\nlearning between domains that differ in action spaces. We present a reward\nshaping method based on source embedding similarity that is applicable to\ndomains with both discrete and continuous action spaces. The efficacy of our\napproach is evaluated on transfer to restricted action spaces in the Acrobot-v1\nand Pendulum-v0 domains (Brockman et al. 2016). A comparison with two baselines\nshows that our method does not outperform these baselines in these continuous\naction spaces but does show an improvement in these discrete action spaces. We\nconclude our analysis with future directions for this work.", "entities_include_in_text": ["Brockman et al. 2016", "Zhu, Lin,\nand Zhou 2020", "Zhu, Lin, and Zhou 2020", "Zhang, Satija, and Pineau 2018", "Petangoda et al. 2019", "Brockman et al. 2016", "Barreto et al. 2016", "Petangoda et al. 2019", "Zhang, Satija, and\nPineau 2018", "Petangoda et al.\n2019", "Zhang, Satija, and Pineau 2018", "Ng, Harada, and\nRussell 1999", "Wiewiora, Cottrell, and\nElkan 2003", "Ng,\nHarada, and Russell 1999", "Brys et al.\n2015", "Brys\net al. 2015", "Wiewiora, Cottrell, and Elkan 2003", "Zhu, Lin, and\nZhou 2020", "Konda and Tsitsiklis 1999", "Zhu, Lin, and Zhou\n2020; Ng, Harada, and Russell 1999", "Wiewiora, Cot-\ntrell, and Elkan 2003", "Brockman et al. 2016", "Geramifard et al. 2015", "Geramifard et al. 2015", "Konda and Tsitsik-\nlis 1999", "Wiewiora, Cottrell, and\nElkan 2003", "Zhu, Lin, and Zhou 2020", "Lillicrap et al. 2015", "Fuji-\nmoto, Hoof, and Meger 2018", "Mnih et al. 2013", "Brockman et al.\n2016", "Brockman\net al. 2016; Geramifard et al. 2015", "Brockman et al. 2016", "Lillicrap et al. 2015", "Fujimoto, Hoof,\nand Meger 2018", "Mnih et al. 2013", "Mnih et al. 2013", "Lil-\nlicrap et al. 2015", "Fujimoto, Hoof, and Meger\n2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation"], "authors": ["[arxiv.Result.Author('Ziad Al-Halah'), arxiv.Result.Author('Santhosh K. Ramakrishnan'), arxiv.Result.Author('Kristen Grauman')]"], "link": ["http://arxiv.org/pdf/2202.02440v1"], "summary": "In reinforcement learning for visual navigation, it is common to develop a\nmodel for each new task, and train that model from scratch with task-specific\ninteractions in 3D environments. However, this process is expensive; massive\namounts of interactions are needed for the model to generalize well. Moreover,\nthis process is repeated whenever there is a change in the task type or the\ngoal modality. We present a unified approach to visual navigation using a novel\nmodular transfer learning model. Our model can effectively leverage its\nexperience from one source task and apply it to multiple target tasks (e.g.,\nObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,\naudio, label). Furthermore, our model enables zero-shot experience learning,\nwhereby it can solve the target tasks without receiving any task-specific\ninteractive training. Our experiments on multiple photorealistic datasets and\nchallenging tasks show that our approach learns faster, generalizes better, and\noutperforms SoTA models by a significant margin.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Handling Distribution Shifts on Graphs: An Invariance Perspective"], "authors": ["[arxiv.Result.Author('Qitian Wu'), arxiv.Result.Author('Hengrui Zhang'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('David Wipf')]"], "link": ["http://arxiv.org/pdf/2202.02466v1"], "summary": "There is increasing evidence suggesting neural networks' sensitivity to\ndistribution shifts, so that research on out-of-distribution (OOD)\ngeneralization comes into the spotlight. Nonetheless, current endeavors mostly\nfocus on Euclidean data, and its formulation for graph-structured data is not\nclear and remains under-explored, given the two-fold fundamental challenges: 1)\nthe inter-connection among nodes in one graph, which induces non-IID generation\nof data points even under the same environment, and 2) the structural\ninformation in the input graph, which is also informative for prediction. In\nthis paper, we formulate the OOD problem for node-level prediction on graphs\nand develop a new domain-invariant learning approach, named\nExplore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage\ninvariant graph features for prediction. The key difference to existing\ninvariant models is that we design multiple context explorers (specified as\ngraph editers in our case) that are adversarially trained to maximize the\nvariance of risks from multiple virtual environments. Such a design enables the\nmodel to extrapolate from a single observed environment which is the common\ncase for node-level prediction. We prove the validity of our method by\ntheoretically showing its guarantee of a valid OOD solution and further\ndemonstrate its power on various real-world datasets for handling distribution\nshifts from artificial spurious features, cross-domain transfers and dynamic\ngraph evolution.", "entities_include_in_text": ["Mansour et al., 2009; Blanchard et al., 2011; Muandet et al., 2013;\nGong et al., 2016) occupies a central role in the ML community. Yet, recent evidence suggests that\ndeep neural networks can be sensitive to distribution shifts, exhibiting unsatisfactory performance\nwithin new environments, e.g., Beery et al. (2018); Su et al. (2019); Recht et al. (2019); Mancini\net al. (2020", "DeGrave et al., 2020).\n\nRecent studies of the OOD generalization problem like Rojas-Carulla et al. (2018", "Arjovsky et al., 2019", "Fakhraei et al., 2015", "Pareja et al., 2020", "AlBadawy et al., 2018", "Berk et al., 2018", "Rojas-Carulla et al., 2018; Arjovsky et al., 2019", "Krueger et al., 2021", "Xu et al., 2019; Jin et al., 2020", "Federici et al.,\n2021", "Wu\net al., 2019", "Velickovic et al., 2018", "Pareja et al., 2020", "Hamilton et al., 2017", "Chien\net al., 2021", "Pareja et al., 2020", "Xu et al., 2021", "Yehudai et al., 2021;\nBevilacqua et al., 2021", "Koh et al., 2021", "Arjovsky et al., 2019", "Sagawa et al., 2019", "Chang et al., 2020", "Ahuja et al.,\n2020", "Zhang et al., 2021), etc. Several works attempt to resolve extended\nsettings. For instance, Ahmed et al. (2021", "Mahajan et al., 2021) also leverages\na matching-based algorithm that resorts to shared representations of cross-domain inputs from the\nsame object. Also, Creager et al. (2021) and Liu et al. (2021", "Rosenfeld et al., 2021; Nagarajan et al., 2021; Kamath et al.,\n2021", "Federici et al., 2021", "Chen et al., 2021", "Zheng et al., 2020; Hasanzadeh et al., 2020", "Baranwal et al., 2021", "Federici\net al., 2021", "within 1950-2011", "within 2011-2014"], "entities_from_reference": ["Plusieurs"]}{"title": ["Neural Logic Analogy Learning"], "authors": ["[arxiv.Result.Author('Yujia Fan'), arxiv.Result.Author('Yongfeng Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02436v1"], "summary": "Letter-string analogy is an important analogy learning task which seems to be\neasy for humans but very challenging for machines. The main idea behind current\napproaches to solving letter-string analogies is to design heuristic rules for\nextracting analogy structures and constructing analogy mappings. However, one\nkey problem is that it is difficult to build a comprehensive and exhaustive set\nof analogy structures which can fully describe the subtlety of analogies. This\nproblem makes current approaches unable to handle complicated letter-string\nanalogy problems. In this paper, we propose Neural logic analogy learning\n(Noan), which is a dynamic neural architecture driven by differentiable logic\nreasoning to solve analogy problems. Each analogy problem is converted into\nlogical expressions consisting of logical variables and basic logical\noperations (AND, OR, and NOT). More specifically, Noan learns the logical\nvariables as vector embeddings and learns each logical operation as a neural\nmodule. In this way, the model builds computational graph integrating neural\nnetwork with logical reasoning to capture the internal logical structure of the\ninput letter strings. The analogy learning problem then becomes a True/False\nevaluation problem of the logical expressions. Experiments show that our\nmachine learning-based Noan approach outperforms state-of-the-art approaches on\nstandard letter-string analogy benchmark datasets.", "entities_include_in_text": ["NeurIPS 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["SMODICE: Versatile Offline Imitation Learning via State Occupancy Matching"], "authors": ["[arxiv.Result.Author('Yecheng Jason Ma'), arxiv.Result.Author('Andrew Shen'), arxiv.Result.Author('Dinesh Jayaraman'), arxiv.Result.Author('Osbert Bastani')]"], "link": ["http://arxiv.org/pdf/2202.02433v1"], "summary": "We propose State Matching Offline DIstribution Correction Estimation\n(SMODICE), a novel and versatile algorithm for offline imitation learning (IL)\nvia state-occupancy matching. We show that the SMODICE objective admits a\nsimple optimization procedure through an application of Fenchel duality and an\nanalytic solution in tabular MDPs. Without requiring access to expert actions,\nSMODICE can be effectively applied to three offline IL settings: (i) imitation\nfrom observations (IfO), (ii) IfO with dynamics or morphologically mismatched\nexpert, and (iii) example-based reinforcement learning, which we show can be\nformulated as a state-occupancy matching problem. We extensively evaluate\nSMODICE on both gridworld environments as well as on high-dimensional offline\nbenchmarks. Our results demonstrate that SMODICE is effective for all three\nproblem settings and significantly outperforms prior state-of-art.", "entities_include_in_text": ["Lange\net al., 2012; Levine et al., 2020", "Zolna et al., 2020; Chang et al., 2021; Anony-\nmous, 2022", "Ross et al., 2011", "Eysenbach et al., 2021", "Torabi et al.,\n2018; 2019; Liu et al., 2019; Radosavovic et al., 2020; Ey-\nsenbach et al., 2021", "Ey-\nsenbach et al., 2021", "Kumar et al., 2019; Lee et al., 2021;\nAnonymous, 2022", "Levine et al., 2020", "Puterman,\n2014", "Dai et al., 2016", "Zhu et al., 2020; Lee et al., 2021; Rudner et al., 2021", "Puterman, 2014", "Lee et al.,\n2021", "Lee et al.,\n2021; Anonymous, 2022", "Eysenbach et al., 2021", "Anonymous, 2022", "Ghasemipour et al., 2019;\nKe et al., 2020; Zhu et al., 2020", "Zolna et al., 2020", "Chang et al.,\n2021", "Torabi et al., 2018;\n2019; Liu et al., 2019; Radosavovic et al., 2020", "Kim et al., 2020; Raychaudhuri et al., 2021", "Liu et al., 2019; Radosavovic\net al., 2020", "Fu et al., 2021", "Liu et al., 2019", "Zolna et al.,\n2020", "Anonymous, 2022", "Haarnoja et al., 2018", "Eysenbach et al., 2021", "Zolna et al., 2020", "Yu et al., 2020; Kidambi et al., 2020", "Yang et al., 2019", "Kostrikov et al., 2020; Zhu et al., 2020", "Anonymous, 2022", "Boyd et al., 2004", "Harris et al., 2020", "Baird, 1995", "Haarnoja et al., 2018"], "entities_from_reference": ["Baird", "Elsevier", "Boyd", "Cambridge", "Chang", "J. D.", "Sreenivas", "Kidambi", "Dai", "Boots", "Chow", "Li", "Eysenbach", "Salakhutdinov", "Kumar", "Fujimoto", "Zemel", "Goodfellow", "Mirza", "Ozair", "Bengio", "Gupta", "Hausman", "Haarnoja", "Zhou", "Mismatched Experts", "Examples Harris", "Millman", "Walt", "Berg", "Smith", "Picus", "Kerkwijk", "Brett", "Haldane", "J. F.", "Wiebe", "Peterson", "Sheppard", "Reddy", "Weckesser", "Nature", "Ermon", "Choudhury", "Barnes", "Lee", "Netrapalli", "Kim", "Zhao", "Machine Learning", "Kingma", "J. Adam", "Kostrikov", "Agrawal", "Dwibedi", "Tompson", "J.", "J. ImitaIn", "Lange", "Gabel", "Riedmiller", "Springer", "Jeon", "Pineau", "Levine", "J. Offline", "Liu", "Jayaraman", "Bastani", "Nachum", "Puterman", "Markov", "John Wiley", "Sons", "Radosavovic", "Wang", "J. Stateonly", "Raychaudhuri", "Paul", "Baar", "Ross", "Gordon", "Rudner", "Osborne", "Gal", "Teh", "Dauphin", "Liang", "Vaughan", "J. W", "Torabi", "Warnell", "Stone", "Examples Yang", "Huang", "Gan", "Thomas", "Zou", "Finn", "Zhang", "Zhu", "Lin", "Zolna", "Novikov", "Aytar", "Denil", "Proofs", "Technical Lemmas", "Examples", "Esd", "Es0", "Offline Imitation", "Fenchel", "Extended Related Work", "Fenchel Duality", "Hence", "T V", "Tabular MDPs", "V V", "Harris", "Offline IL", "Deep Neural Networks", "Remark", "Hyperparameter Value Critic", "Discount", "Actor Mean Clipping", "Appendix G", "Baselines", "Implementation Details", "Observations Experimental Details", "Ant", "Examples Figure", "Diverse AntMaze", "Zero Reward", "Expert Experimental Details", "Examples Table", "Target", "Adam", "Critic", "Offline Dataset Compositions", "Expert Data Size Random Data Size Task Hopper", "Kitchen State Dim", "Expert Dataset", "Examples Experimental Details", "Task", "Ant Algorithm", "Offline", "Microwave Figure", "Kettle", "Microwave", "Examples Algorithm", "V V V", "Expert", "Train Expert", "Train Lagrangian Value", "Policy", "Sample", "Behavior Cloning Update"]}{"title": ["HENRI: High Efficiency Negotiation-based Robust Interface for Multi-party Multi-issue Negotiation over the Internet"], "authors": ["[arxiv.Result.Author('Saurabh Deochake'), arxiv.Result.Author('Shashank Kanth'), arxiv.Result.Author('Subhadip Chakraborty'), arxiv.Result.Author('Suresh Sarode'), arxiv.Result.Author('Vidyasagar Potdar'), arxiv.Result.Author('Debajyoti Mukhopadhyay')]"], "link": ["http://arxiv.org/pdf/2202.02430v1"], "summary": "This paper proposes a framework for a full fledged negotiation system that\nallows multi party multi issue negotiation. It focuses on the negotiation\nprotocol to be observed and provides a platform for concurrent and independent\nnegotiation on individual issues using the concept of multi threading. It\ndepicts the architecture of an agent detailing its components. The paper sets\nforth a hierarchical pattern for the multiple issues concerning every party.\nThe system also provides enhancements such as the time-to-live counters for\nevery advertisement, refinement of utility considering non-functional\nattributes, prioritization of issues, by assigning weights to issues.", "entities_include_in_text": ["DEST 2009", "ICCSA \n2007"], "entities_from_reference": ["Faratin", "Group Decision", "Patrick Wendy Powley", "Web Service", "Nicholas R. Jennings", "Lau", "Electronic Commerce Research", "Rogier Brussee1", "Eck", "Alliances", "Jin Baek Kim", "Min Gi", "Dynamic", "Jamal Bentahar", "Philippe", "Data Engineering", "Privacy Negotiations", "Seventh", "User", "Istanbul", "Chang", "Revenue Models", "Current Generation Social Softwares Systems", "Kuala Lumpur", "Melbourne"]}{"title": ["Distributed Learning With Sparsified Gradient Differences"], "authors": ["[arxiv.Result.Author('Yicheng Chen'), arxiv.Result.Author('Rick S. Blum'), arxiv.Result.Author('Martin Takac'), arxiv.Result.Author('Brian M. Sadler')]"], "link": ["http://arxiv.org/pdf/2202.02491v1"], "summary": "A very large number of communications are typically required to solve\ndistributed learning tasks, and this critically limits scalability and\nconvergence speed in wireless communications applications. In this paper, we\ndevise a Gradient Descent method with Sparsification and Error Correction\n(GD-SEC) to improve the communications efficiency in a general worker-server\narchitecture. Motivated by a variety of wireless communications learning\nscenarios, GD-SEC reduces the number of bits per communication from worker to\nserver with no degradation in the order of the convergence rate. This enables\nlarger-scale model learning without sacrificing convergence or accuracy. At\neach iteration of GD-SEC, instead of directly transmitting the entire gradient\nvector, each worker computes the difference between its current gradient and a\nlinear combination of its previously transmitted gradients, and then transmits\nthe sparsified gradient difference to the server. A key feature of GD-SEC is\nthat any given component of the gradient difference vector will not be\ntransmitted if its magnitude is not sufficiently large. An error correction\ntechnique is used at each worker to compensate for the error resulting from\nsparsification. We prove that GD-SEC is guaranteed to converge for strongly\nconvex, convex, and nonconvex optimization problems with the same order of\nconvergence rate as GD. Furthermore, if the objective function is strongly\nconvex, GD-SEC has a fast linear convergence rate. Numerical results not only\nvalidate the convergence rate of GD-SEC but also explore the communication bit\nsavings it provides. Given a target accuracy, GD-SEC can significantly reduce\nthe communications load compared to the best existing algorithms without\nslowing down the optimization process.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["Symmetric Volume Maps"], "authors": ["[arxiv.Result.Author('S. Mazdak Abulnaga'), arxiv.Result.Author('Oded Stein'), arxiv.Result.Author('Polina Golland'), arxiv.Result.Author('Justin Solomon')]"], "link": ["http://arxiv.org/pdf/2202.02568v1"], "summary": "Although shape correspondence is a central problem in geometry processing,\nmost methods for this task apply only to two-dimensional surfaces. The\nneglected task of volumetric correspondence--a natural extension relevant to\nshapes extracted from simulation, medical imaging, volume rendering, and even\nimproving surface maps of boundary representations--presents unique challenges\nthat do not appear in the two-dimensional case. In this work, we propose a\nmethod for mapping between volumes represented as tetrahedral meshes. Our\nformulation minimizes a distortion energy designed to extract maps\nsymmetrically, i.e., without dependence on the ordering of the source and\ntarget domains. We accompany our method with theoretical discussion describing\nthe consequences of this symmetry assumption, leading us to select a\nsymmetrized ARAP energy that favors isometric correspondences. Our final\nformulation optimizes for near-isometry while matching the boundary. We\ndemonstrate our method on a diverse geometric dataset, producing low-distortion\nmatchings that align to the boundary.", "entities_include_in_text": ["Ovsjanikov et al. 2010", "Lipman and\nFunkhouser 2009; Schmidt et al. 2019", "Ezuz\net al. 2019", "Floater and Hormann 2005; Fu et al. 2021; Sheffer et al. 2007", "Irving et al. 2004", "Gain and\nBechmann 2008; Selim and Koomullil 2016; Sieger et al. 2015", "Smith and Schaefer 2015] for a representative\nexample. In contrast to these past works, we produce maps between\nfar-from-isometric domains without an obvious effective initializa-\ntion. Consequently, our choice of energies is designed to be resilient\nto poor initial maps that are not foldover-free.\n\nVolumetric mappings. Some methods consider the task of comput-\ning correspondences between volumetric shapes. To our knowledge,\nall past methods can be understood as special cases of the deforma-\ntion methods where the task is to extend a fixed boundary map to\nthe interior of a volume.\n\nKovalsky et al. [2015", "Ezuz et al. 2019; Schmidt et al. 2019;\nSchreiner et al. 2004", "Schreiner et al. 2004; Smith\nand Schaefer 2015", "Shtengel\net al. 2017", "Kraevoy and Sheffer 2004", "Gots-\nman et al. 2003; Haker et al. 2000; Lee and Kazhdan 2019", "Aigerman and Lipman 2015, 2016; Aigerman\net al. 2014, 2015; Bright et al. 2017; Schmidt et al. 2019", "Kim et al. 2011; Lipman and Funkhouser 2009", "Aigerman et al. 2015", "Jain\net al. 2007; Mateus et al. 2008; Ovsjanikov et al. 2010; Vestner et al.\n2017", "Ankerst et al. 1999; Salti et al. 2014", "Dubrovina and Kimmel 2011; Kim et al. 2011; Litman and\nBronstein 2013", "Ovsjanikov et al. 2012,\n2016", "Ezuz et al. 2019;\nMandad et al. 2017; Schreiner et al. 2004; Solomon et al. 2012, 2016].\nEzuz et al. [2019", "Klein\net al. 2007", "Avants et al. 2008", "Beg et al. 2005", "Oliveira\nand Tavares 2014; Sotiras et al. 2013; Viergever et al. 2016", "Rabinovich et al. 2017", "Liu et al. 2008", "Schreiner et al. 2004", "Ezuz et al. 2019; Schmidt et al. 2019; Schreiner et al.\n2004], one simple way to achieve symmetry is to optimize the av-\nerage of the distortion energy of a map with the distortion energy\nof its inverse. Ezuz et al. [2019] and Schreiner et al. [2004", "Rabinovich et al. 2017; Smith and Schaefer\n2015", "Ezuz et al. 2019; Schreiner et al. 2004", "Smith and Schaefer 2015", "Stein\net al. 2021", "Ezuz et al. 2019; Schmidt\net al. 2019; Schreiner et al. 2004", "Geman and Yang 1995", "Zhu et al. 1997", "Hu et al. 2020", "Li et al.\n2021", "Li\net al. 2021", "Dyke et al. 2019", "Fu et al. 2016; Li et al.\n2021", "Zhou and Jacobson 2016", "Japan 2022", "Ezuz et al. 2019", "Ezuz et al. 2019", "Palmer et al. 2020", "Abulnaga et al.\n2021", "Benirschke and Driscoll 1967", "Bracci et al. 2019"], "entities_from_reference": ["Plusieurs"]}{"title": ["Transformers and the representation of biomedical background knowledge"], "authors": ["[arxiv.Result.Author('Oskar Wysocki'), arxiv.Result.Author('Zili Zhou'), arxiv.Result.Author(\"Paul O'Regan\"), arxiv.Result.Author('Deborah Ferreira'), arxiv.Result.Author('Magdalena Wysocka'), arxiv.Result.Author('D\u00f3nal Landers'), arxiv.Result.Author('Andr\u00e9 Freitas')]"], "link": ["http://arxiv.org/pdf/2202.02432v1"], "summary": "BioBERT and BioMegatron are Transformers models adapted for the biomedical\ndomain based on publicly available biomedical corpora. As such, they have the\npotential to encode large-scale biological knowledge. We investigate the\nencoding and representation of biological knowledge in these models, and its\npotential utility to support inference in cancer precision medicine - namely,\nthe interpretation of the clinical significance of genomic alterations. We\ncompare the performance of different transformer baselines; we use probing to\ndetermine the consistency of encodings for distinct entities; and we use\nclustering methods to compare and contrast the internal properties of the\nembeddings for genes, variants, drugs and diseases. We show that these models\ndo indeed encode biological knowledge, although some of this is lost in\nfine-tuning for specific tasks. Finally, we analyse how the models behave with\nregard to biases and imbalances in the dataset.", "entities_include_in_text": ["May 2021", "Aug. 2014", "Jan. 2017", "Aug. 2019", "Nov. 2016"], "entities_from_reference": ["Erica K. Barnell", "Florian Borchert", "Debyani Chakravarty", "Dvir Dahary", "Standard", "Genome Medicine", "Arpad M. Danos", "Human Mutation", "Rodrigo Dienstmann", "Cancer Drugs", "Clinical Targetability", "Deborah Ferreira", "Does My", "System Demonstrations", "Online", "Discriminatory Analysis", "Benjamin M Good", "Knowledge", "Enable Personalization", "Benjamin M. Good", "Malachi Griffith", "English", "Nature", "John Hewitt", "Christopher D Manning", "Human Language Technologies", "Short Papers", "Robin Jia", "Cliff Wong", "Hoifung Poon", "Multiscale", "Jinhyuk Lee", "Marilyn M. Li", "Molecular Pathology", "Clinical Oncology", "John Healy", "Density", "Data Mining Workshops", "Steve Astels", "Uniform Manifold Approximation", "Open Source Software", "Source Software", "Tiago Pimentel", "Damian T. Rieke", "Molecular Tumor Boards Worldwide", "Alexander Rives", "Jurica Seva", "Martin Wackerbauer", "Ulf Leser", "Key Sentences", "Melbourne", "Larger Biomedical Domain Language Model", "Ayush Singhal", "Michael Simmons", "Zhiyong Lu", "Text Mining", "Database Curation", "Alex H. Wagner", "Analysis Toolkit", "Hai Wang", "Deep Probabilistic Logic", "Similar", "Note", "Tables Table", "Pair", "Model Entity", "False", "Test", "Balanced", "G2032R", "Ruxolitinib", "Midostaurin", "Entrectinib", "Trametinib", "Sarcoma", "Larotrectinib", "Vemurafenib", "Sorafenib CD74-NRG1", "Afatinib", "Crizotinib Table", "Cluster", "H1047R", "G1049R", "H1047L ERBB3", "M2327I", "Melanoma Cancer Acute Myeloid Leukemia Colorectal Cancer Colorectal Cancer Clear Cell Sarcoma Vemurafenib", "Alectinib ALK", "G1202R Sorafenib", "Carcinoma Cetuximab", "Acute Myeloid Leukemia", "Carcinoma Erlotinib", "Selinexor Vemurafenib Regorafenib Gefitinib", "Figure S.1", "Figure S.2"]}{"title": ["The influence of labeling techniques in classifying human manipulation movement of different speed"], "authors": ["[arxiv.Result.Author('Sadique Adnan Siddiqui'), arxiv.Result.Author('Lisa Gutzeit'), arxiv.Result.Author('Frank Kirchner')]"], "link": ["http://arxiv.org/pdf/2202.02426v1"], "summary": "In this work, we investigate the influence of labeling methods on the\nclassification of human movements on data recorded using a marker-based motion\ncapture system. The dataset is labeled using two different approaches, one\nbased on video data of the movements, the other based on the movement\ntrajectories recorded using the motion capture system. The dataset is labeled\nusing two different approaches, one based on video data of the movements, the\nother based on the movement trajectories recorded using the motion capture\nsystem. The data was recorded from one participant performing a stacking\nscenario comprising simple arm movements at three different speeds (slow,\nnormal, fast). Machine learning algorithms that include k-Nearest Neighbor,\nRandom Forest, Extreme Gradient Boosting classifier, Convolutional Neural\nnetworks (CNN), Long Short-Term Memory networks (LSTM), and a combination of\nCNN-LSTM networks are compared on their performance in recognition of these arm\nmovements. The models were trained on actions performed on slow and normal\nspeed movements segments and generalized on actions consisting of fast-paced\nhuman movement. It was observed that all the models trained on normal-paced\ndata labeled using trajectories have almost 20% improvement in accuracy on test\ndata in comparison to the models trained on data labeled using videos of the\nperformed experiments.", "entities_include_in_text": ["Zhang et al.,\n2019", "Cruciani et al., 2018", "Gutzeit, 2021", "Gutzeit et al., 2014", "Wang and Schmid, 2013", "Donahue et al., 2015", "Ji et al., 2013", "Cruciani et al.,\n2018", "Sham-\nsipour et al., 2017", "Gutzeit, 2021", "Ho, 1995", "Chen and Guestrin, 2016", "Kim, 2014", "Is-\nmail Fawaz et al., 2019", "Hochreiter and Schmid-\nhuber, 1997", "Mutegeki and Han, 2020", "Biewald, 2020", "Gutzeit et al., 2014", "van der Maaten and Hinton, 2008", "Lundberg\nand Lee, 2017", "Ribeiro et al., 2016", "ICPRAM-2021", "KI-2019", "ICPR-2014"], "entities_from_reference": ["Plusieurs"]}{"title": ["OMLT: Optimization & Machine Learning Toolkit"], "authors": ["[arxiv.Result.Author('Francesco Ceccon'), arxiv.Result.Author('Jordan Jalving'), arxiv.Result.Author('Joshua Haddad'), arxiv.Result.Author('Alexander Thebelt'), arxiv.Result.Author('Calvin Tsay'), arxiv.Result.Author('Carl D. Laird'), arxiv.Result.Author('Ruth Misener')]"], "link": ["http://arxiv.org/pdf/2202.02414v1"], "summary": "The optimization and machine learning toolkit (OMLT) is an open-source\nsoftware package incorporating neural network and gradient-boosted tree\nsurrogate models, which have been trained using machine learning, into larger\noptimization problems. We discuss the advances in optimization technology that\nmade OMLT possible and show how OMLT seamlessly integrates with the algebraic\nmodeling language Pyomo. We demonstrate how to use OMLT for solving\ndecision-making problems in both computer science and engineering.", "entities_include_in_text": ["Volpp et al., 2019", "Botoeva et al., 2020", "Boukouvala et al., 2016", "Chollet et al., 2015", "Bynum et al., 2021", "Chollet et al., 2015", "Paszke et al., 2019", "Abadi et al.,\n2015", "Schweidtmann and Mitsos,\n2019", "Lueg\net al., 2021", "Maragno et al., 2021", "Kronqvist et al., 2021; Tsay et al.,\n2021", "LeCun et al., 2010", "Tjeng et al., 2018", "Croce and Hein, 2020", "Serra et al., 2020", "Thebelt\n\net al., 2021", "Lee et al., 2021", "Boukouvala et al.,\n2016; Wilson and Sahinidis, 2017; Boukouvala and Floudas, 2017; Boukouvala et al., 2017;\nHuster et al., 2020", "Bynum\net al., 2021", "Gay, 1997", "Fischetti and Jo, 2018; Raghunathan et al., 2018; Singh et al., 2019; Anderson\net al., 2020; Tjandraatmadja et al., 2020; Dathathri et al., 2020", "Thebelt et al., 2021", "Schweidtmann and Mitsos, 2019", "Anderson et al., 2020", "Yang et al., 2021", "Tsay et al., 2021"], "entities_from_reference": ["Mart", "Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Software", "Ross Anderson", "Joey Huchette", "Will Ma", "Christian Tjandraatmadja", "Juan Pablo Vielma", "Strong", "Mathematical Programming", "Elena Botoeva", "Panagiotis Kouvaros", "Jan Kronqvist", "Alessio Lomuscio", "Fani Boukouvala", "Christodoulos A Floudas", "Argonaut", "Ruth Misener", "Christodoulos A. Floudas", "Global", "European Journal", "Global Optimization", "Michael L Bynum", "Gabriel A Hackebeil", "William E Hart", "Carl D Laird", "Bethany L Nicholson", "John D Siirola", "Jean-Paul Watson", "David L Woodruff", "Fran", "Keras", "Francesco Croce", "Matthias Hein", "Minimally", "Machine Learning", "Sumanth Dathathri", "Krishnamurthy Dvijotham", "Alexey Kurakin", "Aditi Raghunathan", "Jonathan Uesato", "Rudy Bunel", "Shreya Shankar", "Jacob Steinhardt", "Ian Goodfellow", "Percy Liang", "Matteo Fischetti", "Jason Jo", "Deep", "David M Gay", "Wolfgang R Huster", "Artur M Schweidtmann", "Jannik T", "Alexander Mitsos", "Chemical Engineering", "Calvin Tsay", "Springer", "Yann LeCun", "Corinna Cortes", "Online", "Andrew Lee", "Jaffer H Ghouse", "John C Eslick", "Miguel A Zamarripa", "Dan Gunter", "John H Shinn", "Alexander W Dowling", "Debangsu Bhattacharyya", "Laurens Lueg", "Bjarne Grimstad", "Artur M. Schweidtmann", "Donato Maragno", "Holly Wiberg", "Dimitris Bertsimas", "S Ilker Birbil", "Dick", "Hertog", "Velibor V Misi", "Operations Research", "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga", "Machine Learning Toolkit Aditi Raghunathan", "Percy S Liang", "Thiago Serra", "Abhinav Kumar", "Srikumar Ramalingam", "Gagandeep Singh", "Rupanshu Ganvir", "Markus P", "Martin Vechev", "Systems", "Alexander Thebelt", "Miten Mistry", "Robert M Lee", "Nathan Sudermann-Merx", "Krunal Kishor Patel", "Vincent Tjeng", "Kai Xiao", "Russ Tedrake", "Michael Volpp", "Lukas P Fr", "Kirsten Fischer", "Andreas Doerr", "Stefan Falkner", "Frank Hutter", "Christian Daniel", "Zachary T Wilson", "Nikolaos V Sahinidis", "Dominic Yang", "Prasanna Balaprakash", "Sven Leyffer"]}{"title": ["Malleable Agents for Re-Configurable Robotic Manipulators"], "authors": ["[arxiv.Result.Author('Athindran Ramesh Kumar'), arxiv.Result.Author('Gurudutt Hosangadi')]"], "link": ["http://arxiv.org/pdf/2202.02395v1"], "summary": "Re-configurable robots potentially have more utility and flexibility for many\nreal-world tasks. Designing a learning agent to operate such robots requires\nadapting to different configurations. While deep reinforcement learning has had\nimmense success in robotic manipulation, domain adaptation is a significant\nproblem that limits its applicability to real-world robotics. We focus on\nrobotic arms with multiple rigid links connected by joints. Recent attempts\nhave performed domain adaptation and Sim2Real transfer to provide robustness to\nrobotic arm dynamics and sensor/camera variations. However, there have been no\nprevious attempts to adapt to robotic arms with a varying number of links. We\npropose an RL agent with sequence neural networks embedded in the deep neural\nnetwork to adapt to robotic arms that have a varying number of links. Further,\nwith the additional tool of domain randomization, this agent adapts to\ndifferent configurations with varying number/length of links and dynamics\nnoise. We perform simulations on a 2D N-link arm to show the ability of our\nnetwork to transfer and generalize efficiently.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning"], "authors": ["[arxiv.Result.Author('Ashwin Pathak'), arxiv.Result.Author('Raj Shah'), arxiv.Result.Author('Vaibhav Kumar'), arxiv.Result.Author('Yash Jakhotiya')]"], "link": ["http://arxiv.org/pdf/2202.02394v1"], "summary": "Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning .", "entities_include_in_text": ["Kant et al., 2018", "Constant et al., 2017", "Lin, 1999", "Baldwin and Villavicencio, 2002", "Baldwin et al., 2003", "Katz\nand Giesbrecht, 2006", "Kartsaklis et al.,\n2014", "Hashimoto and Tsu-\nruoka, 2016", "Hashempour and Villavicencio,\n2020", "Madabushi et al., 2021", "Madabushi et al., 2021", "Koch et al.,\n2015", "Sung et al., 2018", "Madabushi et al., 2021", "Madabushi et al., 2021"], "entities_from_reference": ["Timothy", "Colin Bannard", "Takaaki Tanaka", "Dominic Widdows", "Timothy Baldwin", "Aline Villavicencio", "Mathieu Constant", "Johanna Monti", "Plas", "Carlos Ramisch", "Michael Rosner", "Multiword", "Reyhaneh Hashempour", "Kazuma Hashimoto", "Yoshimasa Tsuruoka", "Neel Kant", "Raul Puri", "Nikolai Yakovenko", "Bryan Catanzaro", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Graham Katz", "Eugenie Giesbrecht", "Gregory Koch", "Richard Zemel", "Ruslan Salakhutdinov", "Machine Learning", "Dekang Lin", "Madabushi", "Harish Tayyar", "Edward", "Scarton", "Aline", "Dataset", "Flood Sung", "Yongxin Yang", "Li Zhang", "Tao Xiang", "Philip H. S. Torr", "Timothy M. Hospedales"]}{"title": ["Self-Adaptive Forecasting for Improved Deep Learning on Non-Stationary Time-Series"], "authors": ["[arxiv.Result.Author('Sercan O. Arik'), arxiv.Result.Author('Nathanael C. Yoder'), arxiv.Result.Author('Tomas Pfister')]"], "link": ["http://arxiv.org/pdf/2202.02403v1"], "summary": "Real-world time-series datasets often violate the assumptions of standard\nsupervised learning for forecasting -- their distributions evolve over time,\nrendering the conventional training and model selection procedures suboptimal.\nIn this paper, we propose a novel method, Self-Adaptive Forecasting (SAF), to\nmodify the training of time-series forecasting models to improve their\nperformance on forecasting tasks with such non-stationary time-series data. SAF\nintegrates a self-adaptation stage prior to forecasting based on `backcasting',\ni.e. predicting masked inputs backward in time. This is a form of test-time\ntraining that creates a self-supervised learning problem on test samples before\nperforming the prediction task. In this way, our method enables efficient\nadaptation of encoded representations to evolving distributions, leading to\nsuperior generalization. SAF can be integrated with any canonical\nencoder-decoder based time-series architecture such as recurrent neural\nnetworks or attention-based architectures. On synthetic and real-world datasets\nin domains where time-series data are known to be notoriously non-stationary,\nsuch as healthcare and finance, we demonstrate a significant benefit of SAF in\nimproving forecasting accuracy.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["The impact of feature importance methods on the interpretation of defect classifiers"], "authors": ["[arxiv.Result.Author('Gopi Krishnan Rajbahadur'), arxiv.Result.Author('Shaowei Wang'), arxiv.Result.Author('Yasutaka Kamei'), arxiv.Result.Author('Ahmed E. Hassan')]"], "link": ["http://arxiv.org/pdf/2202.02389v1"], "summary": "Classifier specific (CS) and classifier agnostic (CA) feature importance\nmethods are widely used (often interchangeably) by prior studies to derive\nfeature importance ranks from a defect classifier. However, different feature\nimportance methods are likely to compute different feature importance ranks\neven for the same dataset and classifier. Hence such interchangeable use of\nfeature importance methods can lead to conclusion instabilities unless there is\na strong agreement among different methods. Therefore, in this paper, we\nevaluate the agreement between the feature importance ranks associated with the\nstudied classifiers through a case study of 18 software projects and six\ncommonly used classifiers. We find that: 1) The computed feature importance\nranks by CA and CS methods do not always strongly agree with each other. 2) The\ncomputed feature importance ranks by the studied CA methods exhibit a strong\nagreement including the features reported at top-1 and top-3 ranks for a given\ndataset and classifier, while even the commonly used CS methods yield vastly\ndifferent feature importance ranks. Such findings raise concerns about the\nstability of conclusions across replicated studies. We further observe that the\ncommonly used defect datasets are rife with feature interactions and these\nfeature interactions impact the computed feature importance ranks of the CS\nmethods (not the CA methods). We demonstrate that removing these feature\ninteractions, even with simple methods like CFS improves agreement between the\ncomputed feature importance ranks of CA and CS methods. In light of our\nfindings, we provide guidelines for stakeholders and practitioners when\nperforming model interpretation and directions for future research, e.g.,\nfuture research is needed to investigate the impact of advanced feature\ninteraction removal methods on computed feature importance ranks of different\nCS methods.", "entities_include_in_text": ["MSR\n2010"], "entities_from_reference": ["Plusieurs"]}{"title": ["Model-Free Reinforcement Learning for Symbolic Automata-encoded Objectives"], "authors": ["[arxiv.Result.Author('Anand Balakrishnan'), arxiv.Result.Author('Stefan Jaksic'), arxiv.Result.Author('Edgar Aguilar Lozano'), arxiv.Result.Author('Dejan Nickovic'), arxiv.Result.Author('Jyotirmoy Deshmukh')]"], "link": ["http://arxiv.org/pdf/2202.02404v1"], "summary": "Reinforcement learning (RL) is a popular approach for robotic path planning\nin uncertain environments. However, the control policies trained for an RL\nagent crucially depend on user-defined, state-based reward functions. Poorly\ndesigned rewards can lead to policies that do get maximal rewards but fail to\nsatisfy desired task objectives or are unsafe. There are several examples of\nthe use of formal languages such as temporal logics and automata to specify\nhigh-level task specifications for robots (in lieu of Markovian rewards).\nRecent efforts have focused on inferring state-based rewards from formal\nspecifications; here, the goal is to provide (probabilistic) guarantees that\nthe policy learned using RL (with the inferred rewards) satisfies the\nhigh-level formal specification. A key drawback of several of these techniques\nis that the rewards that they infer are sparse: the agent receives positive\nrewards only upon completion of the task and no rewards otherwise. This\nnaturally leads to poor convergence properties and high variance during RL. In\nthis work, we propose using formal specifications in the form of symbolic\nautomata: these serve as a generalization of both bounded-time temporal\nlogic-based specifications as well as automata. Furthermore, our use of\nsymbolic automata allows us to define non-sparse potential-based rewards which\nempirically shape the reward surface, leading to better convergence during RL.\nWe also show that our potential-based rewarding strategy still allows us to\nobtain the policy that maximizes the satisfaction of the given specification.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["A Temporal-Difference Approach to Policy Gradient Estimation"], "authors": ["[arxiv.Result.Author('Samuele Tosatto'), arxiv.Result.Author('Andrew Patterson'), arxiv.Result.Author('Martha White'), arxiv.Result.Author('A. Rupam Mahmood')]"], "link": ["http://arxiv.org/pdf/2202.02396v1"], "summary": "The policy gradient theorem (Sutton et al., 2000) prescribes the usage of a\ncumulative discounted state distribution under the target policy to approximate\nthe gradient. Most algorithms based on this theorem, in practice, break this\nassumption, introducing a distribution shift that can cause the convergence to\npoor solutions. In this paper, we propose a new approach of reconstructing the\npolicy gradient from the start state without requiring a particular sampling\nstrategy. The policy gradient calculation in this form can be simplified in\nterms of a gradient critic, which can be recursively estimated due to a new\nBellman equation of gradients. By using temporal-difference updates of the\ngradient critic from an off-policy data stream, we develop the first estimator\nthat sidesteps the distribution shift issue in a model-free way. We prove that,\nunder certain realizability conditions, our estimator is unbiased regardless of\nthe sampling strategy. We empirically show that our technique achieves a\nsuperior bias-variance trade-off and performance in presence of off-policy\nsamples.", "entities_include_in_text": ["Sutton et al., 2000", "Sutton et al., 2000", "Mnih et al., 2015; Lillicrap\net al., 2016", "Levine et al., 2020", "Owen,\n2013). Many recent papers aim to lower the variance of pure\nimportance sampling correction. Liu et al. (2018) and Liu\net al. (2019) introduce the concept of state-wise importance\nsampling. Imani et al. (2018", "Nachum et al., 2019", "Degris et al., 2012", "Lillicrap et al., 2016", "Mnih et al., 2016", "Fujimoto et al., 2018", "Haarnoja et al., 2018", "Imani et al., 2018", "Imani et al., 2018; Liu et al., 2019", "Fujimoto\net al., 2019", "Degris et al., 2012", "Silver et al., 2014", "Haarnoja et al., 2018;\nHeess et al., 2015", "Shelton, 2001", "Imani et al., 2018; Liu et al., 2018; 2019", "Imani et al., 2018; Liu et al., 2019", "Ghiassian\net al., 2020", "Sutton, 1988", "Baird, 1995", "Sutton et al., 2008", "Sutton et al.,\n2009", "Ghiassian et al., 2020", "temporal-difference with regularized correction, Ghiassan\net al., (2020)", "Kolter, 2011", "Kolter,\n2011", "Lu et al., 2018", "Shelton, 2001", "Imani et al., 2018", "Degris et al., 2012", "Graves et al., 2021", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011", "Kolter, 2011"], "entities_from_reference": ["Function Approximation", "Machine Learning Proceedings", "Degris", "Machine Learning", "Omnipress", "Fujimoto", "Hoof", "Meger", "Machine Learning Research", "Ghiassian", "Patterson", "Garg", "Gupta", "Regularized Corrections", "Imani", "Haarnoja", "Zhou", "Maximum Entropy Deep", "Heess", "Wayne", "Erez", "Stochastic Value Gradients", "Policy Gradient Theorem Using Emphatic Weightings", "Kingma", "J. ADAM", "Kolter", "Lillicrap", "J. J.", "Deep Reinforcement Learning", "Liu", "Li", "Tang", "Agarwal", "Brunskill", "Boutilier", "Mnih", "Riedmiller", "Ostrovski", "Petersen", "Beattie", "Sadik", "Antonoglou", "Kumaran", "Legg", "Nature", "Mirza", "Harley", "Nachum", "Dai", "Kostrikov", "Chow", "Nota", "Thomas", "Owen", "Monte Carlo Theory", "Examples", "Lagoudakis", "Parr", "Levine", "Kumar", "J. Offline", "Peshkin", "Scarce Experience", "Policy Gradient", "Shelton", "Morgan Kaufmann Publishers", "Silver", "Sutton", "Barto", "Singh", "Policy Gradient Methods", "Linear Function Approximation", "Annual International Conference", "Tsitsiklis", "J. N.", "Van Roy", "Sample", "Bellman", "Least Squares", "Markov", "Rnf", "Least Squares Solutions", "Rnmnf", "Pairwise", "Pi", "Aibi", "Discount", "Perfect Features Proof", "Automatic", "Eligibility Trace", "Hence", "Policy Gradient Estimation", "Imanis MDP", "Gradient Estimation", "Adam", "Entropic Regularization", "Random"]}{"title": ["Using Large-scale Heterogeneous Graph Representation Learning for Code Review Recommendations"], "authors": ["[arxiv.Result.Author('Jiyang Zhang'), arxiv.Result.Author('Chandra Maddila'), arxiv.Result.Author('Ram Bairi'), arxiv.Result.Author('Christian Bird'), arxiv.Result.Author('Ujjwal Raizada'), arxiv.Result.Author('Apoorva Agrawal'), arxiv.Result.Author('Yamini Jhawar'), arxiv.Result.Author('Kim Herzig'), arxiv.Result.Author('Arie van Deursen')]"], "link": ["http://arxiv.org/pdf/2202.02385v1"], "summary": "Code review is an integral part of any mature software development process,\nand identifying the best reviewer for a code change is a well accepted problem\nwithin the software engineering community. Selecting a reviewer who lacks\nexpertise and understanding can slow development or result in more defects. To\ndate, most reviewer recommendation systems rely primarily on historical file\nchange and review information; those who changed or reviewed a file in the past\nare the best positioned to review in the future. We posit that while these\napproaches are able to identify and suggest qualified reviewers, they may be\nblind to reviewers who have the needed expertise and have simply never\ninteracted with the changed files before. To address this, we present CORAL, a\nnovel approach to reviewer recommendation that leverages a socio-technical\ngraph built from the rich set of entities (developers, repositories, files,\npull requests, work-items, etc.) and their relationships in modern source code\nmanagement systems. We employ a graph convolutional neural network on this\ngraph and train it on two and a half years of history on 332 repositories. We\nshow that CORAL is able to model the manual history of reviewer selection\nremarkably well. Further, based on an extensive user study, we demonstrate that\nthis approach identifies relevant and qualified reviewers who traditional\nreviewer recommenders miss, and that these developers desire to be included in\nthe review process. Finally, we find that \"classical\" reviewer recommendation\nsystems perform better on smaller (in terms of developers) software projects\nwhile CORAL excels on larger projects, suggesting that there is \"no one model\nto rule them all.\"", "entities_include_in_text": ["Sept. 2018", "Jan 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Automatic Identification of Self-Admitted Technical Debt from Different Sources"], "authors": ["[arxiv.Result.Author('Yikun Li'), arxiv.Result.Author('Mohamed Soliman'), arxiv.Result.Author('Paris Avgeriou')]"], "link": ["http://arxiv.org/pdf/2202.02387v1"], "summary": "Technical debt is a metaphor describing the situation that long-term benefits\n(e.g., maintainability and evolvability of software) are traded for short-term\ngoals. When technical debt is admitted explicitly by developers in software\nartifacts (e.g., code comments or issue tracking systems), it is termed as\nSelf-Admitted Technical Debt or SATD. Technical debt could be admitted in\ndifferent sources, such as source code comments, issue tracking systems, pull\nrequests, and commit messages. However, there is no approach proposed for\nidentifying SATD from different sources. Thus, in this paper, we propose an\napproach for automatically identifying SATD from different sources (i.e.,\nsource code comments, issue trackers, commit messages, and pull requests).", "entities_include_in_text": ["LREC 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?"], "authors": ["[arxiv.Result.Author('Stefan Pasch'), arxiv.Result.Author('Daniel Ehnes')]"], "link": ["http://arxiv.org/pdf/2202.02268v1"], "summary": "To answer this question, we fine-tune transformer-based language models,\nincluding BERT, on different sources of company-related text data for a\nclassification task to predict the one-year stock price performance. We use\nthree different types of text data: News articles, blogs, and annual reports.\nThis allows us to analyze to what extent the performance of language models is\ndependent on the type of the underlying document. StonkBERT, our\ntransformer-based stock performance classifier, shows substantial improvement\nin predictive accuracy compared to traditional language models. The highest\nperformance was achieved with news articles as text source. Performance\nsimulations indicate that these improvements in classification accuracy also\ntranslate into above-average stock market returns.", "entities_include_in_text": ["Devlin et al. 2018", "Loughran and McDonald 2011; Loughran and McDonald 2016; Jiang et al. 2019", "Chen 2021; Sivri et al. 2022", "Gennadiy 2020", "Loughran  and  McDonald  2011", "Loughran  and \n\nMcDonald 2016", "Ahmad  et  al.  2016", "Hillert et al. 2014", "Duz Tan and Tas 2021; Sprenger et al. \n\n2014", "Chan et al. 2020", "Peters et al. 2018", "Devlin  et  al.  2018", "Gonzalez-Carvajal and Garrido-Merchan 2020", "Liu et al. 2020). \n\nNot  surprisingly,  researchers  have  integrated  these  deep  learning  approaches  to  predict \n\nstock price movements by applying such language models on company-related text data, such \n\nas tweets or company reports. For example, Sawhney et al. (2020", "Gennadiy 2020", "Devlin  et  al.  2018", "Araci 2019", "Liu et al. \n\n2019", "Devlin et al. 2018", "Clark et al. 2020", "Gonzalez-Carvajal and Garrido-Merchan \n\n2020", "Fama 1970", "Hoberg and Phillips 2016", "Jegadeesh and Titman 2001", "Yuan et al. 2021", "Devlin et al. 2018"], "entities_from_reference": ["Plusieurs"]}{"title": ["A Discourse on MetODS: Meta-Optimized Dynamical Synapses for Meta-Reinforcement Learning"], "authors": ["[arxiv.Result.Author('Mathieu Chalvidal'), arxiv.Result.Author('Thomas Serre'), arxiv.Result.Author('Rufin VanRullen')]"], "link": ["http://arxiv.org/pdf/2202.02363v1"], "summary": "Recent meta-reinforcement learning work has emphasized the importance of\nmnemonic control for agents to quickly assimilate relevant experience in new\ncontexts and suitably adapt their policy. However, what computational\nmechanisms support flexible behavioral adaptation from past experience remains\nan open question. Inspired by neuroscience, we propose MetODS (for\nMeta-Optimized Dynamical Synapses), a broadly applicable model of\nmeta-reinforcement learning which leverages fast synaptic dynamics influenced\nby action-reward feedback. We develop a theoretical interpretation of MetODS as\na model learning powerful control rules in the policy space and demonstrate\nempirically that robust reinforcement learning programs emerge spontaneously\nfrom them. We further propose a formalism which efficiently optimizes the\nmeta-parameters governing MetODS synaptic processes. In multiple experiments\nand domains, MetODS outperforms or compares favorably with previous\nmeta-reinforcement learning approaches. Our agents can perform one-shot\nlearning, approaches optimal exploration/exploitation strategies, generalize\nnavigation principles to unseen environments and demonstrate a strong ability\nto learn adaptive motor policies.", "entities_include_in_text": ["Clune, 2020", "Mnih et al., 2015", "Mnih et al., 2013", "Jaderberg\net al., 2019", "Levine et al., 2016; Lillicrap\net al., 2016", "Botvinick et al., 2019", "Finn et al., 2017", "Yger et al., 2015", "Florian, 2007", "Lu et al.,\n2019", "Ram-\nsauer et al., 2020", "Schlag\net al., 2020", "Chalvidal et al., 2021", "Houthooft et al., 2018; Xu et al., 2018; Gupta et al., 2018", "Hochreiter et al.,\n2001; Duan et al., 2016; Wang et al., 2018", "Mishra et al., 2018", "Finn et al., 2017", "Miconi, 2016", "Choromanski et al., 2020", "Duan et al., 2016", "Ramsauer et al., 2020", "Mishra et al.,\n2018", "Ramsauer et al., 2020", "Paszke\net al., 2019", "Rumelhart et al., 1985", "Betancourt et al., 2020", "Chalvidal et al.,\n2021", "Mnih et al., 2016", "Harlow,\n1949", "Prim, 1957", "Mishra et al., 2018", "Gittins, 1979", "Mishra et al., 2018", "Todorov\net al., 2012", "Finn et al., 2017", "Betancourt et al., 2020", "Schulman et al., 2018", "Wang et al., 2018", "Mishra et al., 2018"], "entities_from_reference": ["Nature", "Mnih", "Leibo", "Ionescu", "Barak", "Tsodyks", "Bartunov", "Rae", "Betancourt", "Botvinick", "Ritter", "Wang", "J. X.", "Caporale", "Dan", "Annual Review", "Chalvidal", "Ricci", "Choromanski", "Davis", "Slotine", "Varley", "Lee", "Weller", "Sindhwani", "Clune", "J.", "Cobbe", "Hesse", "Hilton", "Schulman", "J. Leveraging", "Upgang", "Duan", "Bartlett", "Finn", "Teh", "Machine Learning", "Machine Learning Research", "J. C. Bandit", "Gupta", "Liu", "Dai", "Le", "Harlow", "Hinton", "Plaut", "Annual Conference", "Hochreiter", "Younger", "Springer", "Hopfield", "J. J.", "Houthooft", "Isola", "Stadie", "Wolski", "Jaderberg", "Czarnecki", "Marris", "Casta", "Beattie", "Rabinowitz", "Ruderman", "Sonnerat", "Deason", "Graepel", "Science", "Koiran", "Krotov", "J. J", "Dense", "Ullman", "Tenenbaum", "Lengyel", "Dayan", "Koller", "Singer", "Roweis", "Levine", "Darrell", "Lillicrap", "Heess", "Erez", "Lin", "Zhao", "Yang", "Zhang", "Jin", "Manohar", "Masse", "Freedman", "Miconi", "Rawal", "Stanley", "J. Differentiable", "Mishra", "Antonoglou", "Riedmiller", "Ostrovski", "Mirza", "Harley", "Mongillo", "Munkhdalai", "Najarro", "Ranzato", "Hadsell", "Paszke", "Gross", "Lerer", "Bradbury", "Chanan", "Killeen", "Gimelshein", "Desmaison", "Kopf", "Raison", "Tejani", "Steiner", "Fang", "Bai", "Pritzel", "Vinyals", "Machine A Discourse", "Schlag", "Ramsauer", "Sch", "Lehner", "Seidl", "Widrich", "Adler", "Kreil", "Kopp", "Ravi", "Regehr", "Jayakumar", "Pascanu", "Stokes", "Thrun", "Rumelhart", "Williams", "San Diego La Jolla Inst", "Cognitive Science", "Santoro", "Todorov", "Systems", "Vilalta", "Artif", "Brain Function", "Kumaran", "Hasselt", "Yger", "Brette", "Zaremba", "Zhu", "Discrete", "Xavier", "Maze Navigation", "Green", "Average", "Reward"]}{"title": ["Towards Training Reproducible Deep Learning Models"], "authors": ["[arxiv.Result.Author('Boyuan Chen'), arxiv.Result.Author('Mingzhi Wen'), arxiv.Result.Author('Yong Shi'), arxiv.Result.Author('Dayi Lin'), arxiv.Result.Author('Gopi Krishnan Rajbahadur'), arxiv.Result.Author('Zhen Ming'), arxiv.Result.Author('Jiang')]"], "link": ["http://arxiv.org/pdf/2202.02326v1"], "summary": "Reproducibility is an increasing concern in Artificial Intelligence (AI),\nparticularly in the area of Deep Learning (DL). Being able to reproduce DL\nmodels is crucial for AI-based systems, as it is closely tied to various tasks\nlike training, testing, debugging, and auditing. However, DL models are\nchallenging to be reproduced due to issues like randomness in the software\n(e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There\nare various practices to mitigate some of the aforementioned issues. However,\nmany of them are either too intrusive or can only work for a specific usage\ncontext. In this paper, we propose a systematic approach to training\nreproducible DL models. Our approach includes three main parts: (1) a set of\ngeneral criteria to thoroughly evaluate the reproducibility of DL models for\ntwo different domains, (2) a unified framework which leverages a\nrecord-and-replay technique to mitigate software-related randomness and a\nprofile-and-patch technique to control hardware-related non-determinism, and\n(3) a reproducibility guideline which explains the rationales and the\nmitigation strategies on conducting a reproducible training process for DL\nmodels. Case study results show our approach can successfully reproduce six\nopen source and one commercial DL models.", "entities_include_in_text": ["accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed August, 2021", "accessed Feb, 2022", "accessed August, 2021"], "entities_from_reference": ["Plusieurs"]}{"title": ["Beam Management with Orientation and RSRP using Deep Learning for Beyond 5G Systems"], "authors": ["[arxiv.Result.Author('Khuong N. Nguyen'), arxiv.Result.Author('Anum Ali'), arxiv.Result.Author('Jianhua Mo'), arxiv.Result.Author('Boon Loong Ng'), arxiv.Result.Author('Vutha Va'), arxiv.Result.Author('Jianzhong Charlie Zhang')]"], "link": ["http://arxiv.org/pdf/2202.02247v1"], "summary": "Beam management (BM), i.e., the process of finding and maintaining a suitable\ntransmit and receive beam pair, can be challenging, particularly in highly\ndynamic scenarios. Side-information, e.g., orientation, from on-board sensors\ncan assist the user equipment (UE) BM. In this work, we use the orientation\ninformation coming from the inertial measurement unit (IMU) for effective BM.\nWe use a data-driven strategy that fuses the reference signal received power\n(RSRP) with orientation information using a recurrent neural network (RNN).\nSimulation results show that the proposed strategy performs much better than\nthe conventional BM and an orientation-assisted BM strategy that utilizes\nparticle filter in another study. Specifically, the proposed data-driven\nstrategy improves the beam-prediction accuracy up to 34% and increases mean\nRSRP by up to 4.2 dB when the UE orientation changes quickly.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["The 6-Ds of Creating AI-Enabled Systems"], "authors": ["[arxiv.Result.Author('John Piorkowski')]"], "link": ["http://arxiv.org/pdf/2202.03172v1"], "summary": "We are entering our tenth year of the current Artificial Intelligence (AI)\nspring, and, as with previous AI hype cycles, the threat of an AI winter looms.\nAI winters occurred because of ineffective approaches towards navigating the\ntechnology valley of death. The 6-D framework provides an end-to-end framework\nto successfully navigate this challenge. The 6-D framework starts with problem\ndecomposition to identify potential AI solutions, and ends with considerations\nfor deployment of AI-enabled systems. Each component of the 6-D framework and a\nprecision medicine use case is described in this paper.", "entities_include_in_text": ["Krizhev-\nsky  et  al.,  2012", "Lawrence,  2019", "Collins and Porras, 1996", "Ravitz et al., 2013", "Jesuthasan  and \nBoudreau, 2018", "Gavin, 2019", "Lawrence, 2017", "Chee et al., 2018", "April 2014", "NIPS 2012"], "entities_from_reference": ["Plusieurs"]}{"title": ["Deep invariant networks with differentiable augmentation layers"], "authors": ["[arxiv.Result.Author('C\u00e9dric Rommel'), arxiv.Result.Author('Thomas Moreau'), arxiv.Result.Author('Alexandre Gramfort')]"], "link": ["http://arxiv.org/pdf/2202.02142v1"], "summary": "Designing learning systems which are invariant to certain data\ntransformations is critical in machine learning. Practitioners can typically\nenforce a desired invariance on the trained model through the choice of a\nnetwork architecture, e.g. using convolutions for translations, or using data\naugmentation. Yet, enforcing true invariance in the network can be difficult,\nand data invariances are not always known a piori. State-of-the-art methods for\nlearning data augmentation policies require held-out data and are based on\nbilevel optimization problems, which are complex to solve and often\ncomputationally demanding. In this work we investigate new ways of learning\ninvariances only from the training data. Using learnable augmentation layers\nbuilt directly in the network, we demonstrate that our method is very\nversatile. It can incorporate any type of differentiable augmentation and be\napplied to a broad class of learning problems beyond computer vision. We\nprovide empirical evidence showing that our approach is easier and faster to\ntrain than modern automatic data augmentation techniques based on bilevel\noptimization, while achieving comparable results. Experiments show that while\nthe invariances transferred to a model through automatic data augmentation are\nlimited by the model expressivity, the invariance yielded by our approach is\ninsensitive to it by design.", "entities_include_in_text": ["LeCun et al., 1989", "Krizhevsky et al., 2012", "Zhou\net al., 2021", "Chambon et al., 2018; Phan et al.,\n2021", "Cubuk et al., 2019", "Cubuk\net al., 2019", "Ho et al., 2019", "Lim et al., 2019", "Hataya et al., 2020", "Li\net al., 2020", "Rommel et al., 2021", "Liu\net al., 2019", "Zaheer et al., 2017)\nencode permutation invariance by summing networks pre-\ndictions. Although related, these methods are not designed\nfor learning symmetries from the data, which is the objective\nof this study.\n\nPrior work on this matter from van der Wilk et al. (2018) pro-\nposes to learn invariances using the marginal likehood in the\ncontext of Gaussian processes. In contrast, we are mostly fo-\ncused on deep neural networks. Zhou et al. (2021", "Benton\net al., 2020", "cf. Chen et al. (2020", "Schulman\net al., 2015", "Bengio et al., 2013", "Grathwohl\net al., 2018", "Benton et al., 2020", "Benton et al., 2020", "Benton et al.,\n2020", "Rommel et al.,\n2021", "Schwabedal et al., 2019", "Wang et al., 2018", "Howard,\n2014", "Inoue, 2018)", "Iber\net al., 2007", "Krizhevsky et al., 2009", "Benton et al., 2020", "Chambon et al.,\n2018", "Cubuk et al., 2019", "Lim\net al., 2019", "Hataya et al., 2020", "Li et al., 2020", "Rommel et al., 2021", "Benton et al., 2020", "medium according to Benton et al. (2020)", "Schirrmeister et al., 2017", "Akiba et al., 2019", "Riba\net al., 2020", "Hataya et al., 2020", "Chambon\net al., 2018", "Gramfort et al.,\n2013", "Schirrmeister et al.,\n2017", "Chambon et al., 2018"], "entities_from_reference": ["Ohta", "Data Mining", "Bengio", "Benton", "Finzi", "Izmailov", "Wilson", "Bouchacourt", "Ibrahim", "Chambon", "Galtier", "Wainrib", "Chen", "Lee", "J.", "Data Augmentation", "Hsieh", "Machine Learning", "Cohen", "Cubuk", "Zoph", "Mane", "Vasudevan", "Le", "Pattern Recognition", "Gramfort", "Larson", "Strohmeier", "Goj", "Jas", "Grathwohl", "Choi", "Roeder", "Duvenaud", "Hataya", "Springer International Publishing", "Zhang", "Ren", "Deep", "Liang", "Population", "Howard", "Chesson", "Quan", "Terminology", "Inoue", "Kingma", "J. Adam", "Krizhevsky", "Denker", "J. S.", "Henderson", "Hubbard", "Jackel", "Wang", "Robertson", "Yang", "Lim", "Kim", "Zela", "Elsken", "Marrakchi", "Brox", "Hutter", "Chang", "Abbasnejad", "Haffari", "Stochastic Implicit Gradients", "Zhou", "Knowles", "Liu", "Simonyan", "Gosselin", "Carrier", "Nielsen", "Phan", "Koch", "Mertins", "De Vos", "Pattern Analysis", "Machine Intelligence", "Riba", "Mishkin", "Bradski", "Source Differentiable Computer", "Rommel", "Paillard", "J. T.", "Glasstetter", "Eggensperger", "Tangermann", "Burgard", "Human Brain Mapping", "Schulman", "Heess", "Schwabedal", "J. T. C.", "J. C.", "Nemati", "Clifford", "Noisy Signals", "Fourier Transform Surrogates", "Wilk", "Bauer", "John", "Hensman", "Peng", "Jiang", "Deep Convolutional Neural Networks", "Series Title", "Zaheer", "Salakhutdinov", "Augerino", "Conv2D ReLU Conv2D", "Figure", "Sinusoids", "Faster AA", "Akiba", "Faster", "Adam", "Balanced", "Tesla V100", "Dense", "Softmax Table", "Frequency Sensors", "Figure B.2", "Loss", "Random"]}{"title": ["Identifying Self-Admitted Technical Debt in Issue Tracking Systems using Machine Learning"], "authors": ["[arxiv.Result.Author('Yikun Li'), arxiv.Result.Author('Mohamed Soliman'), arxiv.Result.Author('Paris Avgeriou')]"], "link": ["http://arxiv.org/pdf/2202.02180v1"], "summary": "Technical debt is a metaphor indicating sub-optimal solutions implemented for\nshort-term benefits by sacrificing the long-term maintainability and\nevolvability of software. A special type of technical debt is explicitly\nadmitted by software engineers (e.g. using a TODO comment); this is called\nSelf-Admitted Technical Debt or SATD. Most work on automatically identifying\nSATD focuses on source code comments. In addition to source code comments,\nissue tracking systems have shown to be another rich source of SATD, but there\nare no approaches specifically for automatically identifying SATD in issues. In\nthis paper, we first create a training dataset by collecting and manually\nanalyzing 4,200 issues (that break down to 23,180 sections of issues) from\nseven open-source projects (i.e., Camel, Chromium, Gerrit, Hadoop, HBase,\nImpala, and Thrift) using two popular issue tracking systems (i.e., Jira and\nGoogle Monorail). We then propose and optimize an approach for automatically\nidentifying SATD in issue tracking systems using machine learning. Our findings\nindicate that: 1) our approach outperforms baseline approaches by a wide margin\nwith regard to the F1-score; 2) transferring knowledge from suitable datasets\ncan improve the predictive performance of our approach; 3) extracted SATD\nkeywords are intuitive and potentially indicating types and indicators of SATD;\n4) projects using different issue tracking systems have less common SATD\nkeywords compared to projects using the same issue tracking system; 5) a small\namount of training data is needed to achieve good accuracy.", "entities_include_in_text": ["Avgeriou et al., 2016", "Li\net al., 2015", "Alves et al., 2016", "Tufano et al., 2017", "Ernst, 2012)", "Potdar and Shihab, 2014", "Potdar and Shihab, 2014", "Sierra\net al., 2019", "Li\net al., 2020; Bellomo et al., 2016", "Merten et al., 2015; Li et al., 2020", "Li et al., 2020", "Li\net al., 2020", "Kim, 2014", "Li et al., 2020", "van Solingen et al., 2002", "Shalev-Shwartz and Ben-David, 2014", "Potdar and Shihab, 2014; Wehaibi et al., 2016; Zampetti et al., 2018", "Smith, 2018", "Potdar and Shihab, 2014; Wehaibi et al., 2016; d. S. Maldon-\nado et al., 2017", "Li et al., 2020", "Alves\net al., 2014", "Fleiss et al., 1981", "Sun et al., 2009", "McCallum et al., 1998", "Tan, 2006", "Genkin et al., 2007", "Xu et al., 2012", "Kowsari et al., 2019", "d. S. Mal-\ndonado et al., 2017; Huang et al., 2018; Flisar and Podgorelec, 2019", "Yao et al., 2019", "Yao et al., 2019", "Ren et al.,\n2019", "Bavota\nand Russo, 2016", "Wei and Zou, 2019", "Phan et al., 2017; Ren\net al., 2019", "Joulin et al., 2017; Wieting et al., 2015", "Mikolov et al., 2018", "Kim, 2014", "see Potdar and Shihab (2014", "Mikolov et al., 2018", "Bo-\njanowski et al., 2017", "Zhang and Wallace, 2017", "Zhang and Wallace, 2017", "Semwal et al., 2018", "d. S. Maldonado et al.,\n2017", "Zhang et al.,\n2015", "Semwal\net al., 2018", "Ortu et al., 2016; Calefato et al., 2018", "Perkins et al., 1992", "see\nPotdar and Shihab (2014)", "Li et al., 2020", "Gu\net al., 2014", "HBase-1990", "see Potdar and Shihab (2014)", "d. S. Mal-\ndonado et al., 2017; Flisar and Podgorelec, 2019; Ren et al., 2019", "d. S. Maldonado et al., 2017; Huang et al., 2018", "Zhang and Wallace, 2017", "Semwal et al., 2018", "LREC 2018)\nOrtu M, Murgia A, Destefanis G, Tourani P, Tonelli R, Marchesi M, Adams\nB (2016"], "entities_from_reference": ["Ribeiro LF", "Caires V", "Mendes TS", "Mendon", "Sp", "Shull F", "Seaman C", "Kruchten P", "Ozkaya", "Dagstuhl", "Russo B", "Nord RL", "Popeck M", "Bojanowski P", "Grave E", "Joulin A", "Mikolov T", "Calefato F", "Lanubile F", "Maiorano F", "Novielli N", "Empirical Software", "Dai K", "Efstathiou V", "Chatzilenas C", "Spinellis D", "Fern", "Garc", "Galar M", "Prati RC", "Krawczyk B", "Herrera F", "Springer Fleiss JL", "Levin B", "Paik MC", "Flisar J", "Podgorelec V", "Freitas Farias MA", "Santos JA", "Kalinowski M", "Springer", "Lewis DD", "Madigan D", "Gu L", "Eils R", "Schlesner M", "Brors B", "Huang Q", "Shihab E", "Xia X", "Lo D", "Li S", "Short Papers", "Maldonado EdS", "Ubayashi N", "Kim Y", "Kowsari K", "Jafari Meimandi K", "Heidarysafa M", "Mendu S", "Barnes L", "Brown D", "Li Y", "Soliman M", "Avgeriou P", "Advanced Applications", "Machine Learning", "Liang P", "Nigam K", "Mager B", "Quirchmayr T", "Paech B", "Puhrsch C", "Ortu M", "Murgia A", "Destefanis G", "Tourani P", "Tonelli R", "Marchesi M", "Adams B", "Perkins DN", "Salomon G", "Gerkmann T", "Potdar A", "Ren X", "Wang X", "Grundy J", "Runeson P", "Rainer A", "Regnell B", "John Wiley", "S Maldonado E", "Maldonado E", "Tsantalis N", "Semwal T", "Yenigalla P", "Mathur G", "Nair SB", "Data Mining", "Ben-David S", "Cambridge", "Sierra G", "Kamei Y", "Smith T", "Solingen R", "Basili V", "Caldiera G", "Rombach HD", "Goal Question Metric", "Sons", "Hoboken", "Steidl D", "Hummel B", "Juergens E", "Ieee", "Lim EP", "Liu Y", "Tan S", "Tufano M", "Palomba F", "Bavota G", "Oliveto R", "Di Penta M", "De Lucia A", "Poshyvanyk D", "Wehaibi S", "Guerrouj L", "Evolution", "Zou K", "Bansal M", "Gimpel K", "Livescu K", "Xavier L", "Ferreira F", "Brito R", "Valente MT", "Guo X", "Ye Y", "Cheng J", "Yao L", "Mao C", "Luo Y", "Serebrenik A", "Di Penta", "Zhang X", "Zhao J", "Zhang Y", "Wallace BC"]}{"title": ["Interpretability methods of machine learning algorithms with applications in breast cancer diagnosis"], "authors": ["[arxiv.Result.Author('Panagiota Karatza'), arxiv.Result.Author('Kalliopi V. Dalakleidi'), arxiv.Result.Author('Maria Athanasiou'), arxiv.Result.Author('Konstantina S. Nikita')]"], "link": ["http://arxiv.org/pdf/2202.02131v1"], "summary": "Early detection of breast cancer is a powerful tool towards decreasing its\nsocioeconomic burden. Although, artificial intelligence (AI) methods have shown\nremarkable results towards this goal, their \"black box\" nature hinders their\nwide adoption in clinical practice. To address the need for AI guided breast\ncancer diagnosis, interpretability methods can be utilized. In this study, we\nused AI methods, i.e., Random Forests (RF), Neural Networks (NN) and Ensembles\nof Neural Networks (ENN), towards this goal and explained and optimized their\nperformance through interpretability techniques, such as the Global Surrogate\n(GS) method, the Individual Conditional Expectation (ICE) plots and the Shapley\nvalues (SV). The Wisconsin Diagnostic Breast Cancer (WDBC) dataset of the open\nUCI repository was used for the training and evaluation of the AI algorithms.\nThe best performance for breast cancer diagnosis was achieved by the proposed\nENN (96.6% accuracy and 0.96 area under the ROC curve), and its predictions\nwere explained by ICE plots, proving that its decisions were compliant with\ncurrent medical knowledge and can be further utilized to gain new insights in\nthe pathophysiological mechanisms of breast cancer. Feature selection based on\nfeatures' importance according to the GS model improved the performance of the\nRF (leading the accuracy from 96.49% to 97.18% and the area under the ROC curve\nfrom 0.96 to 0.97) and feature selection based on features' importance\naccording to SV improved the performance of the NN (leading the accuracy from\n94.6% to 95.53% and the area under the ROC curve from 0.94 to 0.95). Compared\nto other approaches on the same dataset, our proposed models demonstrated state\nof the art performance while being interpretable.", "entities_include_in_text": [], "entities_from_reference": ["Online", "Anderson", "Saygili", "Classification", "Breast Cancers", "Breast Cancer Malignancy", "Networks", "Breast Cancer", "Neighborhood Component Analysis", "Machine Learning Techniques", "Advanced Communication", "Toward Medical XAI", "Nikita", "Diabetes Mellitus", "Santos", "Antunes", "J. Kasmanas", "Machine Learning", "Santurkar", "How Does Batch Normalization Help", "Baba"]}{"title": ["Generative Modeling of Complex Data"], "authors": ["[arxiv.Result.Author('Luca Canale'), arxiv.Result.Author('Nicolas Grislain'), arxiv.Result.Author('Gr\u00e9goire Lothe'), arxiv.Result.Author('Johan Leduc')]"], "link": ["http://arxiv.org/pdf/2202.02145v1"], "summary": "In recent years, several models have improved the capacity to generate\nsynthetic tabular datasets. However, such models focus on synthesizing simple\ncolumnar tables and are not useable on real-life data with complex structures.\nThis paper puts forward a generic framework to synthesize more complex data\nstructures with composite and nested types. It then proposes one practical\nimplementation, built with causal transformers, for struct (mappings of types)\nand lists (repeated instances of a type). The results on standard benchmark\ndatasets show that such implementation consistently outperforms current\nstate-of-the-art models both in terms of machine learning utility and\nstatistical similarity. Moreover, it shows very strong results on two complex\nhierarchical datasets with multiple nesting and sparse data, that were\npreviously out of reach.", "entities_include_in_text": [], "entities_from_reference": ["Plusieurs"]}{"title": ["OntoSeer -- A Recommendation System to Improve the Quality of Ontologies"], "authors": ["[arxiv.Result.Author('Pramit Bhattacharyya'), arxiv.Result.Author('Raghava Mutharaju')]"], "link": ["http://arxiv.org/pdf/2202.02125v1"], "summary": "Building an ontology is not only a time-consuming process, but it is also\nconfusing, especially for beginners and the inexperienced. Although ontology\ndevelopers can take the help of domain experts in building an ontology, they\nare not readily available in several cases for a variety of reasons. Ontology\ndevelopers have to grapple with several questions related to the choice of\nclasses, properties, and the axioms that should be included. Apart from this,\nthere are aspects such as modularity and reusability that should be taken care\nof. From among the thousands of publicly available ontologies and vocabularies\nin repositories such as Linked Open Vocabularies (LOV) and BioPortal, it is\nhard to know the terms (classes and properties) that can be reused in the\ndevelopment of an ontology. A similar problem exists in implementing the right\nset of ontology design patterns (ODPs) from among the several available.\nGenerally, ontology developers make use of their experience in handling these\nissues, and the inexperienced ones have a hard time. In order to bridge this\ngap, we propose a tool named OntoSeer, that monitors the ontology development\nprocess and provides suggestions in real-time to improve the quality of the\nontology under development. It can provide suggestions on the naming\nconventions to follow, vocabulary to reuse, ODPs to implement, and axioms to be\nadded to the ontology. OntoSeer has been implemented as a Prot\\'eg\\'e plug-in.", "entities_include_in_text": ["Apr 2014"], "entities_from_reference": ["Bhattacharyya", "Mutharaju", "Bhattachayya", "Indraprastha Institute", "Cahyono", "Science", "Dreler", "Ngonga Ngomo", "Bounded Jaro Winkler Distances", "Garc", "Hitzler", "Janowicz", "Zaveri", "Gray", "Lopez", "Haller", "Springer International Publishing", "Angele", "Knowledge Management", "Guarino", "Staab", "Springer Berlin Heidelberg", "Hammar", "Ontology Design Patterns", "Gangemi", "Krisnadhi", "Le", "Mikolov", "Machine Learning", "Noy", "Ontology Evaluation", "Ren", "Deemter", "Stevens"]}